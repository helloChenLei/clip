---
title: "人工智能安全：技术与人种学概述"
date: 2023-03-30T14:03:04+08:00
updated: 2023-03-30T14:03:04+08:00
taxonomies:
  tags: []
extra:
  source: https://www.jonstokes.com/p/ai-safety-a-technical-and-ethnographic
  hostname: www.jonstokes.com
  author: Jon Stokes
  original_title: "AI Safety: A Technical & Ethnographic Overview"
  original_lang: en
---

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2F92820971-b8e4-449d-8993-e62ecc3cc198_1408x1024.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92820971-b8e4-449d-8993-e62ecc3cc198_1408x1024.jpeg)

**The story so far:** _We need to talk about The Letter. No not [that letter](https://harpers.org/a-letter-on-justice-and-open-debate/), or [the other letter](https://nytletter.com/), or the [other one](https://concerned.tech/)… **[the AI letter](https://futureoflife.org/open-letter/pause-giant-ai-experiments/)**. The one that calls on all of humanity to, “pause for at least 6 months the training of AI systems more powerful than GPT-4.”_  

到目前为止的故事：我们需要谈谈那封信。不，不是那封信，也不是另一封，或者另一封......人工智能的信。那封呼吁全人类 "暂停训练比GPT-4更强大的AI系统至少6个月 "的信。

_For some weeks I had been noodling on a draft entitled, “AI Safety: An Ethnography,” and today was going to be the day I published it.  

But earlier today that letter dropped, and it so powerfully illuminated the rich, complex contours of the terrain of the “AI safety” issue that I had to fishtail and rework this post to center it.  

几个星期以来，我一直在酝酿一份题为 "人工智能安全 "的草案：人种学"，今天本来是我发表它的日子。但今天早些时候，那封信掉了下来，它如此有力地阐明了 "人工智能安全 "问题的丰富、复杂的轮廓，以至于我不得不把它放在中心位置，重新修改这篇文章。_

_There are two main threads I’ll follow in this piece:  

在这篇文章中，我将遵循两条主线：_

1.  _Technical, covering some key specifics about GPT-4 that make the open letter honestly kind of nonsensical and a bit of a Rorschach test.   
    
    技术性的，涵盖了关于GPT-4的一些关键细节，使公开信说实话有点无稽之谈，有点像罗夏克测试。_
    
2.  _Ethnographic, covering the cultural and tribal divisions and allegiances that are cropping up around the technical issues.  
    
    人种学，涵盖了围绕技术问题出现的文化和部落的分歧和忠诚。_
    

_I’ve crammed these threads into one extremely lengthy post because in order to understand the tribal dynamics you have to understand the technical issues.  

But you also can’t understand how the technical issues are framed without understanding the tribal dynamics.  

These things are intertwined in the “AI safety” issue to a degree that makes them impossible to separate, which is why this topic has fascinated me since the beginning of this newsletter in 2021.  

我把这些话题塞进一个极其冗长的帖子，因为为了理解部落的动态，你必须理解技术问题。但是，如果不了解部落的动态，你也无法理解技术问题是如何构成的。这些东西在 "人工智能安全 "问题上交织在一起，其程度使它们无法分开，这就是为什么从2021年这份通讯开始，这个话题就吸引着我。_

When it comes to the question of how worried we should all be about AI, everyone initially arrives at this issue with their own culture war baggage.  

当涉及到我们应该对人工智能有多担心的问题时，每个人最初都带着自己的文化战争包袱来处理这个问题。  

But what I’m seeing play out after that initial encounter is a two-step process:  

但我看到的是，在最初的接触之后，出现了一个两步的过程：

1.  You immediately start pattern-matching and sorting to figure out which is the “correct” side.  
    
    你立即开始模式匹配和排序，以找出哪个是 "正确 "的一面。  
    
    This tribal sorting is parallel to and even part of the process of educating yourself about the issue itself.  
    
    这种部落整理与教育自己了解问题本身的过程是平行的，甚至是一部分。
    
2.  But AI safety doesn’t easily map onto most existing culture war divides, and once you grasp this then you face a choice:  
    
    但是，人工智能安全并不容易映射到大多数现有的文化战争鸿沟中，一旦你掌握了这一点，你就会面临一个选择：
    
    1.  Re-sort yourself and your network along brand-new AI safety lines.  
        
        按照全新的人工智能安全线对自己和自己的网络进行重新排序。
        
    2.  Tweet through it!  
        
        通过它发微博!
        

In the ethnography portion of this piece, I’ll cover people who’ve picked each path of the decision fork above, i.e. a re-sort or just white-knuckled clinging to existing culture war frames.  

在这篇文章的人种学部分，我将涵盖那些选择了上述决定叉的每一条道路的人，即重新分类或只是白白地坚持现有的文化战争框架。  

I’ll also describe some people who are still stuck on step 1.  

我也会描述一些仍停留在第1步的人。

But most of the action in AI safety — and most of the confusion — is taking place in circles that have re-sorted along new and explicitly AI-safety-focused lines.  

但是，人工智能安全方面的大部分行动--以及大部分混乱--都发生在沿着新的、明确以人工智能安全为重点的路线重新排序的圈子里。  

These re-sorted types are the ones who’ve signed the letter and are signal-boosting it on Twitter.  

这些重新分类的人就是那些签署了这封信并在推特上发出信号的人。

✂️ In respect of how many in existing groups are sorting themselves along “AI safety” lines, it’s clear that the whole concept is rapidly emerging as a powerful [scissor](https://slatestarcodex.com/2018/10/30/sort-by-controversial/) — a statement or meme that divides a tribe of people into strongly pro and strongly con.  

At its most basic level, a scissor is a spectacle that is taken in by the primitive parts of the human brain — specifically the parts that are wired to pre-cognitively sort **friend from foe**.   

✂️ 就现有团体中的许多人正在按照 "人工智能安全 "的路线进行分类而言，很明显，整个概念正在迅速成为一把强大的剪刀--一种将一个部落的人分为强烈支持和强烈反对的声明或备忘录。在其最基本的层面上，剪刀是一种被人类大脑的原始部分所接受的景象--特别是那些被连接到预先认知的敌我分类的部分。

So let’s dive into the letter and the technical issues first, so we can understand how this new, post-culture-war, AI-centered, friend-vs-foe division is shaping up.   

因此，让我们先深入了解这封信和技术问题，这样我们就能了解这个新的、文化战争后的、以人工智能为中心的、朋友与敌人的划分是如何形成的。

The AI open letter falls into the same trap as all open letters: the moment you start asking for enough specifics to make the recommendation actionable, the cracks start to show and the whole project starts looking like a publicity stunt.  

人工智能公开信和所有的公开信一样，落入了同样的陷阱：当你开始要求足够多的细节以使建议可操作时，裂缝开始显现，整个项目开始看起来像一个宣传噱头。

Take a look a the money quote of the AI letter — the big ask:  

请看一下AI信中的一句话--大要求：

> _Therefore, we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4. This pause should be public and verifiable, and include all key actors.  
> 
> If such a pause cannot be enacted quickly, governments should step in and institute a moratorium.  
> 
> 因此，我们呼吁所有人工智能实验室立即暂停比GPT-4更强大的人工智能系统的训练，至少暂停6个月。这种暂停应该是公开的、可核查的，并包括所有的关键行为者。如果这种暂停不能迅速颁布，政府应该介入并制定暂停令。_

The whole AI safety problem is contained in the phrase, “AI systems more powerful than GPT-4.” What _exactly_ does this mean, though?  

整个人工智能安全问题包含在 "比GPT-4更强大的人工智能系统 "这句话中。不过，这到底是什么意思？

**✋ Hold up:** I know what you think is coming next because it’s a standard culture war pattern. We all do it.  

等等：我知道你认为接下来会发生什么，因为这是一个标准的文化战争模式。我们都会这样做。

-   If you come at me, a 2A supporter, with “ban assault weapons now!” then I am gonna drop a 10-kiloton nuance bomb of deep, thorny definitional problems directly over your position, and watch to see if you scramble for cover.  
    
    如果你对我这个2A支持者说："现在禁止攻击性武器！"那么，我将在你的位置上直接投下一颗10千吨级的细微差别炸弹，以解决深层的、棘手的定义问题，并看你是否争相躲避。
    
-   If I come at you, an LGBTQIA+ ally, with “a woman is an adult human female,” you are gonna drop a 10-kiloton nuance bomb of deep, thorny definitional problems directly over my position, and watch to see if I scramble for cover.  
    
    如果我对你这个LGBTQIA+的盟友说 "女人是一个成年女性"，你就会在我的位置上直接投下一颗10吨重的细微差别炸弹，其中包含深刻、棘手的定义问题，然后看我是否会慌忙躲避。
    

But reader, I promise you that I am not about to deploy definitional nuance in order to win an argument.  

但读者，我向你保证，我不会为了赢得争论而部署定义上的细微差别。  

Rather, I will (hopefully) illustrate that the technical, measurable definition of “progress” in AI — and even more narrowly, GPT-4’s progress over GPT-3 — is the whole ballgame.   

相反，我将（希望）说明，人工智能中 "进步 "的技术性、可衡量的定义--甚至更狭义地说，GPT-4比GPT-3的进步--是整个球赛。

**To summarize** the points I’ll make in this section:  

总结一下我在本节中要提出的观点：

1.  There is no “I know it when I see it” position for any side of this issue to fall back to because **nobody has ever actually seen the “it”** in question!  
    
    这个问题的任何一方都没有 "我看到它就知道 "的立场，因为没有人真正看到过有关的 "它"！
    
2.  Sam Altman can’t **even boil down the main intelligence improvements** from GPT-3 to GPT-4, apart from pointing to the models’ performance on some quizzes and tests that are themselves hotly contested as signifiers of human potential in our current political climate.  
    
    萨姆-奥特曼甚至无法归纳出从GPT-3到GPT-4的主要智力改进，除了指出模型在一些测验和测试中的表现，而这些测验和测试本身在我们目前的政治气候中作为人类潜力的标志而受到激烈的争议。
    
3.  OpenAI as a company has opened up its evaluation tools because they’re so in the dark on this core question of what “more powerful” looks like that they desperately need the rest of the world to help them come up with ways to **measure the capability** of the systems they’re developing.  
    
    作为一家公司，OpenAI已经开放了它的评估工具，因为他们对 "更强大 "是什么样子这个核心问题一无所知，他们迫切需要世界其他地方帮助他们想出衡量他们正在开发的系统的能力的方法。
    

👀 To reframe the first point above (“I know it when I see it”) using my two culture war examples: assault weapon ban promoters have all seen things they are certain are obviously “assault weapons,” and gender criticals have all seen people they are certain are obviously “women,” _but not a single soul opining about “AI safety” has ever once encountered the thing they’re worried about us bringing into existence in the near or medium term_.  

用我的两个文化战争的例子来重构上面的第一点（"我一看就知道"）：攻击性武器禁令的倡导者都见过他们肯定是 "攻击性武器 "的东西，而性别批评者都见过他们肯定是 "女性 "的人，但没有一个人在评论 "人工智能安全 "时曾经遇到过他们担心我们在近期或中期内带来的东西。

As a scissor, then, “AI safety” is unique in that it requires two separate cognitive moves:  

那么，作为一把剪刀，"AI安全 "的独特之处在于，它需要两个独立的认知动作：

1.  **Imagine in your mind** what “AGI” would or possibly could look like.  
    
    在你的脑海中想象一下 "AGI "会或可能会是什么样子。
    
2.  Handoff that mental picture to the primitive part of your brain to **pattern match** as “friend” or “foe.”  
    
    将这一心理图景移交给你大脑的原始部分，以模式匹配为 "朋友 "或 "敌人"。
    

Given that this is how it is, I have to confess to quite a bit of sympathy for the anti-“techbro” and anti-“AI hype” tendency to mock those expressing safety concerns as boys sitting around the campfire with flashlights in their faces, creeping each other out with made-up ghost stories.  

鉴于情况就是这样，我不得不承认，我相当同情反 "技术兄弟 "和反 "人工智能炒作 "的倾向，嘲笑那些表达安全关切的人是围坐在篝火旁，用手电筒照着自己的脸，用编造的鬼故事让对方毛骨悚然。  

It kinda does look like this!  

它确实有点像这样!

Even worse is the tendency among so many AI safetyists to analogize the threat they’ve imagined in their minds to the threat posed by nuclear weapons.  

It happens all the time and it never, ever convinces anyone who isn’t already convinced. I almost want to propose **Stokes’s Law:** _as an online discussion of AI safety grows longer, the probability of a comparison to nuclear weapons approaches 1_.  

更糟糕的是，许多人工智能安全主义者倾向于将他们脑海中想象的威胁类比为核武器带来的威胁。这种情况一直在发生，而且从来没有说服过任何还没有被说服的人。我几乎想提出斯托克斯定律：随着关于人工智能安全的在线讨论时间的延长，与核武器进行比较的概率接近1。

It always goes something like this:  

它总是像这样：

> **X-risker:** This jailbroken chatbot just gave me detailed instructions for robbing a bank! Now imagine a more powerful one giving out detailed instructions for making a nuclear bomb.  
> 
> X-risker:这个越狱的聊天机器人刚刚给我提供了抢劫银行的详细说明!现在想象一下，一个更强大的聊天机器人给出了制造核弹的详细说明。
> 
> **Me:** Bruh, those bank robbery instructions are not real. You cannot literally become a successful bank robber by reading those instructions then doing them. Please calm down.  
> 
> 我：兄弟，那些银行抢劫的说明不是真的。你不可能通过阅读这些说明然后去做就真的成为一个成功的银行劫匪。请冷静下来。
> 
> **X-risker:** But imagine the instructions _were_ real! A super powerful AI would be able to give very a very accurate, actionable plan for a bank robbery! And also for nukes! NU. CLE. AR. WEAPONS!  
> 
> X-risker:但想象一下这些指令是真实的！一个超级强大的人工智能将能够给出非常准确的银行抢劫计划！一个超级强大的人工智能将能够为银行抢劫提供一个非常准确的、可操作的计划!还有核弹!NU.CLE。AR.武器！
> 
> **Me:** But Brossandra, if we’re just imagining godlike AIs, why don’t we imagine one that loves us and tells us how to live forever? Why imagine Dionysus and not, say, Apollo?  
> 
> 我：但是布罗桑德拉，如果我们只是想象神一样的人工智能，为什么我们不想象一个爱我们并告诉我们如何长生不老的人呢？为什么要想象狄俄尼索斯，而不是，比如说，阿波罗？
> 
> **X-risker:** How do you not understand that this is like nukes.?! Do you just think everyone should have a nuclear bomb?  
> 
> We have blue-ribbon committees and laws for nukes, so we need them for AGI because it’s LIKE NUKES.  
> 
> X-risker:你怎么不明白，这就像核弹！你只是认为每个人都应该有一颗核弹吗？我们对核弹有蓝带委员会和法律，所以我们对AGI也需要，因为它就像核弹。
> 
> **Me:** Eh, I’m not seeing it. Check out this thread of sick Snoop Dogg memes I made with AI. Anything that can do this is a force for good.  
> 
> 我:嗯，我没有看到。看看我用人工智能制作的这个变态的Snoop Dogg备忘录的主题。任何能做到这一点的东西都是一种善的力量。

And around it goes, with both sides of this conversation doing friend-or-foe pattern matching on entirely different targets — the safetyist is pattern-matching on some viral ChatGPT bank robbery thread, and I’m pattern-matching on [my Snoop thread](https://twitter.com/jonst0kes/status/1633593896184758272), and we’re each free to pattern match on whatever we like that’s vaguely AI-related because there is no actually existing AGI to anchor our arguments in.  

绕来绕去，这次谈话的双方都在对完全不同的目标进行敌我模式匹配--安全主义者在对一些病毒性的ChatGPT银行抢劫线程进行模式匹配，而我在对我的Snoop线程进行模式匹配，而我们每个人都可以自由地对任何我们喜欢的、与人工智能隐约相关的东西进行模式匹配，因为没有实际存在的AGI来固定我们的论点。

If you listen to Lex Fridman’s interview with OpenAI CEO Sam Altman, there are a number of exchanges where Lex tries to get Sam to detail the improvements GPT-4 brings to the table over GPT-3 or to characterize how powerful it is.  

如果你听了Lex Fridman对OpenAI首席执行官Sam Altman的采访，在一些交流中，Lex试图让Sam详细介绍GPT-4比GPT-3带来的改进，或描述它有多强大。

<iframe src="https://www.youtube-nocookie.com/embed/L_Guz73e6fw?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409" data-immersive-translate-effect="1"></iframe>

Sam says a few things that are interesting and directly relevant to the question of defining what is and is not “more powerful than GPT-4.”   

萨姆说了几句话，这些话很有意思，与定义什么是和什么不是 "比GPT-4更强大 "的问题直接相关。

1.  OpenAI did not release the parameter size for GPT-4 because that number, which everyone is fixated on, is not a straightforward **measure of model capability** anymore.  
    
    OpenAI没有公布GPT-4的参数大小，因为这个大家都很看重的数字已经不是衡量模型能力的直接标准。
    
2.  OpenAI improved GPT-4 by making **many** **small improvements** in every phase — from dataset curation to training to fine-tuning to RLHF to inference — that all add up to big gains.  
    
    OpenAI通过在每个阶段进行许多小的改进来改进GPT-4--从数据集策划到训练到微调到RLHF到推理--所有这些加起来就是大的收益。
    
3.  We’re all still discovering what those “big gains” are, which is why OpenAI is committed to letting as many people as possible use the model as early as possible.  
    
    Measuring the full impact of all those additive (or multiplicative?) gains is a **large-scale, group effort**.  
    
    我们都还在发现那些 "大收益 "是什么，这就是为什么OpenAI致力于让尽可能多的人尽早使用这个模型。衡量所有这些加法（或乘法？）收益的全部影响是一项大规模的、集体的努力。
    
4.  In service of #3 above, OpenAI has **opened up its model evaluation tools** and is asking the public to help it develop ways to measure the performance differences between GPT-4 and other models on different types of tasks.  
    
    为了服务于上述第3点，OpenAI已经开放了其模型评估工具，并要求公众帮助其开发方法来衡量GPT-4和其他模型在不同类型任务上的性能差异。
    

Do you see what a hot mess this whole thing is for anyone who wants to “press pause” on a narrow type of AI research — so-called “gain-of-function” research aimed at making the models more “powerful” — but not all of it?  

对于那些想要 "暂停 "一种狭义的人工智能研究--旨在使模型更加 "强大 "的所谓 "功能增益 "研究--而不是所有的研究的人来说，你是否看到这整件事是多么棘手的麻烦？

1️⃣ We can’t put a straightforward, hard limit on AI research based strictly on the number of parameters and/or GPUs used to train models, because there are other ways to get big jumps in capability apart from scale.  

1️⃣我们不能严格根据用于训练模型的参数和/或GPU的数量，对人工智能研究进行直接的硬性限制，因为除了规模之外，还有其他方法可以获得能力上的大跃进。

2️⃣ Furthermore, if we did outlaw purely scale-based approaches, then we must be 100 percent sure the law is enforced **globally with zero exceptions**.  

Because in the regions with anti-scaling laws, researchers will uncover more ways to advance without relying on scale, and when those new discoveries are transferred to a region where scaling is still being done they could **contribute to a fast take-off scenario**.  

2️⃣此外，如果我们真的将纯粹的基于规模的方法列为非法，那么我们必须百分之百地确保该法律在全球范围内执行，没有例外。因为在有反规模法的地区，研究人员会发现更多不依赖规模的进步方式，而当这些新发现被转移到仍在进行规模化的地区时，它们可能会促进快速起飞的情况。

**Some definitions** for the uninitiated:  

为不熟悉的人提供一些定义：

-   **Fast take-off** scenarios are where something happens and we suddenly go from the present state of the art to a godlike AGI, with no time to prepare in anyway way.  
    
    快速起飞的情况是，发生了一些事情，我们突然从目前的技术状态变成了一个神一样的AGI，没有时间以任何方式准备。
    
-   **Slow take-off**, in contrast, is where progress is steady and predictable for some lengthy period of time, and we’re able to collectively adjust our society to new AI capabilities as they open up.  
    
    相比之下，慢速起飞是指在一段较长的时间内，进展是稳定和可预测的，我们能够集体调整我们的社会，以适应新的人工智能能力，因为它们开放了。
    

3️⃣ And how would we keep researchers from making small, incremental advances in capability, advances of the type that could be combined together to get one big advance?  

3️⃣而我们如何防止研究人员在能力方面取得小的、渐进的进展，这些进展可以结合在一起，获得一个大的进展？  

There is no way to regulate this kind of activity, at least that isn’t massively invasive.  

没有办法监管这种活动，至少没有大规模的入侵性。

4️⃣ Finally, we’re still trying to understand how much more capable GPT-4 is than GPT-3, and at what kinds of tasks.  

The red-teaming and testing OpenAI has been doing since the model was finished this summer was **just the beginning** of that process.  

4️⃣最后，我们仍在试图了解GPT-4比GPT-3的能力有多大，以及在什么样的任务上。自今年夏天模型完成以来，OpenAI一直在进行的红队和测试只是这个过程的开始。

What’s happening right now, with hundreds of millions of users poking at the model posting screenshots, or otherwise giving feedback, is the **next phase** of that discovery and measurement process. That’s what’s going on with [OpenAI Evals](https://github.com/openai/evals) — they want the public to contribute high-quality capability measurements.  

现在正在发生的事情，随着数以亿计的用户对模型的探究，发布截图，或以其他方式提供反馈，是该发现和测量过程的下一个阶段。这就是OpenAI Evals正在进行的工作--他们希望公众能够贡献高质量的能力测量。

➡️ Ultimately, the letter’s central demand boils down to a demand that _we stop making AIs more powerful until we can figure out how to measure if the AIs are becoming more powerful_. It’s like saying, “stop making cars that can go faster until we can figure out what speed is and how to measure it.” The only way this demand is not completely absurd is if it’s a demand to pause _all_ development.  

➡️ 归根结底，这封信的核心要求归结为我们停止制造更强大的人工智能，直到我们能够弄清楚如何衡量人工智能是否变得更强大。这就好比说，"在我们能够弄清楚什么是速度以及如何测量它之前，停止制造可以跑得更快的汽车。"这个要求不完全荒谬的唯一方法是，它是一个暂停发展的要求。

You might counter by insisting that it’s just a demand to stop explicitly trying to improve AI capabilities and just focus on explainability and measurement.  

But my brother in technology: explainability and measurement are _critical_ to gain-of-function work. As the meme says, they’re the same picture.  

你可能会反驳说，这只是要求停止明确地试图提高人工智能的能力，而只是专注于可解释性和测量。但我的技术兄弟：可解释性和测量对于功能增益的工作是至关重要的。正如备忘录所说，它们是同一幅画。

At any rate, the letter is not, in fact, a call to pause all development, and is exactly as self-contradictory as my automotive analogy suggests.  

无论如何，这封信事实上并不是呼吁暂停所有的发展，而且正如我的汽车类比所暗示的那样，完全是自相矛盾的。

🚔 After asking for a pause in gain-of-function work, it then goes on to suggest that we institute a regime of inspection, testing, and reporting to verify quantitatively that as everyone is doing their work on the cars nobody is secretly doing the bad thing of making the cars go faster — again, all in the absence of a widely agreed upon definition of speed, much less a means of testing it.   

🚔在要求暂停功能增益工作之后，它又建议我们建立一个检查、测试和报告制度，以定量地核实每个人都在对汽车进行工作，没有人暗中做坏事，使汽车跑得更快--同样，所有这些都缺乏一个广泛认同的速度定义，更没有一个测试的手段。

> _In parallel, AI developers must work with policymakers to dramatically accelerate development of robust AI governance systems. These should at a minimum include: new and capable regulatory authorities dedicated to AI; oversight and tracking of highly capable AI systems and large pools of computational capability; provenance and watermarking systems to help distinguish real from synthetic and to track model leaks; a robust auditing and certification ecosystem; liability for AI-caused harm; robust public funding for technical AI safety research; and well-resourced institutions for coping with the dramatic economic and political disruptions (especially to democracy) that AI will cause.  
> 
> 同时，人工智能开发者必须与政策制定者合作，大幅加快开发强大的人工智能治理系统。这些至少应该包括：专门针对人工智能的新的和有能力的监管机构；监督和跟踪高能力的人工智能系统和大型计算能力池；出处和水印系统，以帮助区分真实和合成，并跟踪模型泄漏；强大的审计和认证生态系统；对人工智能造成的伤害承担责任；为人工智能安全技术研究提供强大的公共资金；以及资源丰富的机构，以应对人工智能将导致的巨大的经济和政治破坏（特别是对民主）。_

To use technical jargon: this is all pretty nuts. Just completely unserious, and I’m shocked at some of the names that signed onto it.  

用技术术语来说：这一切都很疯狂。只是完全不严肃，我对一些签名的人感到震惊。

In fact, after thinking this situation through, now _I’m_ getting scared — the future of our species is apparently in the hands of a group of people who can’t spot an obvious contradiction of this magnitude.  

It’s almost enough to make me look at that list of signatories and think, ironically, “there go the techbros playing God, again!”  

事实上，在仔细考虑了这种情况后，现在我开始害怕了--我们这个物种的未来显然掌握在一群人手中，他们不能发现这种程度的明显矛盾。这几乎足以让我看着那份签署者名单，并讽刺地想，"科技人员又在扮演上帝了！"

But that would be a culture war frame, and as such it’s one I think we should reject when grappling with this issue. It’s best to take AI safety on its own terms.  

但这将是一个文化战争的框架，因此我认为我们在处理这个问题时应该拒绝这个框架。最好的办法是根据自己的条件来看待人工智能的安全。

When trying to navigate the AI safety debate by mapping the technical issues described above to different tribal groups on my timeline, I find I’m constantly disoriented.  

当我试图通过将上述技术问题映射到我时间线上的不同部落群体来浏览人工智能安全辩论时，我发现我一直在迷失方向。  

So it’s easy for me to see why outsiders are lost with this whole thing.   

所以我很容易理解为什么外人对这整个事情感到迷茫。

A small but telling **recent example:** in this [NY Mag puff piece](https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html) on self-styled “AI hype” debunker Emily Bender, the author thinks Sam Altman is an effective altruist but he’s actually effective altruism’s current number one villain.  

I’ve seen people make this same categorization mistake with Peter Thiel, a man who has [unironically characterized](https://www.youtube.com/watch?v=ibR_ULHYirs) EA guru Nick Bostrom as the antichrist.  

最近有一个很小但很有说服力的例子：在《纽约杂志》关于自称 "人工智能炒作 "揭秘者艾米丽-班德的这篇吹捧文章中，作者认为萨姆-奥特曼是一个有效的利他主义者，但他实际上是有效利他主义的头号恶棍。我看到人们在彼得-泰尔身上犯了同样的分类错误，这个人曾不无讽刺地将EA大师尼克-波斯特罗姆定性为反基督者。

😵💫 But I do get why everyone is confused. The tribal signifiers on AI safety Twitter are all over the place.  

Just today, in fact, the rationalist, evopsych, gender critical scholar Geoffrey Miller was [backing woke AI ethicist Gary Marcus](https://twitter.com/primalpoly/status/1641148286521180173) in a thread on the AI letter, and in opposition to both was an [Antifa, tankie, he/him account](https://twitter.com/tante/status/1641175492282793984) who’s worried about the anti-democratic implications of a technocrat-run AI control regime… and off to the side is me, an anti-woke, anti-tankie, pronoun disrespecter cheering he/him on because this aggression cannot stand, man.  

😵💫 但我确实明白为什么大家都很困惑。人工智能安全推特上的部落标志到处都是。事实上，就在今天，理性主义、进化心理学、性别批判学者杰弗里-米勒（Geoffrey Miller）在关于人工智能信件的主题中支持清醒的人工智能伦理学家加里-马库斯（Gary Marcus），而反对这两者的是一个反法西斯、坦克兵、他/他的账户，他担心技术官僚管理的人工智能控制制度的反民主影响...而在一边的是我，一个反清醒、反坦克兵、不尊重人称的人，在为他/他加油，因为这种侵略行为不能忍受，伙计。

**Another example:** If I were at a party with e/acc and EA types on opposite sides of the AI x-risk issue, I wouldn’t be able to tell who was who just by looking at normal exterior signals or even asking non-AI-related questions.  

The two groups are very similar along most tribal vectors. But In an effective accelerationism (shortened to “e/acc”) Twitter space a while back, the opening questions were, _why haven’t the EA/rationalist x-riskers killed any AI researchers or accelerationists yet_, and _when are they going to start doing that?_  

另一个例子：如果我在一个有e/acc和EA类型的人参加的聚会上，处于人工智能X风险问题的对立面，光看正常的外部信号，甚至问与人工智能无关的问题，我都无法分辨谁是谁。这两个群体沿着大多数部落的向量非常相似。但在前一阵子的有效加速主义（简称 "e/acc"）推特空间里，开篇的问题是，为什么EA/理性主义的x风险者还没有杀死任何AI研究者或加速者，他们什么时候开始这么做？

I mean, we now have the leader of one whole wing of x-risk discourse calling for airstrikes on datacenters:  

我的意思是，我们现在有一整翼的X-风险言论的领导人呼吁对数据中心进行空袭：

So it’s a free-for-all out there. A veritable state of nature. But there are a few broad camps emerging.  

因此，那里是一个自由竞争的地方。一个名副其实的自然状态。但有几个大的阵营正在出现。

The Safetyists are people who express some degree of worry about “AI safety,” even though they have very different ways of framing the issue. Such people fall into roughly three camps:  

安全主义者是对 "人工智能安全 "表示某种程度担忧的人，尽管他们对这个问题有非常不同的表述方式。这类人大致分为三个阵营：

1.  **The language police:** Worried that LLMs will say mean words, be used to spread disinformation, or be used for phishing attempts or other social manipulation on a large scale. AI ethicist [Gary Marcus](https://twitter.com/GaryMarcus) is in this camp, as are most “disinfo” and DEI advocacy types in the media and academia who are not deep into AI professionally but are opining about it.  
    
    语言警察:担心LLM会说一些刻薄的话，被用来传播虚假信息，或者被用来进行网络钓鱼尝试或其他大规模的社会操纵。人工智能伦理学家加里-马库斯属于这个阵营，媒体和学术界大多数 "假信息 "和DEI倡导者也属于这个阵营，他们在专业上没有深入研究人工智能，但却在对其发表评论。
    
2.  **The Chernobylists:** Worried about what will happen if we hook ML models we don’t fully understand to real-life systems, especially critical ones or ones with weapons on them. [David Chapman](https://betterwithout.ai/) is in this camp, as am I.   
    
    切尔诺贝利主义者：担心如果我们将我们不完全理解的ML模型与现实生活中的系统挂钩，特别是关键系统或有武器的系统会发生什么。 大卫-查普曼和我一样，都属于这个阵营。
    
3.  **The x-riskers:** Absolutely convinced that the moment an AGI comes on the scene, humanity is doomed. [Eliezer Yudkowsky](https://twitter.com/ESYudkowsky) is the most prominent person in this camp, but there are many others in [rationalist and EA circles](https://lesswrong.com/) who fall into it.  
    
    冒风险者：绝对相信AGI出现的那一刻，人类就注定要失败。 埃利泽-尤德考夫斯基（Eliezer Yudkowsky）是这个阵营中最突出的人，但在理性主义和EA圈子里还有很多人属于这个阵营。
    

👮♀️ The language police and the x-riskers are longstanding culture war enemies.  

👮♀️ 语言警察和X-Riskers是长期的文化战争敌人。  

Neither has much of a care for the other’s specific concerns about AI — language police think arguments about AI killing us all via nanobots are dumb, and x-riskers think worries about LLMs being coaxed into printing a racial stereotype are dumb.  

两者都不太关心对方对人工智能的具体担忧--语言警察认为关于人工智能通过纳米机器人杀死我们所有人的论点是愚蠢的，而X-风险者认为对LLM被哄骗着打印种族刻板印象的担忧是愚蠢的。  

Nonetheless, these two rival camps are temporarily finding common ground on the cause of regulating or slowing AI.  

尽管如此，这两个对立的阵营在监管或减缓人工智能的事业上暂时找到了共同点。

⛑️ The Chernobylists don’t have much interest in either of the concerns of the other two camps — for us (as I said, I’m one), toxic language concerns are picayune, fears of mass disinfo campaigns are overblown, and the x-risk stuff is just sci-fi nonsense.  

⛑️ 切尔诺贝利主义者对其他两个阵营的担忧都没有太大的兴趣--对我们来说（正如我说的，我是其中之一），对有毒语言的担忧是轻描淡写的，对大规模造谣运动的担忧是夸大其词的，而X-风险的东西只是科幻的胡说八道。  

No, we’re worried that somebody is going to deploy a system in some production context where it can cause an accident that kills a lot of people — either a spectacular industrial accident or a slow-rolling, mostly invisible catastrophe of the type that might be downstream of a medical LLM that screws up certain types of drug interactions.  

不，我们担心的是有人会在某些生产环境中部署一个系统，在那里它可能会导致一个杀死很多人的事故--要么是一个壮观的工业事故，要么是一个缓慢滚动的、大部分看不见的灾难，这种类型的灾难可能是一个医学法学硕士的下游，搞砸了某些类型的药物相互作用。

Note that we Chernobylists differ widely on how to address the scenarios we’re worried about.  

请注意，我们切尔诺贝利主义者在如何解决我们所担心的情况上有很大分歧。  

Chapman wants to see AI development slow down, whereas I’m an accelerationist who thinks the answer is to keep building and use AI to find ways to mitigate the chaos that AI progress itself creates.  

查普曼希望看到人工智能发展放缓，而我是一个加速派，认为答案是继续建设，并利用人工智能找到缓解人工智能进步本身所造成的混乱的方法。  

This is a debate for another day, though.  

不过这是另一天的辩论。

👵🏻 The boomers are not technically baby boomers — they’re just people who are still in the very first step of the two-step process I described in the prelude, i.e., they’re cramming all this AI stuff into existing culture war frames based on the tribal signals the different participants in the brawl are throwing off as they go by in the melee.  

👵🏻 严格来说，婴儿潮一代并不是婴儿潮--他们只是仍处于我在序言中描述的两步进程的第一步的人，即他们根据争吵中不同参与者在混战中经过时抛出的部落信号，将所有这些人工智能的东西塞进现有的文化战争框架。

In my own experience, there are two subgroups of AI Boomers: the red tribe and the blue tribe. Their views on AI break down roughly as follows:  

根据我自己的经验，人工智能潮人有两个亚群：红色部落和蓝色部落。他们对人工智能的看法大致细分如下：

**🔵 Blue tribe:  

🔵蓝色部落：**

-   Sam Altman, Peter Thiel, and Elon Musk are capitalists and longtermists, therefore they’re bad because capitalism bad and longtermism bad.   
    
    萨姆-奥特曼、彼得-蒂尔和埃隆-马斯克是资本家和长期主义者，因此他们是坏人，因为资本主义不好，长期主义不好。
    
-   The AIs that have been unleashed by capitalists and BigCos are screwing up all of plans to sell our art and short fiction, which is bad because indie artists are good and capitalist BigCos are bad.  
    
    被资本家和BigCos释放出来的AI正在破坏所有销售我们的艺术和短篇小说的计划，这很糟糕，因为独立艺术家是好的，而资本家BigCos是坏的。
    
-   AI smells like a new tool of right-wing capitalist power for controlling the masses.  
    
    人工智能闻起来像是右翼资本主义权力控制大众的新工具。
    

**🔴 Red tribe:  

🔴 红色部落：**

-   Sam Altman, Peter Thiel, and Elon Musk are sexual degenerates and longtermists, therefore they’re bad because sexual degeneracy bad and longtermism bad. (Everyone hates the longtermists!)  
    
    萨姆-奥特曼、彼得-蒂尔和埃隆-马斯克是性堕落者和长期主义者，因此他们是坏的，因为性堕落者坏，长期主义者坏。 每个人都讨厌长期主义者！）。
    
-   AI is a creation of BigTech, BigTech is woke, ChatGPT is woke, woke is bad, therefore AI is bad by transitivity.  
    
    人工智能是BigTech的创造，BigTech是清醒的，ChatGPT是清醒的，清醒的是坏的，因此根据反证法，人工智能是坏的。
    
-   So much boobs and waifu and porn in every AI art FB group.  
    
    每一个人工智能艺术FB小组中都有这么多的胸部、腰部和色情。
    
-   It ain’t human, which means it’s either angelic or demonic, and it ain’t angelic (see above re: sexual degeneracy), so…  
    
    它不是人类，这意味着它要么是天使，要么是恶魔，而它不是天使（见上文关于性堕落），所以......
    
-   AI smells like a new tool of woke capitalist power.  
    
    人工智能闻起来像一个醒目的资本主义权力的新工具。
    

There is a ton of overlap between Blue Tribe Boomers and Language Police, but I consider the Language Police a separate group because they’re situated in media and academia and have sinecures where they’re paid to tell everyone to stop using certain words and phrases.  

蓝色部落潮人和语言警察之间有大量的重叠，但我认为语言警察是一个独立的群体，因为他们位于媒体和学术界，并有一份工资，告诉大家停止使用某些词汇和短语。

I’ve saved the most interesting (to me) camp for last. These people are way off in their own, largely isolated wing of the AI wars. This camp is also centered on a specific clique.  

我把最有趣的（对我来说）阵营留到最后。这些人在他们自己的、基本上是孤立的人工智能战争的一翼中走得很远。这个阵营也以一个特定的小集团为中心。  

I will name two of the names in that clique in a moment, but first I have to lay out their position.  

我一会儿会说出这个集团中的两个名字，但首先我必须阐述他们的立场。

🧠 Let’s say you’re someone who believes the concept of **IQ is a racist, eugenicist** **fiction**.  

In your view, any attempt to sort people into groups based on some measurement of intelligence — an IQ test, the SAT or ACT, the MCAT, LSAT, GRE, whatever — is definitionally part of a colonialist, eugenicist project that marginalizes indigenous ways of knowing and being, and seeks to reify a white supremacist ideology that views human intellectual capacity as measurable and quantifiable.  

假设你是一个认为智商的概念是种族主义的、优生主义的虚构。在你看来，任何试图根据某种智力测量方法将人们分成不同的群体--智商测试、SAT或ACT、MCAT、LSAT、GRE等--从定义上讲都是殖民主义、优生主义项目的一部分，它将本土的认识和存在方式边缘化，并试图重新证明白人至上主义的意识形态，将人类智力能力视为可衡量和可量化的。

If you are one of these people, then how are you going to relate constructively to any important human endeavor that advertises itself as having “intelligence” in the name?  

如果你是这些人中的一员，那么你将如何建设性地与任何标榜自己有 "智慧 "的重要人类努力联系起来？  

How can you contribute to the kind of high-stakes, polarizing, collective problem-solving efforts around how to quantify machine intelligence described in the technical section?  

你如何为技术部分中描述的那种围绕如何量化机器智能的高风险、两极化、集体解决问题的努力做出贡献？

🫢 The answer is that it’s going to be quite difficult to position yourself in any conversation that moves beyond very basic Language Police concerns.  

答案是，在任何超越语言警察基本关切的对话中，要给自己定位是相当困难的。  

The minute the discussion moves out of the realm of naughty words and into the realm of ML models with dangerous, superhuman capabilities that we don’t understand and can’t control, you have to hit eject.  

一旦讨论脱离了调皮话的范畴，进入了我们不了解也无法控制的具有危险的超人能力的ML模型的领域，你就必须点击弹出。  

This is why the Intelligence Deniers are isolated from the rest of the safety discourse.  

这就是为什么 "情报否认者 "被孤立于其他安全论述之外。

The fundamental premises of the Intelligence Denier clique and their hangers-on in the media and academia are something like the following:  

情报否认者集团及其在媒体和学术界的追随者的基本前提是如下内容：

1.  “Artificial intelligence” is a capitalist marketing term for statistical smoke-and-mirrors. There is no real innovation there and it’s **mostly just a scam**.  
    
    "人工智能 "是一个资本主义营销术语，是统计学上的烟幕弹。那里没有真正的创新，它主要是一个骗局。
    
2.  The **models are just math** and are not “intelligent” because “intelligence” is a colonialist cisheteropatriarchal white supremacist eugenicist construct.  
    
    这些模型只是数学，并不是 "智能"，因为 "智能 "是殖民主义的顺向异性恋的白人至上主义的优生学构造。
    
3.  To the extent that the models are made to act in some human-like fashion, this is a **dangerous, dehumanizing** effort by our enemies to effect the aforementioned capitalist smoke-and-mirrors strategy for profit at the expense of the marginalized and minoritized.   
    
    如果让这些模特儿以某种类似人类的方式行动，这就是我们的敌人为实现上述资本主义的烟幕弹战略而做出的危险的、非人化的努力，以牺牲边缘化和少数人的利益为代价。
    
4.  **Gary Marcus** and other white dudes who are trying to be allies and do “AI ethics” alongside us need to shut up and just center us and signal boost us.  
    
    (That Gary Marcus should shut his trap is surely the only thing AI-related that Emily Bender and Yann Lecun can agree on.)  
    
    加里-马库斯和其他试图成为盟友并与我们一起做 "人工智能伦理 "的白人帅哥需要闭嘴，只需以我们为中心，给我们发信号。(加里-马库斯应该闭上他的嘴，这肯定是艾米丽-本德和扬-勒库恩唯一能同意的与人工智能有关的事情。）
    
5.  If you’re proposing technical solutions to the ML problems we’ve identified (e.g., it spits out stereotypes, it doesn’t reliably recognize non-white faces, it is used by the police in any capacity for any reason, it does any kind of profiling or evaluating of humans in a way that might disadvantage some who are marginalized), then you are doing a “**techno-solutionism**” and that is absolutely not allowed. The only correct answer is to completely reform all of society and dismantle all interlocking systems of oppression.  
    
    Your GitHub pull requests and technical proposals are attempts to maintain the status quo. Much more on this, [here](https://www.jonstokes.com/p/understanding-the-role-of-racist).  
    
    如果你对我们发现的ML问题提出技术解决方案（例如，它吐出的是刻板印象，它不能可靠地识别非白人面孔，它被警察以任何身份以任何理由使用，它以一种可能使一些被边缘化的人处于不利地位的方式对人类进行任何形式的定性或评估），那么你正在做一个 "技术解决方案"，这是绝对不允许的。唯一正确的答案是彻底改革整个社会，拆除所有环环相扣的压迫系统。你的GitHub拉动请求和技术建议是维持现状的尝试。关于这一点的更多信息，请看 。
    
6.  There’s a bunch of stuff about **ableism** that I don’t have the time or energy to get into, but basically, if you say the models “hallucinate” then that is ableist language that marginalizes the mentally ill (I am not making this up).  
    
    They make these and similar arguments that anyone who’s been on Twitter long enough can GPT given the right prompt.  
    
    有一堆关于能力主义的东西，我没有时间或精力去研究，但基本上，如果你说模型 "产生幻觉"，那么这就是能力主义的语言，将精神病患者边缘化（这不是我编的）。他们提出了这些和类似的论点，只要有正确的提示，任何在Twitter上呆得够久的人都可以GPT。
    

(When this newsletter first started in 2021, I spent a bunch of time studying these people and writing about them, so if you scroll back in [my archives](https://www.jonstokes.com/archive) you can find as many of my takes on them as you have time to read.)  

(当这份通讯在2021年首次开始时，我花了大量时间研究这些人，并写了关于他们的文章，所以如果你在我的档案中滚动，你可以找到我对他们的看法，只要你有时间阅读。）

If the above list is the way you’re approaching the AI debate, then what is your reaction going to be to an open letter promoted by Gary Effing Marcus that suggests AIs are becoming too intelligent, too quickly?  

Well, let’s see what one member of the Intelligence Denier clique, **Emily Bender**, has to say:  

如果上述清单是你对待人工智能辩论的方式，那么你对Gary Effing Marcus推动的一封公开信会有什么反应，该信认为人工智能正在变得过于智能，速度太快？好吧，让我们看看 "智力否认者 "集团的一个成员，艾米丽-本德，是怎么说的：

🤬 So yeah, she _hated_ the letter.  

She hates Marcus (the two of them beef with each other on Twitter), she hates the capitalist “techbros” who are investing in and building in AI and who’ve signed that letter, she hates GPT-4 and thinks it’s a scam, she hates Sam Altman… she’s just extremely mad all the time about anything that looks like AI progress.  

What can I say, tho, other than that God’s punishment on such people is that they have to be who they are?  

所以，是的，她讨厌那封信。她讨厌马库斯（他们两人在推特上互相争吵），她讨厌那些投资和建设人工智能并在信上签名的资本主义 "技术男"，她讨厌GPT-4，认为它是一个骗局，她讨厌山姆-奥特曼......她只是对任何看起来像人工智能进步的东西一直都非常生气。我还能说什么呢，除了上帝对这种人的惩罚就是他们必须成为他们自己？

**Timnit Gebru** is another senior member of this clique. I have [written about her before](https://www.jonstokes.com/p/googles-colosseum) in 2021. If you’ve followed my coverage of her, you know I credit her with raising some important issues in AI ethics, but lately, she’s completely gone off the deep end with elaborate, genealogical conspiracy theories about how the quest for AGI is a literal actual eugenics project.  

蒂姆尼特-格布鲁斯是这个集团的另一个高级成员。我以前在2021年写过关于她的文章。如果你关注过我对她的报道，你知道我认为她提出了一些人工智能伦理方面的重要问题，但最近，她完全走入了深渊，提出了精心设计的家谱式阴谋论，说对AGI的追求是一个字面上的实际优生项目。

<iframe src="https://www.youtube-nocookie.com/embed/P7XT4TWLzJw?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409" data-immersive-translate-effect="1"></iframe>

I’m not going to get into any of that, because if that stuff is your bag then why on earth are you reading my newsletter?   

我不打算讨论这些，因为如果这些东西是你的囊中之物，那么你到底为什么要读我的通讯？

There are other members of this clique — a few more senior members, and a few junior members — but I’m restricting my coverage here to Bender and Gebru because they’re the most prominent and have consistently put themselves out there publicly.  

这个集团还有其他成员--一些更高级的成员，以及一些初级成员--但我在这里的报道仅限于本德和格布鲁，因为他们是最突出的，而且一直公开地把自己放在那里。

If you haven’t guessed by now, the Intelligence Deniers are people who’ve taken the “tweet through it!” path after grappling with AI safety concerns.  

如果你现在还没有猜到，智能否认者是那些在努力解决人工智能安全问题后采取 "通过推特！"道路的人。  

They’re close enough to the issue to understand the debate, but they are just not going to budge on any of their existing idpol positions to make room for a new set of allies, some of whom (the rationalists) are decidedly “problematic.”  

他们离这个问题很近，能够理解辩论，但他们就是不打算在他们现有的偶像派立场上让步，为一组新的盟友腾出空间，其中一些人（理性主义者）显然是 "有问题的"。

At any rate, this camp seems destined to shrink through infighting and general toxicity until they drive their allies into the Language Police camp. You hate to see it.  

无论如何，这个阵营似乎注定要通过内讧和普遍的毒害而萎缩，直到他们把他们的盟友赶到语言警察阵营。你讨厌看到这一点。

📈 The other thing working against this crew is that **the models just keep getting more capable** and, well, intelligent.  

So if you’re committed on principle to the stance that there is no real innovation going on here, it’s all capitalist flimflammery, intelligence is a racist lie, and so on, you’re destined to migrate further to the fringes as AI keeps posting dramatic wins.  

另一件不利于这批人的事情是，这些模型的能力越来越强，而且，越来越聪明。因此，如果你原则上坚持认为这里没有真正的创新，都是资本主义的花架子，智能是种族主义的谎言，等等，那么随着人工智能不断取得戏剧性的胜利，你就注定要进一步迁移到边缘地带。

For those who haven’t read Dune, the Butlerian Jihad was a crusade against any kind of computer or machine whose function approximates what the human mind can do — this included everything from artificial intelligence to pocket calculators.   

对于那些没有读过《沙丘》的人来说，"巴特勒圣战 "是对任何一种功能与人类思维相近的计算机或机器的讨伐--这包括从人工智能到袖珍计算器的一切。

🔫 I think most of the camps and subgroups described in this article are going to converge into a type of real-life Butlerianism.   

我认为这篇文章中描述的大多数阵营和分组都将汇聚成一种现实生活中的巴特勒主义。

The Intelligence Deniers are already fully Butlerian.  

They don’t take any of the nonlinguistic AI safety concerns seriously, but they still want to crusade against AI, so they’ve adopted a quasi-mystical of ineffable humaneness that you can read all about in the [NY Mag’s Bender fanfic](https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html).  

拒绝智能的人已经完全是巴特勒式的了。他们不把任何非语言的人工智能安全问题当回事，但他们仍然想讨伐人工智能，所以他们采用了一种不可言喻的人性的准神秘主义，你可以在《纽约杂志》的班德粉丝小说中读到这一切 。

The Boomers and most of the AI Safetyists are going to join the Intelligence Deniers in their Butlerianism either because they have their own metaphysical critiques of AI, or out of convenience because they agree with the ultimate goal of slowing or stopping AI progress.  

Boomers和大多数人工智能安全主义者将加入智能否认者的管家主义，要么是因为他们对人工智能有自己的形而上学批判，要么是出于方便，因为他们同意减缓或阻止人工智能进展的最终目标。

🏎️ If you’re not a Butlerian, then you’ll be an effective accelerationist (e/acc). This latter camp is the one I’m in, and we are increasingly (it seems) outnumbered.   

🏎️ 如果你不是一个巴特勒主义者，那么你将是一个有效的加速主义者（e/acc）。这后一个阵营就是我所在的阵营，而我们的人数越来越多（似乎）。

You may disagree with me that Butlerianism and e/acc are the only two minima that everyone is going to sort into, but you’d be wrong. So you might as well get to know the sides so you can pick one.  

你可能不同意我的观点，认为巴特勒主义和e/acc是每个人都要分类的唯一两个最小值，但你会错的。所以你不妨了解一下双方的情况，这样你就可以选一个。

On the topic of scaling laws and GPT-4’s parameter count, Anton Troynikov speculated in a recent interview with me that OpenAI did not release GPT-4’s parameter count because it may be the same or even smaller than GPT-3’s parameter count, and if so that would be a clue that they’ve made proprietary advances (i.e., something beyond just turning up the scaling knob another notch) and don’t want competitors to know that information.  

关于缩放法则和GPT-4的参数数的话题，Anton Troynikov在最近接受我的采访时推测，OpenAI没有公布GPT-4的参数数，因为它可能与GPT-3的参数数相同甚至更少，如果是这样，这将是一个线索，他们已经取得了专有的进展（也就是说，不仅仅是把缩放旋钮再调高一个档次），并且不希望竞争对手知道这些信息。

The more I consider Anton’s speculation, the more I’m convinced it’s correct.  

我越是考虑安东的推测，就越是相信它是正确的。  

GPT-4 is probably a smaller model than GPT-3 or GPT-3.5, and the point in releasing it this way is to understand the impact of the other advances they’ve made before they go back to turning up the scaling knob with GPT-5 and possibly rocket all the way into an entirely new regime that looks a lot more like a superhuman AGI.  

GPT-4可能是一个比GPT-3或GPT-3.5更小的模型，以这种方式发布的意义在于了解他们所取得的其他进展的影响，然后他们再去转动GPT-5的缩放旋钮，并可能一路火箭般地进入一个全新的制度，看起来更像一个超人的AGI。

Sam has consistently stressed the need for slow takeoff and incrementalism so that humanity can progressively come to grips with more capable models as they develop.  

He says explicitly in the Lex interview that he **fears fast takeoff** and wants to avoid that if at all possible.  

萨姆一直强调需要缓慢的起飞和渐进主义，以便人类能够在发展过程中逐步掌握能力更强的模型。他在Lex采访中明确表示，他害怕快速起飞，如果可能的话，希望避免这种情况。

And as an **addendum to this addendum**, I feel compelled to point out a subtle contradiction in the GPT-4 performance picture Sam presents on that podcast.  

On the one hand, he tells Lex he was not surprised by GPT-4’s performance because it obeys certain laws; on the other, he says all that stuff about tons of incremental gains adding up to one big gain, _and_ he stresses that they’re not merely following the regular scaling law with GPT-4, _and_ he says they need help characterizing how much more powerful it is.  

作为这个附录的补充，我觉得有必要指出Sam在该播客中展示的GPT-4性能图片中的一个微妙的矛盾。一方面，他告诉Lex他对GPT-4的性能并不感到惊讶，因为它遵守了某些规律；另一方面，他说了所有那些关于成吨的递增收益加起来就是一个大收益的东西，他强调他们对GPT-4不仅仅是遵循常规的缩放规律，而且他说他们需要帮助描述它有多强大。

There’s a part of this picture missing, and I don’t know what it is.  

这张照片少了一部分，我不知道它是什么。  

GPT-4’s “intelligence” is either neatly characterized by some “number go up” type law, or it’s hard to measure and is the product of a bunch of small optimizations. But it seems like it can’t be both.  

GPT-4的 "智能 "要么被一些 "数字上升 "类型的规律整齐地描述出来，要么它很难测量，是一堆小优化的产物。但似乎它不可能两者兼得。

If you figure this one out, let me know in the comments.  

如果你想出了这个问题，请在评论中告诉我。
