---
title: "ä½¿ç”¨ç¨³å®šæ‰©æ•£å’ŒLoRAåˆ¶ä½œè‡ªç”»åƒ"
date: 2023-11-10T17:10:43+08:00
updated: 2023-11-10T17:10:43+08:00
taxonomies:
  tags: []
extra:
  source: https://www.shruggingface.com/blog/self-portraits-with-stable-diffusion-and-lora
  hostname: www.shruggingface.com
  author: 
  original_title: "Making Self Portraits With Stable Diffusion and LoRA"
  original_lang: en
---

Over the last few months, products like [Lensa](https://prisma-ai.com/lensa) and [AvatarAI](https://avatarai.me/) have exploded onto the scene, generating artistically rendered portraits and selfies. They range from hyper-realistic photographs that are hard to differentiate from real to highly stylized paintings and illustrations which take a look and feel of a variety of mediums.

As an artist, these products are super interesting to me. I love how they enable and democratize the ability to remix images into different styles. However, using these products feels more like applying an Instagram filter than actual creation. So Iâ€™m left wondering: how do they work? How can you take a few photos and generate something new and unique from them? How can I use these techniques to apply my own unique artistic spin?

In this post, we will explore the easiest way to generate your own custom avatars and selfies, using an open-source image generation model named Stable Diffusion and fine-tuning it with a technique named LoRA (aka. Low-Rank Adaptation).

### Stable Diffusion

[Stable Diffusion](https://huggingface.co/spaces/stabilityai/stable-diffusion) is an open-source image generation AI model, trained with billions of images found on the internet. While there are many advanced knobs, bells, and whistles â€” you can ignore the complexity and make things easy on yourself by thinking of it as a simple tool that does one thing.

Describe what you want to see, then click the generate button! Easy peasy.  

æè¿°ä½ æƒ³çœ‹åˆ°çš„å†…å®¹ï¼Œç„¶åç‚¹å‡»ç”ŸæˆæŒ‰é’®ï¼ç®€å•æ˜“åšã€‚

The easiest way to get started with Stable Diffusion is to head over to [Replicate](https://replicate.com/stability-ai/stable-diffusion?ref=shruggingface.com) and click the submit button.  

å¼€å§‹ä½¿ç”¨Stable Diffusionæœ€ç®€å•çš„æ–¹å¼æ˜¯å‰å¾€Replicateå¹¶ç‚¹å‡»æäº¤æŒ‰é’®ã€‚  

Youâ€™ll notice there are several fields to play with, I suggest sticking to just changing the prompt to get a feel for what you can make.  

ä½ ä¼šæ³¨æ„åˆ°æœ‰å‡ ä¸ªå­—æ®µå¯ä»¥ç©è€ï¼Œæˆ‘å»ºè®®åªæ”¹å˜æç¤ºï¼Œä»¥æ„Ÿå—ä½ å¯ä»¥åšä»€ä¹ˆã€‚

I generated these images using the text `astronaut riding a horse on mars, drawing, sketch, pencil`.  

æˆ‘ä½¿ç”¨æ–‡æœ¬ `astronaut riding a horse on mars, drawing, sketch, pencil` ç”Ÿæˆäº†è¿™äº›å›¾ç‰‡ã€‚  

You can see all of the other parameters and recreate something similar by visiting this Replicate link [https://replicate.com/p/h6h4e3remjcn5i4by35qyzucbe](https://replicate.com/p/h6h4e3remjcn5i4by35qyzucbe)  

ä½ å¯ä»¥é€šè¿‡è®¿é—®è¿™ä¸ªå¤åˆ¶é“¾æ¥https://replicate.com/p/h6h4e3remjcn5i4by35qyzucbeæ¥æŸ¥çœ‹æ‰€æœ‰å…¶ä»–å‚æ•°ï¼Œå¹¶é‡æ–°åˆ›å»ºç±»ä¼¼çš„ä¸œè¥¿ã€‚

At the bottom, there is a `Tweak It` button that will copy all of the same parameters, giving you the ability to `fork` my generation.  

åœ¨åº•éƒ¨ï¼Œæœ‰ä¸€ä¸ª `Tweak It` æŒ‰é’®ï¼Œå®ƒå¯ä»¥å¤åˆ¶æ‰€æœ‰ç›¸åŒçš„å‚æ•°ï¼Œè®©ä½ æœ‰èƒ½åŠ› `fork` æˆ‘çš„ç”Ÿæˆã€‚

![PROMPT: astronaut riding a horse on mars, drawing, sketch, pencil](image.jpeg)

So thatâ€™s all cool, but can it do different styles? You betcha!  

é‚£éƒ½æŒºé…·çš„ï¼Œä½†æ˜¯å®ƒèƒ½åšä¸åŒçš„é£æ ¼å—ï¼Ÿä½ å¯ä»¥è‚¯å®šçš„ï¼  

I generated the following images using the prompt `astronaut riding a horse on mars, painting, impressionistic style, oil, highly detailed`. I added words like `painting, impressionistic style, and oil` to create a result that had a different look and feel.  

æˆ‘ä½¿ç”¨æç¤º `astronaut riding a horse on mars, painting, impressionistic style, oil, highly detailed` ç”Ÿæˆäº†ä»¥ä¸‹å›¾ç‰‡ã€‚æˆ‘æ·»åŠ äº†åƒ `painting, impressionistic style, and oil` è¿™æ ·çš„è¯ï¼Œä»¥åˆ›é€ å‡ºä¸€ç§ä¸åŒçš„å¤–è§‚å’Œæ„Ÿè§‰ã€‚  

Also, hereâ€™s a link for you to inspect/tweak the parameters on your own: [https://replicate.com/p/dc5qehlh3nc6vmttezm3jwreq4](https://replicate.com/p/dc5qehlh3nc6vmttezm3jwreq4)  

å¦å¤–ï¼Œè¿™æ˜¯ä¸€ä¸ªé“¾æ¥ä¾›ä½ è‡ªè¡Œæ£€æŸ¥/è°ƒæ•´å‚æ•°ï¼šhttps://replicate.com/p/dc5qehlh3nc6vmttezm3jwreq4

![PROMPT: astronaut riding a horse on mars, painting, impressionistic style, oil, highly detailed](image.1.jpeg)

This is all great, but because it is trained on a `small` 2-billion-image subset of imagery on the internet, it doesnâ€™t know how to represent many people, places, or things.  

è¿™å¾ˆæ£’ï¼Œä½†æ˜¯å› ä¸ºå®ƒæ˜¯åœ¨äº’è”ç½‘ä¸Šçš„20äº¿å›¾åƒå­é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œæ‰€ä»¥å®ƒä¸çŸ¥é“å¦‚ä½•è¡¨ç¤ºè®¸å¤šäººã€åœ°ç‚¹æˆ–äº‹ç‰©ã€‚  

It does a fantastic job with landscapes, patterns, and such â€” but what happens if I try to generate a `Jake Dahn riding a horse on mars, drawing, sketch, pencil`? It clearly has no idea who I am ğŸ¤£Â [https://replicate.com/p/ixfnjezokrgq7ly23rdhl427cq](https://replicate.com/p/ixfnjezokrgq7ly23rdhl427cq)  

å®ƒåœ¨å¤„ç†é£æ™¯ã€å›¾æ¡ˆç­‰æ–¹é¢åšå¾—éå¸¸å‡ºè‰²â€”â€”ä½†æ˜¯å¦‚æœæˆ‘è¯•å›¾ç”Ÿæˆä¸€ä¸ª `Jake Dahn riding a horse on mars, drawing, sketch, pencil` ä¼šå‘ç”Ÿä»€ä¹ˆå‘¢ï¼Ÿå¾ˆæ˜æ˜¾ï¼Œå®ƒå®Œå…¨ä¸çŸ¥é“æˆ‘æ˜¯è°ğŸ¤£ https://replicate.com/p/ixfnjezokrgq7ly23rdhl427cq

![PROMPT: Jake Dahn riding a horse on mars, drawing, sketch, pencil](image.2.jpeg)

### Fine Tuning Stable Diffusion with LoRA (Low-Resource Adaptation)  

ä½¿ç”¨LoRA (ä½èµ„æºé€‚åº”)è¿›è¡Œç²¾ç»†è°ƒæ•´ç¨³å®šæ‰©æ•£

Awesome, so as demonstrated above, we can pretty quickly generate some neat imagery, but how do I generate images of myself using Stable Diffusion?  

å¤ªæ£’äº†ï¼Œæ­£å¦‚ä¸Šé¢æ‰€ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå¿«ç”Ÿæˆä¸€äº›æ•´æ´çš„å›¾åƒï¼Œä½†æ˜¯æˆ‘è¯¥å¦‚ä½•ä½¿ç”¨ç¨³å®šæ‰©æ•£æ¥ç”Ÿæˆæˆ‘è‡ªå·±çš„å›¾åƒå‘¢ï¼Ÿ

In the AI space, thereâ€™s an emerging mental model pattern where you take a large base model (like Stable Diffusion) and `fine tune` on top of it.  

åœ¨AIé¢†åŸŸï¼Œå‡ºç°äº†ä¸€ç§æ–°çš„æ€ç»´æ¨¡å‹æ¨¡å¼ï¼Œä½ å¯ä»¥åœ¨ä¸€ä¸ªå¤§å‹åŸºç¡€æ¨¡å‹ï¼ˆå¦‚ç¨³å®šæ‰©æ•£ï¼‰çš„åŸºç¡€ä¸Šè¿›è¡Œ `fine tune` ã€‚  

Basically, taking a massively capable off-the-shelf free and open source AI model and customizing them on your own data.  

åŸºæœ¬ä¸Šï¼Œå°±æ˜¯é‡‡ç”¨ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ç°æˆçš„å…è´¹å¼€æºAIæ¨¡å‹ï¼Œå¹¶æ ¹æ®ä½ è‡ªå·±çš„æ•°æ®è¿›è¡Œå®šåˆ¶ã€‚

There are many techniques for performing this sort of fine-tuning.  

æœ‰è®¸å¤šæŠ€æœ¯å¯ä»¥è¿›è¡Œè¿™ç§å¾®è°ƒã€‚  

At the time of writing, the most popular technique is called [DreamBooth](https://dreambooth.github.io/). Thereâ€™s also [Textual Inversion](https://textual-inversion.github.io/), [LoRA](https://huggingface.co/blog/lora), and more!  

åœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼Œæœ€å—æ¬¢è¿çš„æŠ€æœ¯è¢«ç§°ä¸ºDreamBoothã€‚è¿˜æœ‰æ–‡æœ¬åè½¬ï¼ŒLoRAç­‰ç­‰ï¼  

Each technique seems to have somewhat similar results, but the technical decisions and tradeoffs made behind the scenes can actually yield quite different artifacts.  

æ¯ç§æŠ€æœ¯ä¼¼ä¹éƒ½æœ‰äº›è®¸ç›¸ä¼¼çš„ç»“æœï¼Œä½†æ˜¯åœ¨å¹•ååšå‡ºçš„æŠ€æœ¯å†³ç­–å’Œæƒè¡¡å®é™…ä¸Šå¯èƒ½äº§ç”Ÿæˆªç„¶ä¸åŒçš„äº§ç‰©ã€‚

For example, DreamBooth takes a long time to train, and it outputs a large multi-gigabyte `.ckpt` model checkpoint file.  

ä¾‹å¦‚ï¼ŒDreamBoothéœ€è¦å¾ˆé•¿æ—¶é—´æ¥è®­ç»ƒï¼Œå¹¶ä¸”å®ƒè¾“å‡ºä¸€ä¸ªå¤§å‹çš„å¤šå‰å­—èŠ‚çš„æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶ã€‚  

~25-35 minutes to train on an Nvidia T4, and ~10-15min to train on a Nvidia A100. So far, in my experience, DreamBooth gives the best results, but it's also the most resource intensive, and the checkpoint files are kind of painful to work with because they are so large.  

åœ¨Nvidia T4ä¸Šè®­ç»ƒéœ€è¦å¤§çº¦25-35åˆ†é’Ÿï¼Œè€Œåœ¨Nvidia A100ä¸Šè®­ç»ƒéœ€è¦å¤§çº¦10-15åˆ†é’Ÿã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ ¹æ®æˆ‘çš„ç»éªŒï¼ŒDreamBoothæä¾›äº†æœ€å¥½çš„ç»“æœï¼Œä½†å®ƒä¹Ÿæ˜¯æœ€æ¶ˆè€—èµ„æºçš„ï¼Œè€Œä¸”æ£€æŸ¥ç‚¹æ–‡ä»¶å› ä¸ºå¤ªå¤§è€Œæœ‰äº›éš¾ä»¥å¤„ç†ã€‚

LoRA yields similar results as DreamBooth, but it only takes 5-7 minutes to train, and it produces a much more portable 10-15MB `.safetensors` weights file. I've heard they can sometimes balloon up to 200MB, but I haven't seen that happen yet.  

LoRAçš„ç»“æœä¸DreamBoothç›¸ä¼¼ï¼Œä½†å®ƒåªéœ€è¦5-7åˆ†é’Ÿçš„è®­ç»ƒæ—¶é—´ï¼Œè€Œä¸”å®ƒäº§ç”Ÿçš„æƒé‡æ–‡ä»¶æ›´ä¾¿æºï¼Œåªæœ‰10-15MB `.safetensors` ã€‚æˆ‘å¬è¯´å®ƒä»¬æœ‰æ—¶ä¼šè†¨èƒ€åˆ°200MBï¼Œä½†æˆ‘è¿˜æ²¡æœ‰çœ‹åˆ°è¿™ç§æƒ…å†µå‘ç”Ÿã€‚  

In addition to the small file size portability, you can also mix and match multiple LoRA concepts, objects, and styles into a single prompt ğŸ¤¯.  

é™¤äº†å°æ–‡ä»¶å¤§å°çš„ä¾¿æºæ€§ï¼Œä½ è¿˜å¯ä»¥å°†å¤šä¸ªLoRAæ¦‚å¿µã€å¯¹è±¡å’Œé£æ ¼æ··åˆåŒ¹é…åˆ°ä¸€ä¸ªæç¤ºä¸­ğŸ¤¯ã€‚  

This makes it much more akin to a "brush" in a painting app, where you can mix and match different styles and objects to create a unique image.  

è¿™ä½¿å®ƒæ›´åƒæ˜¯ç»˜ç”»åº”ç”¨ä¸­çš„â€œç”»ç¬”â€ï¼Œä½ å¯ä»¥æ··åˆæ­é…ä¸åŒçš„é£æ ¼å’Œç‰©ä½“æ¥åˆ›å»ºä¸€ä¸ªç‹¬ç‰¹çš„å›¾åƒã€‚

That means I can generate a photo of myself in the style of a van Gogh painting, Pixar character, anime character, etc.  

è¿™æ„å‘³ç€æˆ‘å¯ä»¥ç”Ÿæˆä¸€å¼ ä»¥æ¢µé«˜ç”»é£ã€çš®å…‹æ–¯è§’è‰²ã€åŠ¨æ¼«è§’è‰²ç­‰é£æ ¼çš„è‡ªå·±çš„ç…§ç‰‡ã€‚

So without further ado, letâ€™s train some LoRA weights based on photos of me!  

é‚£ä¹ˆï¼Œä¸å†åºŸè¯ï¼Œè®©æˆ‘ä»¬æ ¹æ®æˆ‘çš„ç…§ç‰‡æ¥è®­ç»ƒä¸€äº›LoRAæƒé‡å§ï¼

### 1\. Select Images for your Image Training Set  

ä¸ºä½ çš„å›¾åƒè®­ç»ƒé›†é€‰æ‹©å›¾ç‰‡

Now, before we proceed, I think itâ€™s important to note that at the time of writing this post, training LoRA weights feels more like an art than a science.  

ç°åœ¨ï¼Œåœ¨æˆ‘ä»¬ç»§ç»­ä¹‹å‰ï¼Œæˆ‘è®¤ä¸ºæœ‰å¿…è¦æŒ‡å‡ºï¼Œåœ¨å†™è¿™ç¯‡æ–‡ç« çš„æ—¶å€™ï¼Œè®­ç»ƒLoRAæƒé‡æ›´åƒæ˜¯ä¸€ç§è‰ºæœ¯è€Œä¸æ˜¯ç§‘å­¦ã€‚  

Everything I am sharing is open for interpretation, Iâ€™m not showing you THE way, Iâ€™m showing you A way.  

æˆ‘åˆ†äº«çš„ä¸€åˆ‡éƒ½æ˜¯å¼€æ”¾ç»™ä½ è‡ªç”±è§£è¯»çš„ï¼Œæˆ‘å¹¶ä¸æ˜¯åœ¨å‘ä½ å±•ç¤ºå”¯ä¸€çš„é“è·¯ï¼Œæˆ‘åªæ˜¯åœ¨å‘ä½ å±•ç¤ºä¸€ç§å¯èƒ½çš„é“è·¯ã€‚  

You are the hacker/artist, you do you.  

ä½ æ˜¯é»‘å®¢/è‰ºæœ¯å®¶ï¼Œä½ åšä½ è‡ªå·±ã€‚  

If you have any ideas for improvements I should make, or suggestions of things to try in the future, I would love to hear from you!  

å¦‚æœä½ æœ‰ä»»ä½•æˆ‘åº”è¯¥æ”¹è¿›çš„å»ºè®®ï¼Œæˆ–è€…å¯¹æœªæ¥å°è¯•çš„äº‹æƒ…æœ‰ä»»ä½•å»ºè®®ï¼Œæˆ‘å¾ˆæ„¿æ„å¬å–ä½ çš„æ„è§ï¼

I have read a bunch of advice from strangers on the internet claiming many things you MUST do.  

æˆ‘å·²ç»é˜…è¯»äº†ä¸€å †æ¥è‡ªäº’è”ç½‘ä¸Šé™Œç”Ÿäººçš„å»ºè®®ï¼Œä»–ä»¬å£°ç§°æœ‰å¾ˆå¤šäº‹æƒ…ä½ å¿…é¡»åšã€‚  

Things like â€œyou must use images that have varied lighting and environmental conditions.â€ Advice like â€œyou must limit recurring patterns, or they will appear in every image generation.â€ The Replicate blog post about LoRA also says that itâ€™s `better at style, worse at faces`, but it honestly works pretty well for my face.  

åƒâ€œä½ å¿…é¡»ä½¿ç”¨å…·æœ‰ä¸åŒå…‰ç…§å’Œç¯å¢ƒæ¡ä»¶çš„å›¾åƒã€‚â€è¿™æ ·çš„å»ºè®®ï¼Œâ€œä½ å¿…é¡»é™åˆ¶é‡å¤çš„æ¨¡å¼ï¼Œå¦åˆ™å®ƒä»¬å°†å‡ºç°åœ¨æ¯ä¸€æ¬¡çš„å›¾åƒç”Ÿæˆä¸­ã€‚â€ Replicateåšå®¢ä¸Šå…³äºLoRAçš„æ–‡ç« ä¹Ÿè¯´å®ƒæ˜¯ `better at style, worse at faces` ï¼Œä½†å¯¹æˆ‘æ¥è¯´ï¼Œå®ƒå®é™…ä¸Šå·¥ä½œå¾—ç›¸å½“å¥½ã€‚

So far, in my experience, most of these things donâ€™t seem to matter ğŸ¤·â™‚ï¸  

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œåœ¨æˆ‘çš„ç»éªŒä¸­ï¼Œè¿™äº›äº‹æƒ…å¤§å¤šæ•°ä¼¼ä¹å¹¶ä¸é‡è¦ğŸ¤·â™‚ï¸

Today we will keep it simple by using eight square-cropped transparent PNG photos I took of myself.  

ä»Šå¤©æˆ‘ä»¬å°†é€šè¿‡ä½¿ç”¨æˆ‘æ‹æ‘„çš„å…«å¼ æ–¹å½¢è£å‰ªçš„é€æ˜PNGç…§ç‰‡æ¥ä¿æŒç®€å•ã€‚  

I used Photoshop to quickly crop and cut out the background scenery from my photos.  

æˆ‘ä½¿ç”¨Photoshopå¿«é€Ÿè£å‰ªå¹¶å‰ªæ‰äº†ç…§ç‰‡ä¸­çš„èƒŒæ™¯é£æ™¯ã€‚  

However, to get started quickly, I recommend using the free browser-based tool [birme.net](http://birme.net/) to perform cropping.  

ç„¶è€Œï¼Œä¸ºäº†å¿«é€Ÿå¼€å§‹ï¼Œæˆ‘æ¨èä½¿ç”¨å…è´¹çš„åŸºäºæµè§ˆå™¨çš„å·¥å…·birme.netæ¥è¿›è¡Œè£å‰ªã€‚  

Transparency doesnâ€™t seem to matter all that much, but I havenâ€™t done a rigorous side-by-side experiment yet.  

é€æ˜åº¦ä¼¼ä¹å¹¶ä¸é‚£ä¹ˆé‡è¦ï¼Œä½†æˆ‘è¿˜æ²¡æœ‰è¿›è¡Œä¸¥è°¨çš„å¯¹æ¯”å®éªŒã€‚

I took the following photos with a camera on a tripod in a well-lit room.  

æˆ‘åœ¨ä¸€ä¸ªå…‰çº¿å……è¶³çš„æˆ¿é—´é‡Œï¼Œç”¨æ¶åœ¨ä¸‰è„šæ¶ä¸Šçš„ç›¸æœºæ‹æ‘„äº†ä»¥ä¸‹ç…§ç‰‡ã€‚  

My goal was to capture my face while looking up/down/left/right, both smiling and not smiling.  

æˆ‘çš„ç›®æ ‡æ˜¯åœ¨æˆ‘å‘ä¸Š/ä¸‹/å·¦/å³çœ‹æ—¶æ•æ‰åˆ°æˆ‘çš„è„¸ï¼Œæ— è®ºæˆ‘æ˜¯åœ¨å¾®ç¬‘è¿˜æ˜¯æ²¡æœ‰å¾®ç¬‘ã€‚  

Iâ€™ve read many recommendations about using different lighting, scenes, and environments for these training photos â€” but these simple portraits seem to work reasonably well.  

æˆ‘å·²ç»é˜…è¯»äº†è®¸å¤šå…³äºåœ¨è¿™äº›è®­ç»ƒç…§ç‰‡ä¸­ä½¿ç”¨ä¸åŒçš„ç…§æ˜ã€åœºæ™¯å’Œç¯å¢ƒçš„å»ºè®®â€”â€”ä½†è¿™äº›ç®€å•çš„è‚–åƒä¼¼ä¹å·¥ä½œå¾—ç›¸å½“å¥½ã€‚

![A training set comprised of photos of myself](image.3.jpeg)

### 2\. Prepare Instance Data .ZIP File  

2\. å‡†å¤‡å®ä¾‹æ•°æ® .ZIP æ–‡ä»¶

Now that youâ€™ve selected the photos youâ€™d like to train with, weâ€™ll need to prepare them to be uploaded by compressing them into a `.zip` file.  

ç°åœ¨æ‚¨å·²ç»é€‰æ‹©äº†è¦è¿›è¡Œè®­ç»ƒçš„ç…§ç‰‡ï¼Œæˆ‘ä»¬éœ€è¦å°†å®ƒä»¬å‹ç¼©æˆ `.zip` æ–‡ä»¶ä»¥ä¾¿ä¸Šä¼ ã€‚

On MacOS, you can do this by selecting the image files in `Finder â†’ right click â†’ "Compress"`  

åœ¨MacOSä¸Šï¼Œä½ å¯ä»¥é€šè¿‡åœ¨ `Finder â†’ right click â†’ "Compress"` ä¸­é€‰æ‹©å›¾åƒæ–‡ä»¶æ¥å®ç°è¿™ä¸€æ“ä½œ

![Finder â†’ right click â†’ Compress](image.4.jpeg)

### 3\. Upload and Train  

3\. ä¸Šä¼ å¹¶è®­ç»ƒ

To continue following along, youâ€™ll want to first head over to Replicate and [create an account](https://replicate.com/signin?next=/cloneofsimo/lora-training?ref=shruggingface.com). Replicate is by far the easiest way to train LoRA weights for Stable Diffusion.  

è¦ç»§ç»­è·Ÿéšï¼Œä½ é¦–å…ˆéœ€è¦å‰å¾€Replicateå¹¶åˆ›å»ºä¸€ä¸ªè´¦æˆ·ã€‚Replicateæ— ç–‘æ˜¯è®­ç»ƒStable Diffusionçš„LoRAæƒé‡çš„æœ€ç®€å•æ–¹æ³•ã€‚  

Their web UI allows you to drag and drop your `.zip` file and kick things off in a single form.  

ä»–ä»¬çš„ç½‘é¡µç”¨æˆ·ç•Œé¢å…è®¸ä½ æ‹–æ”¾ä½ çš„ `.zip` æ–‡ä»¶ï¼Œå¹¶åœ¨ä¸€ä¸ªè¡¨å•ä¸­å¯åŠ¨æ‰€æœ‰äº‹æƒ…ã€‚

Head over to [https://replicate.com/cloneofsimo/lora-training](https://replicate.com/cloneofsimo/lora-training?ref=shruggingface.com)  

å‰å¾€https://replicate.com/cloneofsimo/lora-training

Then drag and drop your `.zip` file to the `instance_data` drag+drop form field.  

ç„¶åå°†ä½ çš„ `.zip` æ–‡ä»¶æ‹–æ”¾åˆ° `instance_data` æ‹–æ”¾è¡¨å•å­—æ®µä¸­ã€‚

In the screenshot below, you can see that Iâ€™ve chosen a file named `jake-transparent-3428.zip` for my `instance_data` field.  

åœ¨ä¸‹é¢çš„æˆªå›¾ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°æˆ‘ä¸ºæˆ‘çš„ `instance_data` å­—æ®µé€‰æ‹©äº†ä¸€ä¸ªåä¸º `jake-transparent-3428.zip` çš„æ–‡ä»¶ã€‚  

In general, itâ€™s always helpful to name your Replicate uploads with descriptive file names, as it will help you make sense of what youâ€™re looking at when reviewing results.  

æ€»çš„æ¥è¯´ï¼Œä¸ºä½ çš„Replicateä¸Šä¼ æ–‡ä»¶ä½¿ç”¨æè¿°æ€§çš„æ–‡ä»¶åæ€»æ˜¯æœ‰å¸®åŠ©çš„ï¼Œå› ä¸ºè¿™å°†å¸®åŠ©ä½ åœ¨æŸ¥çœ‹ç»“æœæ—¶ç†è§£ä½ æ­£åœ¨çœ‹çš„å†…å®¹ã€‚

At the time of writing, the `task` field allows you to train for `face`, `object`, and `style`.  

åœ¨æ’°å†™æ—¶ï¼Œ `task` é¢†åŸŸå…è®¸ä½ ä¸º `face` ï¼Œ `object` å’Œ `style` è¿›è¡Œè®­ç»ƒã€‚  

Iâ€™m a little vague on what the underlying difference is between these task templates, but I believe these to be true:  

æˆ‘å¯¹è¿™äº›ä»»åŠ¡æ¨¡æ¿ä¹‹é—´çš„åŸºæœ¬åŒºåˆ«æœ‰äº›æ¨¡ç³Šï¼Œä½†æˆ‘ç›¸ä¿¡è¿™äº›æ˜¯æ­£ç¡®çš„ï¼š

-   `face` - Using the `face` option requires all input images to have a human face and only one per image.  
    
    `face` - ä½¿ç”¨ `face` é€‰é¡¹è¦æ±‚æ‰€æœ‰è¾“å…¥å›¾åƒéƒ½å¿…é¡»æœ‰ä¸€ä¸ªäººè„¸ï¼Œå¹¶ä¸”æ¯å¼ å›¾åƒåªèƒ½æœ‰ä¸€ä¸ªã€‚  
    
    This wonâ€™t work well for pets/animals.  
    
    è¿™å¯¹å® ç‰©/åŠ¨ç‰©æ¥è¯´æ•ˆæœä¸ä½³ã€‚
-   `object` â€” Using the `object` option should work best for objects, animals, pets, etc.  
    
    `object` â€” ä½¿ç”¨ `object` é€‰é¡¹å¯¹äºç‰©ä½“ã€åŠ¨ç‰©ã€å® ç‰©ç­‰åº”è¯¥æ•ˆæœæœ€å¥½ã€‚
-   `style` â€” Using the `style` option works best in transferring the visual style onto other imagery.  
    
    `style` - ä½¿ç”¨ `style` é€‰é¡¹åœ¨å°†è§†è§‰é£æ ¼è½¬ç§»åˆ°å…¶ä»–å›¾åƒä¸Šæ•ˆæœæœ€å¥½ã€‚

Ignore the seed field, and leave the resolution at `512`.  

å¿½ç•¥ç§å­å­—æ®µï¼Œå°†åˆ†è¾¨ç‡ä¿æŒåœ¨ `512` ã€‚

Go ahead and click submit!  

ç»§ç»­å¹¶ç‚¹å‡»æäº¤ï¼

![Replicate.com screenshot of the lora-training model](image.5.png)

The training run will take ~7 minutes to complete. Grab some coffee â˜•Â or tea ğŸµ.  

è®­ç»ƒè¿è¡Œå°†éœ€è¦å¤§çº¦7åˆ†é’Ÿæ‰èƒ½å®Œæˆã€‚å»æ‹¿äº›å’–å•¡â˜•æˆ–èŒ¶ğŸµå§ã€‚  

Youâ€™ll start to see output scrolling by that looks like this:  

ä½ ä¼šå¼€å§‹çœ‹åˆ°åƒè¿™æ ·æ»šåŠ¨çš„è¾“å‡ºï¼š

![Screenshot of the output logs](image.6.jpeg)

### 4\. Downloading Your Weights  

4\. ä¸‹è½½æ‚¨çš„æƒé‡

ğŸ‰Â Awesome, your weights are done training! You should see something like:  

ğŸ‰ å¤ªæ£’äº†ï¼Œä½ çš„æƒé‡å·²ç»å®Œæˆè®­ç»ƒï¼ä½ åº”è¯¥ä¼šçœ‹åˆ°ç±»ä¼¼çš„æƒ…å†µï¼š

![Screenshot of the output download buttons](image.7.jpeg)

The output is a single `.safetensors` LoRA weights file. Click the download button to save the weights somewhere that future-you can find.  

è¾“å‡ºæ˜¯ä¸€ä¸ªå•ä¸€çš„ `.safetensors` LoRAæƒé‡æ–‡ä»¶ã€‚ç‚¹å‡»ä¸‹è½½æŒ‰é’®å°†æƒé‡ä¿å­˜åœ¨æœªæ¥ä½ å¯ä»¥æ‰¾åˆ°çš„åœ°æ–¹ã€‚  

You will also want to save the weights URL by right-clicking the `.safetensors` button and then clicking `Copy Link` â€” make sure to save this URL somewhere readily accessible.  

ä½ ä¹Ÿä¼šæƒ³è¦é€šè¿‡å³é”®ç‚¹å‡» `.safetensors` æŒ‰é’®ï¼Œç„¶åç‚¹å‡» `Copy Link` æ¥ä¿å­˜æƒé‡URL â€”â€” ç¡®ä¿å°†æ­¤URLä¿å­˜åœ¨å®¹æ˜“è®¿é—®çš„åœ°æ–¹ã€‚  

Youâ€™ll be using the URL in the next step.  

ä½ å°†åœ¨ä¸‹ä¸€æ­¥ä¸­ä½¿ç”¨è¿™ä¸ªURLã€‚

Iâ€™ve started saving all of my weights into a Notion DB, where I also track the training dataset, URL to weights, and the weights file itself.  

æˆ‘å·²ç»å¼€å§‹å°†æˆ‘æ‰€æœ‰çš„æƒé‡ä¿å­˜åˆ°Notionæ•°æ®åº“ä¸­ï¼Œæˆ‘åœ¨é‚£é‡Œä¹Ÿè·Ÿè¸ªè®­ç»ƒæ•°æ®é›†ï¼Œæƒé‡çš„URLï¼Œä»¥åŠæƒé‡æ–‡ä»¶æœ¬èº«ã€‚

If youâ€™re anything like me, itâ€™s probably easy to imagine amassing a fairly large/diverse set of LoRA concepts, objects, and styles.  

å¦‚æœä½ å’Œæˆ‘æœ‰äº›ç›¸ä¼¼ï¼Œé‚£ä¹ˆä½ å¯èƒ½å¾ˆå®¹æ˜“æƒ³è±¡ç§¯ç´¯ä¸€å¥—ç›¸å½“å¤§ä¸”å¤šæ ·åŒ–çš„LoRAæ¦‚å¿µã€å¯¹è±¡å’Œé£æ ¼ã€‚  

So saving it somewhere searchable will make life easier in the future.  

æ‰€ä»¥å°†å…¶ä¿å­˜åœ¨æŸä¸ªå¯ä»¥æœç´¢çš„åœ°æ–¹å°†ä¼šä½¿æœªæ¥çš„ç”Ÿæ´»æ›´åŠ è½»æ¾ã€‚  

I also suggest saving the training data zip file in the same place, so you can reproduce similar weights in the future by using next-generation fine-tuning techniques.  

æˆ‘è¿˜å»ºè®®å°†è®­ç»ƒæ•°æ®çš„å‹ç¼©æ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ä½ç½®ï¼Œè¿™æ ·ä½ å¯ä»¥é€šè¿‡ä½¿ç”¨ä¸‹ä¸€ä»£å¾®è°ƒæŠ€æœ¯åœ¨æœªæ¥å¤ç°ç±»ä¼¼çš„æƒé‡ã€‚

![Screenshot my Notion DB, where I store LoRA weights, and their training sets](image.8.png)

### 5\. Generating Images With Custom Stable Diffusion LoRA weights  

5\. ç”Ÿæˆå…·æœ‰è‡ªå®šä¹‰ç¨³å®šæ‰©æ•£LoRAæƒé‡çš„å›¾åƒ

ğŸ¥³Â Awesome, now that youâ€™ve got your `safetensors` weights file, you can use it to generate images that use your face as a reference!  

ğŸ¥³å¤ªæ£’äº†ï¼Œç°åœ¨ä½ å·²ç»å¾—åˆ°äº†ä½ çš„ `safetensors` æƒé‡æ–‡ä»¶ï¼Œä½ å¯ä»¥ç”¨å®ƒæ¥ç”Ÿæˆä½¿ç”¨ä½ çš„è„¸ä½œä¸ºå‚è€ƒçš„å›¾ç‰‡ï¼

On Replicate, if you search public models for â€œLoRAâ€ youâ€™ll find a handful of â€œflavoredâ€ Stable Diffusion models that will play nicely with your LoRA weights. [https://replicate.com/explore?query=lora](https://replicate.com/explore?query=lora)  

åœ¨Replicateä¸Šï¼Œå¦‚æœä½ æœç´¢å…¬å…±æ¨¡å‹ä¸­çš„â€œLoRAâ€ï¼Œä½ ä¼šæ‰¾åˆ°ä¸€äº›â€œæœ‰ç‰¹è‰²â€çš„ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å°†å¾ˆå¥½åœ°é…åˆä½ çš„LoRAæƒé‡ã€‚https://replicate.com/explore?query=lora

These models are all Stable Diffusion or Stable Diffusion pre-trained DreamBooth checkpoints that have been set up on Replicate in a way that lets you add your LoRA weights on top.  

è¿™äº›æ¨¡å‹éƒ½æ˜¯ç¨³å®šæ‰©æ•£æˆ–é¢„è®­ç»ƒçš„ç¨³å®šæ‰©æ•£DreamBoothæ£€æŸ¥ç‚¹ï¼Œå®ƒä»¬å·²ç»åœ¨Replicateä¸Šè®¾ç½®ï¼Œè®©ä½ å¯ä»¥åœ¨é¡¶éƒ¨æ·»åŠ ä½ çš„LoRAæƒé‡ã€‚

Each of these models has the following two required parameters:  

è¿™äº›æ¨¡å‹æ¯ä¸€ä¸ªéƒ½æœ‰ä»¥ä¸‹ä¸¤ä¸ªå¿…éœ€çš„å‚æ•°ï¼š

-   `prompt`: A prompt that contains the stringÂ `<1>`Â in place of the trained concept, e.g.Â `an astronaut riding a horse in the style of <1>`. UseÂ `<2>`,Â `<3>`Â if you pass multiple URLs to theÂ `lora_urls`Â input.  
    
    `prompt` : ä¸€ä¸ªåŒ…å«å­—ç¬¦ä¸² `<1>` çš„æç¤ºï¼Œç”¨äºæ›¿ä»£è®­ç»ƒçš„æ¦‚å¿µï¼Œä¾‹å¦‚ `an astronaut riding a horse in the style of <1>` ã€‚å¦‚æœä½ å‘ `lora_urls` è¾“å…¥ä¼ é€’å¤šä¸ªURLï¼Œä½¿ç”¨ `<2>` ï¼Œ `<3>` ã€‚
-   `lora_urls`: The URL or URLs of your trained LoRA concept(s) you copied in the previous step.  
    
    `lora_urls` : ä½ åœ¨ä¸Šä¸€æ­¥å¤åˆ¶çš„å·²è®­ç»ƒLoRAæ¦‚å¿µçš„URLæˆ–URLsã€‚  
    
    You can pass a single URL or a list of URLs separated by a pipe characterÂ `|`. Passing multiple URLs allows you to combine multiple concepts into a single image.  
    
    ä½ å¯ä»¥ä¼ é€’ä¸€ä¸ªå•ç‹¬çš„URLï¼Œæˆ–è€…ç”¨ç®¡é“ç¬¦å· `|` åˆ†éš”çš„URLåˆ—è¡¨ã€‚ä¼ é€’å¤šä¸ªURLå¯ä»¥è®©ä½ å°†å¤šä¸ªæ¦‚å¿µåˆå¹¶åˆ°ä¸€å¼ å›¾ç‰‡ä¸­ã€‚

#### Stable Diffusion  

ç¨³å®šæ‰©æ•£

These prompts were generated with the barebones Stable Diffusion model.  

è¿™äº›æç¤ºæ˜¯ç”¨æœ€åŸºæœ¬çš„ç¨³å®šæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„ã€‚  

I had to play with the LoRA scale parameter to get the best results. I found that a scale of `0.5` or `0.4` worked best for my face.  

æˆ‘ä¸å¾—ä¸è°ƒæ•´LoRAçš„æ¯”ä¾‹å‚æ•°ä»¥è·å¾—æœ€ä½³ç»“æœã€‚æˆ‘å‘ç° `0.5` æˆ– `0.4` çš„æ¯”ä¾‹æœ€é€‚åˆæˆ‘çš„è„¸ã€‚

[https://replicate.com/cloneofsimo/lora](https://replicate.com/cloneofsimo/lora?ref=shruggingface.com)

```
PROMPT: portrait of <1> in a suit, detailed painting, hd, hq, high resolution, high detail, 4 k, 8 k
```

![PROMPT: portrait of <1> in a suit, detailed painting, hd, hq, high resolution, high detail, 4 k, 8 k](image.9.jpeg)

![PROMPT: portrait of <1> in a suit, detailed painting, hd, hq, high resolution, high detail, 4 k, 8 k](image.10.jpeg)

```
PROMPT: portrait of <1>, detailed painting, hd, hq, high resolution, high detail, 4 k, 8 k
```

![PROMPT: portrait of <1>, detailed painting, hd, hq, high resolution, high detail, 4 k, 8 k](image.11.jpeg)

![PROMPT: portrait of <1>, detailed painting, hd, hq, high resolution, high detail, 4 k, 8 k](image.12.jpeg)

```
PROMPT: portrait of <1> in a leopard print housecoat, detailed painting, hd, hq, high resolution, high detail, 4 k, 8 k
```

![PROMPT: portrait of <1> in a leopard print housecoat, detailed painting, hd, hq, high resolution, high detail, 4 k, 8 k](image.13.jpeg)

![PROMPT: portrait of <1> in a leopard print housecoat, detailed painting, hd, hq, high resolution, high detail, 4 k, 8 k](image.14.jpeg)

#### Realistic Vision 1.3  

çœŸå®è§†è§‰ 1.3

This is the Realistic Vision 1.3 model, which is Stable Diffusion + extra DreamBooth training on top.  

è¿™æ˜¯Realistic Vision 1.3æ¨¡å‹ï¼Œå®ƒæ˜¯ç¨³å®šæ‰©æ•£+é¢å¤–çš„DreamBoothè®­ç»ƒã€‚  

This model yielded the overall best results, and responded well to many prompts.  

è¿™ä¸ªæ¨¡å‹äº§ç”Ÿäº†æ€»ä½“ä¸Šæœ€å¥½çš„ç»“æœï¼Œå¹¶ä¸”å¯¹è®¸å¤šæç¤ºååº”è‰¯å¥½ã€‚  

I think it's likely because it was trained on lots of photos of people, so it's better at generating faces than the other models.  

æˆ‘è®¤ä¸ºè¿™å¯èƒ½æ˜¯å› ä¸ºå®ƒæ¥å—äº†å¤§é‡äººç‰©ç…§ç‰‡çš„è®­ç»ƒï¼Œæ‰€ä»¥åœ¨ç”Ÿæˆé¢éƒ¨å›¾åƒæ–¹é¢æ¯”å…¶ä»–æ¨¡å‹æ›´ä¼˜ç§€ã€‚

[https://replicate.com/cloneofsimo/realistic\_vision\_v1.3](https://replicate.com/cloneofsimo/realistic_vision_v1.3?ref=shruggingface.com)

[https://civitai.com/models/4201/realistic-vision-v13](https://civitai.com/models/4201/realistic-vision-v13?ref=shruggingface.com)

```
PROMPT: concept art of <1>, abstract impressionistic style, detailed face, detailed eyes, realistic eyes, realistic face, colorful background, graffiti background, highres, RAW photo 8k uhd, dslr
```

#### Inkpunk Diffusion  

å¢¨æ°´æœ‹å…‹æ‰©æ•£

This was my favorite model to play with because it did such a great job of capturing realistic details but portraying them in a specific illustration style.  

è¿™æ˜¯æˆ‘æœ€å–œæ¬¢çš„æ¨¡å‹ï¼Œå› ä¸ºå®ƒéå¸¸å‡ºè‰²åœ°æ•æ‰åˆ°äº†çœŸå®çš„ç»†èŠ‚ï¼Œä½†åˆä»¥ç‰¹å®šçš„æ’å›¾é£æ ¼å‘ˆç°å‡ºæ¥ã€‚  

It's a Stable Diffusion + Dreambooth model, vaguely inspired by Gorillaz, FLCL, and Yoji Shinkawa.  

è¿™æ˜¯ä¸€ä¸ªç¨³å®šæ‰©æ•£+æ¢¦æƒ³å±•ä½æ¨¡å‹ï¼Œå¤§è‡´å—åˆ°äº†Gorillazï¼ŒFLCLå’Œæ–°å·æ´‹äºŒçš„å¯å‘ã€‚

[https://replicate.com/cloneofsimo/inkpunk\_lora](https://replicate.com/cloneofsimo/inkpunk_lora?ref=shruggingface.com)

[https://huggingface.co/Envvi/Inkpunk-Diffusion](https://huggingface.co/Envvi/Inkpunk-Diffusion?ref=shruggingface.com)

```
PROMPT: a photo of <1>, nvinkpunk
```

#### GTA5 Diffusion  

GTA5æ‰©æ•£

This model was trained on the loading screens, gta storymode, and gta online DLCs artworks.  

è¿™ä¸ªæ¨¡å‹æ˜¯åœ¨åŠ è½½å±å¹•ã€GTAæ•…äº‹æ¨¡å¼å’ŒGTAåœ¨çº¿DLCsè‰ºæœ¯ä½œå“ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚  

Which includes characters, background, chop, and some objects.  

å…¶ä¸­åŒ…æ‹¬è§’è‰²ï¼ŒèƒŒæ™¯ï¼Œå‰ªè¾‘å’Œä¸€äº›ç‰©ä½“ã€‚

[https://replicate.com/cloneofsimo/gta5\_lora](https://replicate.com/cloneofsimo/gta5_lora?ref=shruggingface.com)

[https://huggingface.co/ItsJayQz/GTA5\_Artwork\_Diffusion](https://huggingface.co/ItsJayQz/GTA5_Artwork_Diffusion?ref=shruggingface.com)

```
PROMPT: a photo of <1> gtav style
```

#### Vintedois Diffusion

[https://replicate.com/cloneofsimo/vintedois\_lora](https://replicate.com/cloneofsimo/vintedois_lora?ref=shruggingface.com)

[https://huggingface.co/22h/vintedois-diffusion-v0-1](https://huggingface.co/22h/vintedois-diffusion-v0-1?ref=shruggingface.com)

```
close up portrait of <1> in a Sci Fi suit - 4k uhd, hyper detailed, photorealistic, steampunk, lovecraft colors, dan mumford colors, psychedelic black light, epic composition
```

### Using Multiple LoRA Concepts  

ä½¿ç”¨å¤šä¸ªLoRAæ¦‚å¿µ

Last but certainly not least, I wanted to try out style transfer and use multiple LoRA concepts simultaneously.  

æœ€åä½†ç»å¯¹ä¸æ˜¯æœ€ä¸é‡è¦çš„ï¼Œæˆ‘æƒ³å°è¯•é£æ ¼è½¬æ¢å¹¶åŒæ—¶ä½¿ç”¨å¤šä¸ªLoRAæ¦‚å¿µã€‚

I kicked off another round of LoRA training, but this time I used the type `style` and trained it with 70 transparent PNGs of the excellent [Toy Faces](https://amrit.art/toy-faces) Library.  

æˆ‘å¯åŠ¨äº†å¦ä¸€è½®LoRAè®­ç»ƒï¼Œä½†è¿™æ¬¡æˆ‘ä½¿ç”¨äº†ç±»å‹ `style` ï¼Œå¹¶ç”¨ä¼˜ç§€çš„Toy Faceså›¾ä¹¦é¦†çš„70å¼ é€æ˜PNGè¿›è¡Œäº†è®­ç»ƒã€‚  

I purchased this stock library back in 2020 and used it for avatars in an unreleased project, so it was cool to rediscover them for something completely different.  

æˆ‘åœ¨2020å¹´è´­ä¹°äº†è¿™ä¸ªè‚¡ç¥¨åº“ï¼Œç”¨äºä¸€ä¸ªæœªå‘å¸ƒçš„é¡¹ç›®ä¸­çš„å¤´åƒï¼Œæ‰€ä»¥é‡æ–°å‘ç°å®ƒä»¬ç”¨äºå®Œå…¨ä¸åŒçš„äº‹æƒ…æ˜¯å¾ˆé…·çš„ã€‚

![Untitled](Untitled2010.webp)

Mixing the LoRA concept of my face with the style of toy faces yielded an interesting mix of imagery that looks sort of like me, sort of like toy faces, but not really like either ğŸ¤·â™‚ï¸  

å°†æˆ‘çš„è„¸éƒ¨çš„LoRAæ¦‚å¿µä¸ç©å…·è„¸éƒ¨çš„é£æ ¼æ··åˆï¼Œäº§ç”Ÿäº†ä¸€ç§çœ‹èµ·æ¥æœ‰ç‚¹åƒæˆ‘ï¼Œæœ‰ç‚¹åƒç©å…·è„¸éƒ¨ï¼Œä½†å®é™…ä¸Šå¹¶ä¸çœŸæ­£åƒä»»ä½•ä¸€ç§çš„æœ‰è¶£å›¾åƒğŸ¤·â™‚ï¸

To generate these, I used the [Realistic Vision 1.3](https://replicate.com/cloneofsimo/realistic_vision_v1.3?ref=shruggingface.com) model, which I felt had the best/most accurate results overall. The prompt was  

ä¸ºäº†ç”Ÿæˆè¿™äº›ï¼Œæˆ‘ä½¿ç”¨äº†Realistic Vision 1.3æ¨¡å‹ï¼Œæˆ‘è§‰å¾—å®ƒæ€»ä½“ä¸Šæœ‰æœ€å¥½/æœ€å‡†ç¡®çš„ç»“æœã€‚æç¤ºæ˜¯

```
PROMPT: photo of <1>, in the style of <2>, detailed faces, highres, RAW photo 8k uhd, dslr
```

and the `lora_urls` parameter included two urls, separated by the `|` character. I also set the `lora_scales` to be `0.5|0.6`, which I believe keeps things fairly balanced but skews slightly toward the style than the photo.  

å¹¶ä¸” `lora_urls` å‚æ•°åŒ…å«äº†ä¸¤ä¸ªURLï¼Œç”± `|` å­—ç¬¦åˆ†éš”ã€‚æˆ‘è¿˜å°† `lora_scales` è®¾ç½®ä¸º `0.5|0.6` ï¼Œæˆ‘ç›¸ä¿¡è¿™æ ·å¯ä»¥ä¿æŒäº‹ç‰©çš„å¹³è¡¡ï¼Œä½†æ˜¯ç¨å¾®åå‘äºé£æ ¼è€Œéç…§ç‰‡ã€‚  

If you crank up the `lora_scales` to 1, the outputs start to look very similar to the input images from the training image set.  

å¦‚æœä½ å°† `lora_scales` è°ƒæ•´åˆ°1ï¼Œè¾“å‡ºå¼€å§‹çœ‹èµ·æ¥ä¸è®­ç»ƒå›¾åƒé›†ä¸­çš„è¾“å…¥å›¾åƒéå¸¸ç›¸ä¼¼ã€‚

### Conclusion  

ç»“è®º

Since August, Iâ€™ve spent a few hundred hours playing around with Stable Diffusion and getting a feel for what it does and how it works.  

è‡ªå…«æœˆä»¥æ¥ï¼Œæˆ‘å·²ç»èŠ±äº†å‡ ç™¾ä¸ªå°æ—¶æ¥ç©å¼„ç¨³å®šæ‰©æ•£ï¼Œä»¥äº†è§£å®ƒçš„åŠŸèƒ½å’Œå·¥ä½œæ–¹å¼ã€‚  

For me, as a not-so-talented digital artist and painter, itâ€™s been extremely helpful for getting images out of my mind and into the world.  

å¯¹æˆ‘æ¥è¯´ï¼Œä½œä¸ºä¸€ä¸ªä¸å¤ªæœ‰æ‰åçš„æ•°å­—è‰ºæœ¯å®¶å’Œç”»å®¶ï¼Œå®ƒå¯¹äºå°†æˆ‘è„‘æµ·ä¸­çš„å›¾åƒè½¬åŒ–ä¸ºç°å®éå¸¸æœ‰å¸®åŠ©ã€‚  

While vanilla Stable Diffusion is impressive on its own, I am most excited about the long-term potential for artists and designers to wield it as a tool that helps them create the best work of their lives.  

è™½ç„¶é¦™è‰ç¨³å®šæ‰©æ•£æœ¬èº«å°±å¾ˆä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†æˆ‘æœ€æœŸå¾…çš„æ˜¯è‰ºæœ¯å®¶å’Œè®¾è®¡å¸ˆå°†å…¶ä½œä¸ºä¸€ç§å·¥å…·ï¼Œå¸®åŠ©ä»–ä»¬åˆ›ä½œå‡ºä»–ä»¬ä¸€ç”Ÿä¸­æœ€å¥½çš„ä½œå“çš„é•¿æœŸæ½œåŠ›ã€‚

I think fine-tuning techniques like LoRA, DreamBooth, and Textual Inversion are all significant steps toward making that dream a reality.  

æˆ‘è®¤ä¸ºåƒLoRAã€DreamBoothå’ŒTextual Inversionè¿™æ ·çš„å¾®è°ƒæŠ€æœ¯éƒ½æ˜¯å®ç°è¿™ä¸ªæ¢¦æƒ³çš„é‡è¦æ­¥éª¤ã€‚  

Today, fine-tuning gives you some creative control over the imagery you produce, but it is ****very**** exploratory.  

ä»Šå¤©ï¼Œå¾®è°ƒä¸ºä½ æä¾›äº†ä¸€äº›å¯¹ä½ äº§ç”Ÿçš„å›¾åƒçš„åˆ›é€ æ€§æ§åˆ¶ï¼Œä½†å®ƒéå¸¸å…·æœ‰æ¢ç´¢æ€§ã€‚  

When working with fine-tuning in Stable Diffusion, you really never know what youâ€™re going to get.  

åœ¨ä½¿ç”¨ç¨³å®šæ‰©æ•£è¿›è¡Œå¾®è°ƒæ—¶ï¼Œä½ çœŸçš„æ°¸è¿œä¸çŸ¥é“ä¼šå¾—åˆ°ä»€ä¹ˆã€‚

The process is very incremental and full of trial and error.  

è¿™ä¸ªè¿‡ç¨‹éå¸¸æ¸è¿›ï¼Œå……æ»¡äº†å°è¯•å’Œé”™è¯¯ã€‚  

Sometimes youâ€™ll generate 100 images before you start getting the results youâ€™re looking for.  

æœ‰æ—¶å€™ï¼Œä½ éœ€è¦ç”Ÿæˆ100å¼ å›¾ç‰‡æ‰èƒ½å¼€å§‹å¾—åˆ°ä½ æƒ³è¦çš„ç»“æœã€‚  

Sometimes youâ€™ll generate 100 images and give up because things arenâ€™t going in the right direction.  

æœ‰æ—¶å€™ï¼Œä½ ä¼šç”Ÿæˆ100å¼ å›¾ç‰‡ï¼Œç„¶åå› ä¸ºäº‹æƒ…æ²¡æœ‰æœç€æ­£ç¡®çš„æ–¹å‘å‘å±•è€Œæ”¾å¼ƒã€‚

As it stands today, I feel like Stable Diffusion pushes artists to think more like a creative director than an artist.  

å°±ç°åœ¨æ¥çœ‹ï¼Œæˆ‘è§‰å¾—ç¨³å®šæ‰©æ•£æ›´åƒæ˜¯åœ¨æ¨åŠ¨è‰ºæœ¯å®¶å»æ›´åƒä¸€ä¸ªåˆ›æ„æ€»ç›‘è€Œä¸æ˜¯è‰ºæœ¯å®¶å»æ€è€ƒã€‚  

Itâ€™s all about communicating a â€œvisionâ€ of what you see in your mind's eye and incrementally working toward making it a reality.  

è¿™éƒ½æ˜¯å…³äºä¼ è¾¾ä½ åœ¨å¿ƒçµçœ¼ä¸­çœ‹åˆ°çš„â€œæ„¿æ™¯â€ï¼Œå¹¶é€æ­¥åŠªåŠ›ä½¿å…¶æˆä¸ºç°å®ã€‚  

Looking at an image, critiquing whatâ€™s good and whatâ€™s bad, changing the prompt word by word.  

çœ‹ç€ä¸€å¹…å›¾åƒï¼Œæ‰¹è¯„ä»€ä¹ˆæ˜¯å¥½çš„ï¼Œä»€ä¹ˆæ˜¯åçš„ï¼Œé€å­—æ”¹å˜æç¤ºã€‚

In the future, Iâ€™m hoping that image generation models become more integrated into tools like Photoshop and Procreate, enhancing the skills of artists and reducing the effort it takes to go from image in mind to image in the world.  

åœ¨æœªæ¥ï¼Œæˆ‘å¸Œæœ›å›¾åƒç”Ÿæˆæ¨¡å‹èƒ½æ›´å¤šåœ°èå…¥åˆ°åƒPhotoshopå’ŒProcreateè¿™æ ·çš„å·¥å…·ä¸­ï¼Œæå‡è‰ºæœ¯å®¶çš„æŠ€èƒ½ï¼Œå‡å°‘ä»å¿ƒä¸­çš„å›¾åƒåˆ°ç°å®ä¸–ç•Œçš„å›¾åƒæ‰€éœ€çš„åŠªåŠ›ã€‚

Until then, I will continue happily hacking on Stable Diffusion with fine-tuned models for fun.  

åœ¨é‚£ä¹‹å‰ï¼Œæˆ‘å°†ç»§ç»­æ„‰å¿«åœ°ç ”ç©¶ç¨³å®šæ‰©æ•£ï¼Œå¹¶ä¸ºäº†ä¹è¶£å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
