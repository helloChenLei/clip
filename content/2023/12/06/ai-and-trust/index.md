---
title: "人工智能与信任"
date: 2023-12-06T11:39:53+08:00
updated: 2023-12-06T11:39:53+08:00
taxonomies:
  tags: []
extra:
  source: https://www.belfercenter.org/publication/ai-and-trust
  hostname: www.belfercenter.org
  author: Author: Bruce Schneier
  original_title: "AI and Trust"
  original_lang: en
---

Analysis & Opinions \- Belfer Center for Science and International Affairs, Harvard Kennedy School  

分析与观点 - 贝尔弗中心（Belfer Center for Science and International Affairs），哈佛肯尼迪学院

_This essay was originally presented at an_ [_AI Cyber Lunch_](https://www.belfercenter.org/program/science-technology-and-public-policy#!ai-cyber-lunch-series) _on September 20, 2023, at Harvard Kennedy School._  

本文最初是在2023年9月20日哈佛肯尼迪学院的AI网络午餐上发表的。

___

I trusted a lot today. I trusted my phone to wake me on time.  

今天我信任了很多事情。我相信我的手机会准时叫醒我。  

I trusted Uber to arrange a taxi for me, and the driver to get me to the airport safely.  

我相信 Uber 会为我安排一辆出租车，并且司机会安全地送我到机场。  

I trusted thousands of other drivers on the road not to ram my car on the way.  

我相信其他成千上万的司机在路上不会撞到我的车。  

At the airport, I trusted ticket agents and maintenance engineers and everyone else who keeps airlines operating.  

在机场，我相信售票员、维修工程师和其他保持航空公司运营的人员。  

And the pilot of the plane I flew.  

还有我驾驶的飞机的飞行员。  

And thousands of other people at the airport and on the plane, any of which could have attacked me.  

还有机场和飞机上的成千上万的其他人，其中任何一个都可能袭击我。  

And all the people that prepared and served my breakfast, and the entire food supply chain—any of them could have poisoned me.  

所有为我准备和提供早餐的人，以及整个食品供应链中的任何人都有可能对我下毒。  

When I landed here, I trusted thousands more people: at the airport, on the road, in this building, in this room.  

当我降落在这里时，我信任更多的人：在机场、在路上、在这栋建筑物里、在这个房间里。  

And that was all before 10:30 this morning.  

而这一切都发生在今天上午10点30分之前。

Trust is essential to society. Humans as a species are trusting.  

信任对于社会至关重要。作为一个物种，人类是信任的。  

We are all sitting here, mostly strangers, confident that nobody will attack us.  

我们都坐在这里，大多数是陌生人，相信没有人会攻击我们。  

If we were a roomful of chimpanzees, this would be impossible.  

如果我们是一群黑猩猩，这是不可能的。  

We trust many thousands of times a day. Society can't function without it.  

我们每天都要信任成千上万次。没有它，社会无法正常运转。  

And that we don't even think about it is a measure of how well it all works.  

我们甚至不去考虑这个问题，这说明一切都运作得很好。

In this talk, I am going to make several arguments.  

在这次演讲中，我将提出几个观点。  

One, that there are two different kinds of trust—interpersonal trust and social trust—and that we regularly confuse them.  

首先，存在着两种不同的信任——人际信任和社会信任——我们经常混淆它们。

Two, that the confusion will increase with artificial intelligence.

  

第二点，混淆将随着人工智能的增长而增加。  

We will make a fundamental category error.  

我们将犯一个基本的分类错误。  

We will think of AIs as friends when they're really just services.  

当AI只是服务时，我们会把它们当作朋友来看待。  

Three, that the corporations controlling AI systems will take advantage of our confusion to take advantage of us.  

第三点，控制人工智能系统的公司将利用我们的困惑来占我们的便宜。  

They will not be trustworthy. And four, that it is the role of government to create trust in society.  

它们将不值得信赖。第四点，政府的角色是在社会中建立信任。

And therefore, it is their role to create an environment for trustworthy AI.

  

因此，他们的角色是为可信赖的人工智能创造一个环境。  

And that means regulation.  

这意味着需要进行监管。  

Not regulating AI, but regulating the organizations that control and use AI.  

不是对人工智能进行监管，而是对控制和使用人工智能的组织进行监管。

Okay, so let’s back up and take that all a lot slower.  

好的，让我们回过头来，慢慢来解释这些。  

Trust is a complicated concept, and the word is overloaded with many meanings.  

信任是一个复杂的概念，这个词有很多含义。  

There's personal and intimate trust.  

有个人的亲密信任。  

When we say that we trust a friend, it is less about their specific actions and more about them as a person.  

当我们说我们信任一个朋友时，与其说是因为他们的具体行为，更多的是因为他们作为一个人。  

It's a general reliance that they will behave in a trustworthy manner.  

这是一种普遍的依赖，即它们会以值得信赖的方式行事。  

We trust their intentions, and know that those intentions will inform their actions.  

我们信任他们的意图，并知道这些意图将指导他们的行动。  

Let’s call this "interpersonal trust."  

我们称之为"人际信任"。

There's also the less intimate, less personal trust.  

还有一种不那么亲密、不那么个人的信任。  

We might not know someone personally, or know their motivations—but we can trust their behavior.  

我们可能不认识某人，也不了解他们的动机，但我们可以信任他们的行为。  

We don't know whether or not someone wants to steal, but maybe we can trust that they won't.  

我们不知道某人是否想要偷窃，但也许我们可以相信他们不会这样做。  

It's really more about reliability and predictability.  

这更多地涉及可靠性和可预测性。  

We'll call this "social trust." It's the ability to trust strangers.  

我们称之为"社会信任"。这是对陌生人的信任能力。

Interpersonal trust and social trust are both essential in society today. This is how it works.  

人际信任和社会信任在当今社会都是至关重要的。这是它的运作方式。  

We have mechanisms that induce people to behave in a trustworthy manner, both interpersonally and socially.  

我们有机制来促使人们在人际和社会上表现出值得信赖的行为。  

This, in turn, allows others to be trusting. Which enables trust in society.  

反过来，这使得其他人能够信任。这样就实现了社会的信任。  

And that keeps society functioning.  

这使得社会能够正常运转。  

The system isn't perfect—there are always going to be untrustworthy people—but most of us being trustworthy most of the time is good enough.  

这个系统并不完美——总会有不值得信任的人——但大多数人大部分时间都是值得信任的，这已经足够好了。

I wrote about this in 2012 in a book called [_Liars and Outliers_](https://www.schneier.com/books/liars-and-outliers/).  

我在2012年的一本名为《骗子和局外人》的书中写到了这个问题。  

I wrote about four systems for enabling trust: our innate morals, concern about our reputations, the laws we live under, and security technologies that constrain our behavior.  

我写到了四个促进信任的系统：我们内在的道德，对声誉的关注，我们所生活的法律，以及限制我们行为的安全技术。  

I wrote about how the first two are more informal than the last two.  

我写到了前两者比后两者更不正式。  

And how the last two scale better, and allow for larger and more complex societies.  

以及后两者更具可扩展性，能够支持更大规模和更复杂的社会。  

They enable cooperation amongst strangers.  

它们使得陌生人之间能够合作。

What I didn't appreciate is how different the first and last two are.  

我没有意识到的是第一个和最后两个是多么不同。  

Morals and reputation are person to person, based on human connection, mutual vulnerability, respect, integrity, generosity, and a lot of other things besides.  

道德和声誉是人与人之间的，基于人际关系、相互脆弱、尊重、诚信、慷慨以及其他许多因素。  

These underpin interpersonal trust.  

这些是人际信任的基础。  

Laws and security technologies are systems of trust that force us to act trustworthy.  

法律和安全技术是迫使我们表现出可信行为的信任系统。  

And they're the basis of social trust.  

它们是社会信任的基础。

Taxi driver used to be one of the country's most dangerous professions. Uber changed that.  

出租车司机曾经是该国最危险的职业之一。优步改变了这一点。  

I don’t know my Uber driver, but the rules and the technology lets us both be confident that neither of us will cheat or attack each other.  

我不认识我的优步司机，但规则和技术让我们都能够确信彼此不会欺骗或攻击对方。  

We are both under constant surveillance and are competing for star rankings.  

我们都处于不断监视之下，并且在竞争星级评分。

Lots of people write about the difference between living in a high-trust and a low-trust society.  

很多人写过生活在高信任和低信任社会之间的差异。  

How reliability and predictability make everything easier.  

可靠性和可预测性使一切变得更容易。  

And what is lost when society doesn't have those characteristics.  

当社会缺乏这些特征时，会失去什么。  

Also, how societies move from high-trust to low-trust and vice versa.  

此外，社会如何从高信任转变为低信任，反之亦然。  

This is all about social trust.  

这一切都与社会信任有关。

That literature is important, but for this talk the critical point is that social trust scales better.  

那些文献很重要，但对于这次演讲来说，关键点是社会信任更具可扩展性。  

You used to need a personal relationship with a banker to get a loan.  

过去，你需要与银行家建立个人关系才能获得贷款。  

Now it's all done algorithmically, and you have many more options to choose from.  

现在一切都是通过算法完成的，你有更多选择的机会。

Social trust scales better, but embeds all sorts of bias and prejudice.  

社会信任更具可扩展性，但也嵌入了各种偏见和歧视。  

That's because, in order to scale, social trust has to be structured, system- and rule-oriented, and that's where the bias gets embedded.  

这是因为为了扩大规模，社会信任必须是有结构、系统和规则导向的，而这就是偏见嵌入的地方。  

And the system has to be mostly blinded to context, which removes flexibility.  

而且系统必须在很大程度上忽视上下文，这就削弱了灵活性。

But that scale is vital.  

但这种规模是至关重要的。  

In today's society we regularly trust—or not—governments, corporations, brands, organizations, groups.  

在今天的社会中，我们经常信任或不信任政府、公司、品牌、组织、团体。  

It's not so much that I trusted the particular pilot that flew my airplane, but instead the airline that puts well-trained and well-rested pilots in cockpits on schedule.  

我并不是特别信任驾驶我飞机的那个飞行员，而是相信那家安排有训练有素、休息充足的飞行员按时驾驶飞机的航空公司。  

I don't trust the cooks and waitstaff at a restaurant, but the system of health codes they work under.  

我不信任餐馆的厨师和服务员，但我信任他们遵守的卫生规范体系。  

I can't even describe the banking system I trusted when I used an ATM this morning.  

我甚至无法描述我今天早上使用的自动取款机所信任的银行系统。  

Again, this confidence is no more than reliability and predictability.  

再次强调，这种信任只是基于可靠性和可预测性。

Think of that restaurant again. Imagine that it's a fast-food restaurant, employing teenagers.  

再想象一下那家餐厅。假设它是一家快餐店，雇佣了一些青少年。  

The food is almost certainly safe—probably safer than in high-end restaurants—because of the corporate systems or reliability and predictability that is guiding their every behavior.  

由于公司的可靠性和可预测性系统指导着他们的一举一动，食物几乎可以肯定是安全的，可能比高档餐厅还要安全。

That's the difference. You can ask a friend to deliver a package across town.  

这就是区别所在。你可以请朋友帮忙把包裹送到城市的另一边。  

Or you can pay the Post Office to do the same thing.  

或者你可以支付邮局来完成同样的事情。  

The former is interpersonal trust, based on morals and reputation.  

前者是基于道德和声誉的人际信任。  

You know your friend and how reliable they are.  

你了解你的朋友以及他们的可靠程度。  

The second is a service, made possible by social trust.  

第二个是一种服务，这得益于社会信任。  

And to the extent that is a reliable and predictable service, it's primarily based on laws and technologies.  

而这种可靠且可预测的服务主要依赖于法律和技术。  

Both can get your package delivered, but only the second can become the global package delivery systems that is FedEx.  

两者都可以将您的包裹送达，但只有第二种才能成为全球包裹投递系统，即联邦快递。

Because of how large and complex society has become, we have replaced many of the rituals and behaviors of interpersonal trust with security mechanisms that enforce reliability and predictability—social trust.  

由于社会的规模和复杂性，我们已经用安全机制取代了许多人际信任的仪式和行为，以确保可靠性和可预测性——社会信任。

But because we use the same word for both, we regularly confuse them.  

但由于我们用同样的词来表示两者，我们经常混淆它们。  

And when we do that, we are making a category error.  

当我们这样做时，我们犯了一个范畴错误。

And we do it all the time. With governments. With organizations. With systems of all kinds.  

而我们一直都在这样做。与政府一样。与组织一样。与各种系统一样。  

And especially with corporations.  

尤其是与公司一样。

We might think of them as friends, when they are actually services.  

我们可能把它们当作朋友，而实际上它们只是服务。  

Corporations are not moral; they are precisely as immoral as the law and their reputations let them get away with.  

公司并不具备道德，它们的道德水平与法律和声誉容许它们逃脱的程度一样不道德。

So corporations regularly take advantage of their customers, mistreat their workers, pollute the environment, and lobby for changes in law so they can do even more of these things.  

因此，公司经常利用他们的顾客，虐待他们的员工，污染环境，并游说修改法律，以便能够更多地做这些事情。

Both language and the laws make this an easy category error to make.  

语言和法律都使这种分类错误变得容易发生。  

We use the same grammar for people and corporations.  

我们对人和公司使用相同的语法。  

We imagine that we have personal relationships with brands.  

我们想象我们与品牌有个人关系。  

We give corporations some of the same rights as people.  

我们赋予公司与人类相同的权利。

Corporations like that we make this category error—see, I just made it myself—because they profit when we think of them as friends.  

公司喜欢我们犯这种分类错误，因为当我们把它们当作朋友时，它们就能获利。看，我刚刚自己也犯了这个错误。  

They use mascots and spokesmodels. They have social media accounts with personalities.  

它们使用吉祥物和代言人。它们拥有个性化的社交媒体账号。  

They refer to themselves like they are people.  

它们以自己为人。

But they are not our friends. Corporations are not capable of having that kind of relationship.  

但是它们不是我们的朋友。公司无法建立那种关系。

We are about to make the same category error with AI.  

我们即将犯与人工智能相同的类别错误。  

We're going to think of them as our friends when they're not.  

当它们并非如此时，我们却把它们当作朋友。

A lot has been written about AIs as existential risk.  

关于人工智能作为存在风险的问题已经有很多文章写过。  

The worry is that they will have a goal, and they will work to achieve it even if it harms humans in the process.  

担心的是，它们会有一个目标，并努力实现这个目标，即使在这个过程中伤害到人类。  

You may have read about the "[paperclip maximizer](https://www.huffpost.com/entry/artificial-intelligence-oxford_n_5689858)": an AI that has been programmed to make as many paper clips as possible, and ends up destroying the earth to achieve those ends.  

你可能已经听说过“回形针最大化者”的故事：一种人工智能被编程成尽可能制造更多回形针，最终为了达到这个目标而摧毁了地球。  

It's a weird fear. Science fiction author Ted Chiang writes about it.  

这是一种奇怪的恐惧。科幻作家Ted Chiang写过相关的文章。  

Instead of solving all of humanity's problems, or wandering off proving mathematical theorems that no one understands, the AI single-mindedly pursues the goal of maximizing production.  

AI并不是为了解决人类所有问题，或者去证明没有人能理解的数学定理，而是单纯地追求最大化生产的目标。  

Chiang's point is that this is every corporation’s business plan.  

蒋的观点是这是每个公司的商业计划。  

And that our fears of AI are basically fears of capitalism.  

我们对人工智能的恐惧基本上是对资本主义的恐惧。  

Science fiction writer Charlie Stross takes this one step further, and calls corporations "[slow AI](https://www.antipope.org/charlie/blog-static/2018/01/dude-you-broke-the-future.html)." They are profit maximizing machines.  

科幻作家查理·斯特罗斯进一步提出，将公司称为“缓慢的AI”。它们是利润最大化的机器。  

And the most successful ones do whatever they can to achieve that singular goal.  

而最成功的人会尽一切努力实现这个独特的目标。

And near-term AIs will be controlled by corporations.  

而近期的人工智能将由公司控制。  

Which will use them towards that profit-maximizing goal. They won't be our friends.  

它们将利用这些目标来实现利润最大化的目标。它们不会成为我们的朋友。  

At best, they'll be useful services. More likely, they'll spy on us and try to manipulate us.  

最多，它们将成为有用的服务。更有可能的是，它们会监视我们并试图操纵我们。

This is nothing new. Surveillance is the business model of the Internet.  

这并不是什么新鲜事。监视是互联网的商业模式。  

Manipulation is the other business model of the Internet.  

操纵是互联网的另一个商业模式。

Your Google search results lead with URLs that someone paid to show to you.  

你的谷歌搜索结果首先展示的是有人付费展示给你的网址。  

Your Facebook and Instagram feeds are filled with sponsored posts.  

你的Facebook和Instagram动态中充斥着赞助帖子。  

Amazon searches return pages of products whose sellers paid for placement.  

亚马逊的搜索结果返回的是那些付费展示的产品页面。

This is how the Internet works. Companies spy on us as we use their products and services.  

这就是互联网的运作方式。当我们使用它们的产品和服务时，公司会监视我们。  

Data brokers buy that surveillance data from the smaller companies, and assemble detailed dossiers on us.  

数据经纪商从这些小公司购买监控数据，并对我们进行详细档案的整理。  

Then they sell that information back to those and other companies, who combine it with data they collect in order to manipulate our behavior to serve their interests.  

然后他们将这些信息卖回给这些公司和其他公司，这些公司将其与自己收集的数据结合起来，以操纵我们的行为以符合他们的利益。  

At the expense of our own.  

这是以我们自己的利益为代价的。

We use all of these services as if they are our agents, working on our behalf.  

我们使用所有这些服务，仿佛它们是我们的代理人，为我们工作。  

In fact, they are double agents, also secretly working for their corporate owners.  

事实上，它们是双重代理人，同时也在秘密地为它们的公司所有者工作。  

We trust them, but they are not trustworthy. They're not friends; they're services.  

我们信任它们，但它们不值得信任。它们不是朋友，而是服务。

It's going to be no different with AI. And the result will be much worse, for two reasons.  

AI也不会有所不同。结果将会更糟，有两个原因。

The first is that these AI systems will be more relational.  

首先，这些AI系统将更具关联性。  

We will be conversing with them, using natural language.  

我们将与它们进行对话，使用自然语言。  

As such, we will naturally ascribe human-like characteristics to them.  

因此，我们自然会赋予它们类似人类的特征。

This relational nature will make it easier for those double agents to do their work.  

这种关联性质将使得那些双重特工更容易开展工作。  

Did your chatbot recommend a particular airline or hotel because it's truly the best deal, given your particular set of needs?  

你的聊天机器人是否推荐了某个特定的航空公司或酒店，因为它真的是最好的选择，符合你的特定需求？  

Or because the AI company got a kickback from those providers?  

还是因为 AI 公司从这些供应商那里获得了回扣？  

When you asked it to explain a political issue, did it bias that explanation towards the company's position?  

当你询问它解释一个政治问题时，它是否会偏向于公司的立场来解释？  

Or towards the position of whichever political party gave it the most money?  

还是偏向于给予它最多资金的政党的立场？  

The conversational interface will help hide their agenda.  

对话界面将有助于隐藏他们的议程。

The second reason to be concerned is that these AIs will be more intimate.  

第二个值得关注的原因是这些人工智能将更加亲密。  

One of the promises of generative AI is a personal digital assistant.  

生成式 AI 的一个承诺是个人数字助理。  

Acting as your advocate with others, and as a butler with you.  

它可以作为你与他人的代言人，也可以作为你的管家。  

This requires an intimacy greater than your search engine, email provider, cloud storage system, or phone.  

这需要比你的搜索引擎、电子邮件提供商、云存储系统或手机更亲密的关系。  

You're going to want it with you 24/7, constantly training on everything you do.  

你会希望它24/7与你同在，不断地学习你的一切。  

You will want it to know everything about you, so it can most effectively work on your behalf.  

你会希望它了解你的一切，这样它才能最有效地为你工作。

And it will help you in many ways. It will notice your moods and know what to suggest.  

它将以多种方式帮助你。它会察觉你的情绪并知道该提出什么建议。  

It will anticipate your needs and work to satisfy them.  

它会预测你的需求并努力满足它们。  

It will be your therapist, life coach, and relationship counselor.  

它将成为你的心理治疗师、生活教练和关系顾问。

You will default to thinking of it as a friend.  

你会默认将它视为朋友。  

You will speak to it in natural language, and it will respond in kind.  

你会用自然语言与它交流，它也会以同样的方式回应你。  

If it is a robot, it will look humanoid—or at least like an animal.  

如果它是一个机器人，它会看起来像人类——或者至少像动物。  

It will interact with the whole of your existence, just like another person would.  

它将与你的整个存在互动，就像另一个人一样。

The natural language interface is critical here.  

在这里，自然语言界面至关重要。  

We are primed to think of others who speak our language as people.  

我们往往认为说我们语言的人是人类。  

And we sometimes have trouble thinking of others who speak a different language that way.  

而对于说不同语言的人，我们有时很难以同样的方式思考。  

We make that category error with obvious non-people, like cartoon characters.  

我们对于明显不是人类的事物，比如卡通角色，也会犯这种类别错误。  

We will naturally have a "theory of mind" about any AI we talk with.  

我们自然会对与之交谈的任何 AI 产生一种"心智理论"。

More specifically, we tend to assume that something's implementation is the same as its interface.  

更具体地说，我们倾向于假设某个实现与其接口相同。  

That is, we assume that things are the same on the inside as they are on the surface.  

也就是说，我们假设内部和外表是一样的。  

Humans are like that: we're people through and through.  

人类就是这样：我们完全是人。  

A government is systemic and bureaucratic on the inside.  

政府在内部是系统性和官僚化的。  

You're not going to mistake it for a person when you interact with it.  

当你与它互动时，你不会把它误认为是一个人。  

But this is the category error we make with corporations.  

但这是我们对于公司犯下的类别错误。  

We sometimes mistake the organization for its spokesperson.  

有时我们会把组织误认为是其发言人。  

AI has a fully relational interface—it talks like a person—but it has an equally fully systemic implementation.  

人工智能具有完全关联的界面——它说话的方式就像一个人——但它同样具有完全系统化的实现。  

Like a corporation, but much more so.  

就像一个公司，但更加如此。  

The implementation and interface are more divergent of anything we have encountered to date…by a lot.  

实现和接口在我们迄今遇到的任何事物中都更为分歧...差距很大。

And you will want to trust it. It will use your mannerisms and cultural references.  

而你会希望相信它。它会使用你的举止和文化参考。  

It will have a convincing voice, a confident tone, and an authoritative manner.  

它会有一种令人信服的声音，自信的语调和权威的方式。  

Its personality will be optimized to exactly what you like and respond to.  

它的个性将被优化为完全符合你的喜好和回应。

It will act trustworthy, but it will not be trustworthy. We won't know how they are trained.  

它会表现得可信赖，但它并不可信赖。我们不会知道它们是如何被训练的。  

We won't know their secret instructions.  

我们不会知道它们的秘密指令。  

We won't know their biases, either accidental or deliberate.  

我们也不会知道它们的偏见，无论是意外的还是故意的。

We do know that they are built at enormous expense, mostly in secret, by profit-maximizing corporations for their own benefit.  

我们知道它们是以巨大的成本构建的，主要是在秘密中，由追求利润最大化的公司为自己的利益而建造的。

It's no accident that these corporate AIs have a human-like interface.  

这些企业 AI 具有类似人类的接口并非偶然。  

There's nothing inevitable about that. It's a design choice.  

这并非是不可避免的。这是一种设计选择。  

It could be designed to be less personal, less human-like, more obviously a service—like a search engine . The companies behind those AIs want you to make the friend/service category error.  

它可以被设计得更加不个人化、不像人类那样，更明显地是一种服务，就像一个搜索引擎。那些人工智能背后的公司希望你犯下朋友/服务类别错误。  

It will exploit your mistaking it for a friend. And you might not have any choice but to use it.  

它将利用你将其误认为朋友的错误。而你可能别无选择，只能使用它。

There is something we haven't discussed when it comes to trust: power.  

在谈到信任时，还有一件事我们没有讨论到：权力。  

Sometimes we have no choice but to trust someone or something because they are powerful.  

有时候，我们别无选择，只能信任某人或某物，因为他们很强大。  

We are forced to trust the local police, because they're the only law enforcement authority in town.  

我们被迫信任当地警察，因为他们是镇上唯一的执法机构。  

We are forced to trust some corporations, because there aren't viable alternatives.  

我们被迫信任一些公司，因为没有可行的替代选择。  

To be more precise, we have no choice but to entrust ourselves to them.  

更准确地说，我们别无选择，只能将自己托付给它们。  

We will be in this same position with AI.  

我们将与人工智能面临同样的困境。  

We will have no choice but to entrust ourselves to their decision-making.  

我们别无选择，只能将决策的权力委托给它们。

The friend/service confusion will help mask this power differential.  

朋友/服务的混淆将有助于掩盖这种权力差异。  

We will forget how powerful the corporation behind the AI is, because we will be fixated on the person we think the AI is.  

我们会忘记AI背后的公司有多么强大，因为我们会专注于我们认为AI是一个人。

So far, we have been talking about one particular failure that results from overly trusting AI.  

到目前为止，我们一直在谈论过度信任AI所导致的一种特定失败。  

We can call it something like "hidden exploitation." There are others.  

我们可以称之为“隐藏的剥削”。还有其他情况。  

There's outright fraud, where the AI is actually trying to steal stuff from you.  

有明目张胆的欺诈行为，AI实际上试图从你那里偷东西。  

There's the more prosaic mistaken expertise, where you think the AI is more knowledgeable than it is because it acts confidently.  

还有更加平凡的错误专业知识，你认为AI比它实际上更有知识，因为它表现得自信。  

There's incompetency, where you believe that the AI can do something it can't.  

还有无能，你相信AI能做一些它实际上做不到的事情。  

There's inconsistency, where you mistakenly expect the AI to be able to repeat its behaviors.  

还有不一致性，你错误地期望AI能够重复其行为。  

And there's illegality, where you mistakenly trust the AI to obey the law.  

还有非法行为，你错误地相信AI会遵守法律。  

There are probably more ways trusting an AI can fail.  

相信 AI 可能会失败的方式还有很多。

All of this is a long-winded way of saying that we need trustworthy AI.  

这一切都是在说我们需要可信赖的 AI。  

AI whose behavior, limitations, and training are understood.  

AI 的行为、限制和训练是可以理解的。  

AI whose biases are understood, and corrected for. AI whose goals are understood.  

AI 的偏见是可以理解和纠正的。AI 的目标是可以理解的。  

That won’t secretly betray your trust to someone else.  

它不会秘密地背叛你的信任给其他人。

The market will not provide this on its own.  

市场无法自行提供这一点。  

Corporations are profit maximizers, at the expense of society.  

公司是以牺牲社会为代价来追求利润最大化的。  

And the incentives of surveillance capitalism are just too much to resist.  

而监视资本主义的激励是无法抵挡的。

It's government that provides the underlying mechanisms for the social trust essential to society.  

政府提供了社会信任所必需的基本机制，这对社会至关重要。  

Think about contract law. Or laws about property, or laws protecting your personal safety.  

想想合同法。或者关于财产的法律，或者保护个人安全的法律。  

Or any of the health and safety codes that let you board a plane, eat at a restaurant, or buy a pharmaceutical without worry.  

或者任何让你能够放心登机、在餐馆用餐或购买药品的健康和安全法规。

The more you can trust that your societal interactions are reliable and predictable, the more you can ignore their details.  

你越能相信社会互动是可靠和可预测的，你就越能忽略它们的细节。  

Places where governments don't provide these things are not good places to live.  

那些政府没有提供这些东西的地方不适合居住。

Government can do this with AI. We need AI transparency laws. When it is used. How it is trained.  

政府可以通过人工智能来实现这一点。我们需要人工智能透明法。它是如何被使用的。它是如何被训练的。  

What biases and tendencies it has. We need laws regulating AI—and robotic—safety.  

它有什么偏见和倾向。我们需要监管人工智能和机器人安全的法律。  

When it is permitted to affect the world. We need laws that enforce the trustworthiness of AI.  

它何时被允许影响世界。我们需要强制执行人工智能的可信度的法律。  

Which means the ability to recognize when those laws are being broken.  

这意味着能够识别这些法律何时被违反。  

And penalties sufficiently large to incent trustworthy behavior.  

以及足够大的处罚来激励可信赖的行为。

Many countries are contemplating AI safety and security laws—the EU is the furthest along—but I think they are making a critical mistake.  

许多国家正在考虑人工智能安全和安全法律——欧盟是最进展最远的，但我认为他们犯了一个关键的错误。  

They try to regulate the AIs and not the humans behind them.  

他们试图对人工智能进行监管，而不是背后的人类。

AIs are not people; they don't have agency. They are built by, trained by, and controlled by people.  

人工智能不是人，它们没有行动能力。它们是由人构建、训练和控制的。  

Mostly for-profit corporations.  

大多数情况下是营利性公司。  

Any AI regulations should place restrictions on those people and corporations.  

任何人工智能的规定都应该对那些人和公司进行限制。  

Otherwise the regulations are making the same category error I’ve been talking about.  

否则，这些规定就会犯我一直在谈论的同类错误。  

At the end of the day, there is always a human responsible for whatever the AI's behavior is.  

归根结底，无论人工智能的行为如何，总有一个人要负责。  

And it's the human who needs to be responsible for what they do—and what their companies do.  

而且，人类需要对自己的行为以及自己所在公司的行为负责。  

Regardless of whether it was due to humans, or AI, or a combination of both.  

无论是由人类、人工智能还是两者的组合导致的。  

Maybe that won't be true forever, but it will be true in the near future.  

也许这种情况将来不再成立，但在不久的将来仍然是成立的。  

If we want trustworthy AI, we need to require trustworthy AI controllers.  

如果我们想要可信赖的人工智能，我们需要要求可信赖的人工智能控制者。

We already have a system for this: fiduciaries.  

我们已经有了一个这样的系统：受托人。  

There are areas in society where trustworthiness is of paramount importance, even more than usual.  

在社会中有一些领域，诚信至关重要，甚至比平常更重要。  

Doctors, lawyers, accountants…these are all trusted agents.  

医生、律师、会计师...这些都是值得信赖的代理人。  

They need extraordinary access to our information and ourselves to do their jobs, and so they have additional legal responsibilities to act in our best interests.  

他们需要特殊的访问权限来获取我们的信息和了解我们自己的情况，因此他们有额外的法律责任来维护我们的最佳利益。  

They have fiduciary responsibility to their clients.  

他们对客户负有受托责任。

We need the same sort of thing for our data. The idea of a data fiduciary is not new.  

我们的数据也需要同样的保护。数据受托人的概念并不新鲜。  

But it's even more vital in a world of generative AI assistants.  

但在生成式人工智能助手的世界中，这一点更加重要。

And we need one final thing: public AI models.  

我们还需要最后一件事：公共 AI 模型。  

These are systems built by academia, or non-profit groups, or government itself, that can be owned and run by individuals.  

这些是由学术界、非营利组织或政府自身构建的系统，可以由个人拥有和运行。

The term "public model" has been thrown around a lot in the AI world, so it’s worth detailing what this means.  

在人工智能领域，"公共模型"这个术语被广泛使用，因此值得详细说明其含义。  

It's not a corporate AI model that the public is free to use.  

它不是公众可以自由使用的企业人工智能模型。  

It's not a corporate AI model that the government has licensed.  

它也不是政府授权的企业人工智能模型。  

It's not even an open-source model that the public is free to examine and modify.  

这甚至不是一个公众可以自由审查和修改的开源模型。

A public model is a model built by the public for the public.  

公共模型是由公众为公众构建的模型。  

It requires political accountability, not just market accountability.  

它需要政治问责制，而不仅仅是市场问责制。  

This means openness and transparency paired with a responsiveness to public demands.  

这意味着公开透明，同时响应公众需求。  

It should also be available for anyone to build on top of. This means universal access.  

它还应该对任何人开放，供其在其基础上构建。这意味着普遍访问。  

And a foundation for a free market in AI innovations.  

它是 AI 创新自由市场的基础。  

This would be a counter-balance to corporate-owned AI.  

这将是对企业拥有的 AI 的一种制衡。

We can never make AI into our friends.  

我们永远不能把 AI 当作我们的朋友。  

But we can make them into trustworthy services—agents and not double agents.  

但我们可以将它们变成可信赖的服务——代理人而不是双重代理人。  

But only if government mandates it. We can put limits on surveillance capitalism.  

但前提是政府要求。我们可以对监视资本主义进行限制。  

But only if government mandates it.  

但前提是政府要求。

Because the point of government is to create social trust.  

因为政府的目的是建立社会信任。  

I started this talk by explaining the importance of trust in society, and how interpersonal trust doesn't scale to larger groups.  

我开始这个演讲是为了解释信任在社会中的重要性，以及人际信任无法扩展到更大的群体。  

That other, impersonal kind of trust—social trust, reliability and predictability—is what governments create.  

另一种无关个人的信任——社会信任、可靠性和可预测性——是政府所创造的。

To the extent a government improves the overall trust in society, it succeeds.  

政府在提高社会整体信任度方面取得成功。  

And to the extent a government doesn't, it fails.  

而如果政府没有做到这一点，它就会失败。

But they have to.  

但他们必须这样做。  

We need government to constrain the behavior of corporations and the AIs they build, deploy, and control.  

我们需要政府来限制企业及其构建、部署和控制的人工智能的行为。  

Government needs to enforce both predictability and reliability.  

政府需要同时强制可预测性和可靠性。

That's how we can create the social trust that society needs to thrive.  

这就是我们可以创造社会所需的社会信任的方式。
