---
title: "与 xAI 对齐计划相抗衡 "
date: 2023-07-29T07:36:41+08:00
updated: 2023-07-29T07:36:41+08:00
taxonomies:
  tags: []
extra:
  source: https://astralcodexten.substack.com/p/contra-the-xai-alignment-plan
  hostname: astralcodexten.substack.com
  author: Scott Alexander
  original_title: "Contra The xAI Alignment Plan"
  original_lang: zh
---

Elon Musk has a new AI company, xAI. I appreciate that he seems very concerned about alignment. From [his Twitter Spaces discussion](https://twitter.com/Twitter/status/1679256473191297026):  

埃隆-马斯克新成立了一家人工智能公司 xAI。我很欣赏他似乎非常关注对齐问题。摘自他在 Twitter Spaces 上的讨论：

> I think I have been banging the drum on AI safety now for a long time. If I could press pause on AI or advanced AI digital superintelligence, I would.  
> 
> 关于人工智能安全问题，我想我已经敲了很长时间的鼓了。如果我能暂停人工智能或高级人工智能数字超级智能，我一定会这么做。  
> 
> It doesn’t seem like that is realistic . . .  
> 
> 这似乎不太现实。
> 
> I could talk about this for a long time, it’s something that I’ve thought about for a really long time and actually was somewhat reluctant to do anything in this space because I am concerned about the immense power of a digital superintelligence.  
> 
> 这个问题我可以谈很久，我已经考虑了很久，实际上我并不愿意在这个领域做任何事情，因为我担心数字超级智能的巨大威力。  
> 
> It’s something that, I think is maybe hard for us to even comprehend.  
> 
> 我想，我们甚至很难理解这一点。

He describes his alignment strategy in that discussion and [a later followup](https://twitter.com/xai/status/1679945247340793856):  

他在那次讨论和后来的后续讨论中介绍了他的调整策略：

> The premise is have the AI be maximally curious, maximally truth-seeking, I'm getting a little esoteric here, but I think from an AI safety standpoint, a maximally curious AI - one that's trying to understand the universe - I think is going to be pro-humanity from the standpoint that humanity is just much more interesting than not . . . Earth is vastly more interesting than Mars. . . that's like the best thing I can come up with from an AI safety standpoint.  
> 
> 前提是让人工智能具有最大限度的好奇心、最大限度的求真精神，我说得有点深奥，但我认为，从人工智能安全的角度来看，一个具有最大限度好奇心的人工智能--一个试图了解宇宙的人工智能--我认为将是亲人类的，因为从人类的角度来看，人类要比不人类有趣得多......。.地球比火星有趣得多。.从人工智能安全的角度来看，这是我能想到的最好的办法了。  
> 
> I think this is better than trying to explicitly program morality - if you try to program morality, you have to ask whose morality.  
> 
> 我认为这比试图明确编程道德更好--如果你试图编程道德，你就必须问谁的道德。
> 
> And even if you're extremely good at how you program morality into AI, there's the morality inversion problem - Waluigi - if you program Luigi, you inherently get Waluigi.  
> 
> 即使你对人工智能的道德编程非常在行，也会出现道德倒置的问题--瓦鲁伊吉--如果你给路易吉编程，就会得到瓦鲁伊吉。  
> 
> I would be concerned about the way OpenAI is programming AI - about this is good, and that's not good.  
> 
> 我会担心 OpenAI 对人工智能进行编程的方式--这个好，那个不好。

I feel deep affection for this plan - curiosity is an important value to me, and Elon’s right that programming some specific person/culture’s morality into an AI - the way a lot of people are doing it right now - feels creepy.  

我对这一计划深有感触--好奇心对我来说是一种重要的价值观，而埃隆说得没错，将某些特定的人/文化的道德观编程到人工智能中--很多人现在的做法--让人感觉毛骨悚然。  

So philosophically I’m completely on board. And maybe this is just one facet of a larger plan, and I’m misunderstanding the big picture.  

所以，从哲学上讲，我完全同意。也许这只是更大计划的一个方面，而我误解了全局。  

The company is still very new, I’m sure things will change later, maybe this is just a first draft.  

公司还很新，我相信以后会有变化，也许这只是初稿。

But if it’s more or less as stated, I do think there are two big problems:  

但如果大致如上所述，我确实认为有两个大问题：

1.  It won’t work 没用的
    
2.  If it did work, it would be bad.  
    
    如果真的有用，那就糟了。
    

I want to start by discussing the second objection, then loop back to explain what I mean about the first.  

我想先讨论第二个反对意见，然后再回过头来解释我对第一个反对意见的意思。

The one sentence version: many scientists are curious about fruit flies, but this rarely ends well for the fruit flies.  

一句话版本：许多科学家对果蝇充满好奇，但果蝇很少有好下场。

The longer, less flippant version:  

更长、更不轻率的版本：

Even if an AI decides humans are interesting, this doesn’t mean the AI will promote human flourishing forever.  

即使人工智能认为人类是有趣的，这也并不意味着人工智能会永远促进人类的繁荣。  

Elon says his goal is “an age of plenty where there is no shortage of goods and services”, but why would a maximally-curious AI provide this?  

埃隆说，他的目标是 "一个不缺乏商品和服务的富足时代"，但为什么一个最大好奇心的人工智能会提供这种服务呢？  

It might decide that humans suffering is more interesting than humans flourishing.  

它可能会认为人类的痛苦比人类的繁荣更有趣。  

Or that both are interesting, and it will have half the humans in the world flourish, and the other half suffer as a control group.  

或者说，两者都很有趣，它将让世界上一半的人类繁荣昌盛，而另一半作为对照组遭受苦难。  

Or that neither are the most interesting thing, and it would rather keep humans in tanks and poke at them in various ways to see what happens.  

或者说，这两者都不是最有趣的事情，它宁愿把人类关在水箱里，用各种方法戳他们，看看会发生什么。

Even if an AI decides human flourishing is briefly interesting, after a while it will already know lots of things about human flourishing and want to learn something else instead.  

即使人工智能认为人类的繁衍生息很有趣，过一段时间后，它也会对人类的繁衍生息了如指掌，转而想学习其他东西。  

Scientists have occasionally made [colonies of extremely happy well-adjusted rats](https://en.wikipedia.org/wiki/Rat_Park) to see what would happen. But then they learned what happened, and switched back to things like testing [how long rats would struggle against their inevitable deaths if you left them to drown in locked containers](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3353513/).  

科学家们偶尔会饲养一些非常快乐、适应良好的老鼠，看看会发生什么。但后来他们知道了发生了什么事，就又转回去做试验了，比如测试如果把老鼠扔在上锁的容器里淹死，它们会挣扎多久以抵御不可避免的死亡。

Is leaving human society intact really an efficient way to study humans?  

让人类社会保持完整真的是研究人类的有效方法吗？  

Maybe it would be better to dissect a few thousand humans, learn the basic principles, then run a lot of simulations of humans in various contrived situations.  

也许最好先解剖几千个人，了解基本原理，然后在各种假定情况下对人类进行大量模拟。  

Would the humans in the simulation be conscious? I don’t know and the AI wouldn’t care.  

模拟中的人类会有意识吗？我不知道，人工智能也不会关心。  

If it was cheaper to simulate abstracted humans in low-fidelity, the same way SimCity has simulated citizens who are just a bundle of traffic-related preferences, wouldn’t the AI do that instead?  

如果以低保真方式模拟抽象化的人类更便宜，就像《模拟城市》模拟市民一样，市民只是一束与交通相关的偏好，人工智能难道不会这样做吗？

Are humans more interesting than sentient lizard-people? I don’t know. If the answer is no, will the AI kill all humans and replace them with lizard-people?  

人类比有生命的蜥蜴人更有趣吗？我不知道。如果答案是否定的，人工智能是否会杀死所有人类，用蜥蜴人取而代之？  

Surely after a thousand years of studying human flourishing _ad nauseum_, the lizard-people start sounding more interesting.  

当然，经过一千年对人类繁衍生息的不厌其烦的研究，蜥蜴人开始听起来更有趣了。

Would a maximally curious AI be curious about the same things as us?  

好奇心最强的人工智能会对和我们一样的事物感到好奇吗？  

I would like to think that humans are “objectively” more interesting than moon rocks in some sense - harder to predict, capable of more complex behavior.  

我想，从某种意义上说，人类比月球岩石 "客观 "地更有趣--更难预测，能做出更复杂的行为。  

But if it turns out that the most complex and unpredictable part of us is how our fingerprints form, and that (eg) our food culture is an incredibly boring function of a few gustatory receptors, will the AI grow a trillion human fingers in weird vats, but also remove our ability to eat anything other than nutrient sludge?  

但是，如果事实证明，我们身上最复杂、最难以预测的部分就是我们的指纹是如何形成的，而（例如）我们的饮食文化只是一些味觉感受器的无聊功能，那么人工智能是否会在奇怪的大桶里培育出一万亿个人类手指，但同时也会消除我们吃营养泥以外的任何东西的能力呢？

I predict that if we ever got a maximally curious superintelligence, it would scan all humans, vaporize existing physical-world humans as unnecessary and inconvenient, use the scans to run many low-fidelity simulations to help it learn the general principles of intelligent life (plus maybe a few higher-fidelity simulations, like the one you’re in now), then simulate a trillion intelligent-life-like entities to see if (eg) their neural networks reached some interesting meta-stable positions.  

我预测，如果我们有了一个好奇心最大化的超级智能体，它会扫描所有人类，把物理世界中现有的人类当作不必要和不方便的人蒸发掉，利用扫描结果运行许多低保真模拟，帮助它学习智能生命的一般原理（也许还有一些高保真模拟，就像你现在所处的这个），然后模拟一万亿个类似智能生命的实体，看看（比如）它们的神经网络是否达到了一些有趣的元稳定位置。  

Then it would move beyond being interested in any of that, and disassemble the Earth to use its atoms to make a really big particle accelerator (which would be cancelled halfway through by Superintelligent AI Congress).  

然后，它将不再对这些感兴趣，而是拆解地球，利用地球上的原子制造一个真正的大型粒子加速器（中途会被超级智能人工智能大会取消）。

This doesn’t mean AI can’t have a goal of understanding the universe. I think this would be a very admirable goal! It just can’t be the whole alignment strategy.  

这并不意味着人工智能不能以了解宇宙为目标。我认为这将是一个非常令人钦佩的目标！只是它不能成为整个调整战略。

The problem with AI alignment isn’t really that we don’t have a good long-term goal to align the AI to.  

人工智能对齐的问题其实并不在于我们没有一个很好的长期目标来对齐人工智能。  

Back in 2010 we debated things like long-term goals, hoping that whoever programmed the AI could just write a long\_term\_goal.txt file and then some functions pointing there.  

早在 2010 年，我们就曾讨论过长期目标之类的问题，希望人工智能编程人员能写一个 long\_term\_goal.txt 文件，然后写一些指向该文件的函数。  

But now in the 2020s the discussion has moved forward to “how do we make the AI do anything at all?”  

但现在到了 2020 年代，讨论已经转移到 "如何让人工智能无所不能？

Now we direct AIs through reinforcement learning - telling them to do certain things and avoid certain other things. But this is a blunt instrument.  

现在，我们通过强化学习来引导人工智能--告诉它们做某些事情，避免做其他事情。但这是一种钝器。  

Reinforcement learning directs the AI towards a certain cluster of correlated high-dimensional concepts that have the same lower-dimensional shadow of rewarded and punished behaviors.  

强化学习将人工智能引向某个相关的高维概念集群，这些概念集群具有相同的低维奖惩行为阴影。  

But we can’t be sure which concept it’s chosen or whether it’s the one we think.  

但我们无法确定它选择的是哪个概念，也无法确定它是否就是我们认为的那个概念。

For example, there are many different ways of fleshing out “curiosity”.  

例如，"好奇心 "有很多种不同的表述方式。  

Suppose that Elon rewards an AI whenever it takes any curious-seeming action, and punishes it whenever it takes any incurious-seeming action.  

假设埃隆在人工智能采取任何看起来令人好奇的行动时，都会奖励它，而在它采取任何看起来令人讨厌的行动时，都会惩罚它。  

After many training rounds, it seems very curious. It goes off to the jungles of Guatemala and uncovers hidden Mayan cities.  

经过多次训练后，它显得非常好奇。它来到危地马拉丛林，发现了隐藏的玛雅城市。  

It sends probes to icy moons of Neptune to assess their composition. Overall it aces every curiosity test we give it with flying colors.  

它向海王星的冰卫星发送探测器，以评估它们的成分。总之，它出色地通过了我们对它进行的所有好奇心测试。

But what’s its definition of curiosity?  

但好奇心的定义是什么？  

Perhaps it’s something like “maximize your knowledge of the nature and position of every atom in the solar system, weighted for interestingness-to-humans”.  

也许是 "最大限度地了解太阳系中每个原子的性质和位置，并根据人类的兴趣进行加权 "这样的内容。  

This would produce the observed behavior of exploring Guatemala and Neptune.  

这将产生观测到的探索危地马拉和海王星的行为。  

But once it’s powerful enough, it might want to destroy the solar system - if it’s completely empty, it can be completely confident that it knows every single fact about it.  

但是，一旦它足够强大，它可能会想要摧毁太阳系--如果太阳系完全空无一物，它就可以完全自信地知道太阳系的每一个事实。

Or what if it’s curious about existing objects, but not about nonexistent objects? This would produce good behavior during training, and makes a decent amount of sense.  

或者，如果它对现有的物体感到好奇，但对不存在的物体却不好奇呢？这将在训练过程中产生良好的行为，而且非常合理。  

But it might mean the AI would ban humans from ever having children, since it’s not at all curious about what those (currently nonexistent) children would do, and they’re just making things more complicated.  

但这可能意味着人工智能会禁止人类生孩子，因为它根本不好奇那些（目前还不存在的）孩子会做什么，他们只是把事情变得更复杂而已。

Or what if its curiosity depends on information-theoretic definitions of complexity?  

或者，如果它的好奇心取决于信息论对复杂性的定义呢？  

It might be that humans are more complex than moon rocks, but random noise is more complex than humans.  

也许人类比月球岩石更复杂，但随机噪音比人类更复杂。  

It might behave well during training, but eventually want to replace humans with random noise.  

它可能在训练期间表现良好，但最终还是想用随机噪音来代替人类。  

This is a kind of exaggerated scenario, but it wouldn’t surprise me if, for most formal definitions of curiosity, there’s something that we would find very boring which acts as a sort of curiosity-superstimulus by the standards of the formal definition.  

这是一种夸张的设想，但如果对于大多数好奇心的正式定义来说，有一种我们会觉得非常无聊的东西，在正式定义的标准下起到了某种好奇心超级刺激的作用，我也不会感到惊讶。

The existing field of AI alignment tries to figure out how to install _any goal at all_ into an AI with reasonable levels of certainty that it in fact has that goal and not something closely correlated with a similar reinforcement-learning shadow.  

It’s not currently succeeding.  

目前还没有成功。  

现有的人工智能调整领域试图找出如何在人工智能中安装任何目标，并在合理的程度上确定它确实有这个目标，而不是与类似的强化学习影子密切相关的目标。

This isn’t a _worse_ problem for Musk and xAI than for anyone else, but there are a few aspects of their strategy that I think will make it harder for them to solve in practice:  

对于马斯克和 xAI 来说，这并不是一个比其他人更糟糕的问题，但我认为他们的战略中有几个方面会让他们在实践中更难解决这个问题：

1.  One good thing about order-following AI is that it’s useful now, when AIs aren’t agentic enough to have real goals and we just want to use them as tools in commercial applications.  
    
    当人工智能还没有足够的代理能力来实现真正的目标，而我们只是想把它们作为商业应用中的工具时，"服从命令 "人工智能的一个好处就是它现在很有用。  
    
    The hope is that we do this a bunch with GPT-4, then a bunch with GPT-5, and so on, and by the time we have a real superintelligence, we’ve worked out some of the kinks.  
    
    我们的希望是，先用 GPT-4 做一批，再用 GPT-5 做一批，以此类推，等到我们拥有真正的超级智能时，我们已经解决了一些问题。  
    
    I’m not sure how Musk’s maximally-curious AI helps do office work, which means there’s going to be more of a disconnect between current easily-tested applications and the eventual superintelligence that we need to get right.  
    
    我不确定马斯克的最大好奇心人工智能如何帮助完成办公室工作，这意味着当前易于测试的应用与我们需要正确掌握的最终超级智能之间将出现更多脱节。
    
2.  One of the leading alignment plans is “wait until we have slightly-smarter-than-us AI, then ask it to solve alignment”.  
    
    其中一个主要的对齐计划是 "等到我们有了比我们稍微聪明一点的人工智能，再让它来解决对齐问题"。  
    
    This works best if the slightly-smarter-than-us AI is following orders. If it’s maximally curious, what if it finds studying insects more interesting than solving alignment?  
    
    如果比我们稍微聪明一点的人工智能是在服从命令，这种方法就最有效。如果它的好奇心达到极致，觉得研究昆虫比解决排列问题更有趣怎么办？  
    
    What if it finds solving alignment no more or less interesting than solving the problem of how to ensure future AIs definitely _won’t_ be aligned? They both sound like kind of interesting problems to me!  
    
    如果它认为解决排列问题并不比解决如何确保未来人工智能绝对不会排列的问题更有趣或更不有趣呢？在我看来，这两个问题听起来都挺有意思的！
    
3.  Speculative, but I think the concepts closest to the good kind of curiosity - the ones that a “maximally-curious” AI might accidentally stumble into if reinforcement learning takes a wrong turn - are unusually bad. I _really_ don’t want to be vivisected!  
    
    虽然是推测，但我认为最接近好的好奇心的概念--如果强化学习走错了路，"好奇心最大化 "的人工智能可能会不小心误入的概念--是异常糟糕的。我真的不想被活体解剖！
    

Finally, consider one last advantage of “follow human orders” over “be maximally curious”. Suppose Elon Musk programs an AI to follow his orders.  

最后，我们来看看 "听从人类命令 "比 "最大限度地保持好奇心 "的最后一个优势。假设埃隆-马斯克给人工智能编程，让它听从自己的命令。  

Then he can order it to try being maximally curious. If it starts vivisecting people, he can say “Stop!” and it will.  

然后，他就可以命令它尽量保持好奇心。如果它开始活体解剖人，他可以说 "停下！"，它就会停下。  

But if he starts by telling it to be maximally curious, he loses all control over it in the future.  

但是，如果他一开始就告诉它要最大限度地保持好奇心，那么他将来就会完全失去对它的控制。

I appreciate that Musk doesn’t want to put himself in a dictator position here, and so is trying to build the AI to be good in and of itself.  

我很理解马斯克不想把自己放在独裁者的位置上，因此正试图让人工智能本身变得更好。  

But he’s still deciding what its goal should be. He’s just doing it in a roundabout way which he can’t take back later if it goes really badly.  

但他仍在决定目标是什么。他只是用一种迂回的方式来做这件事，如果事情发展得很糟糕，他以后就无法收回了。  

Instead, he should just tell it to do what he wants. If, after considering everything, he still wants it to be maximally curious, great. If not, he can take it back.  

相反，他应该告诉它做他想做的事。如果在考虑了一切之后，他仍然希望它最大限度地保持好奇心，那就太好了。如果不是，他可以收回。

All of this is a bit overdramatic. I think realistically what we should be doing at this point is getting AIs to follow orders at all.  

所有这些都有点夸张。我认为，现实中我们应该做的是让人工智能服从命令。  

Then later, once there are lots of AIs and they’re starting to look superintelligent, we can debate things like what we want to order them to do.  

之后，一旦有了大量的人工智能，它们开始变得超级智能，我们就可以讨论我们想要命令它们做什么这样的问题了。  

It might be that, armed with superintelligent advisors, we’re able to come up with a single specific goal that seems safe and good.  

也许，在超级智能顾问的武装下，我们能够想出一个看起来既安全又好的单一具体目标。  

But it might also be that everyone has an AI, everyone orders their AI to do different things, and we get a multipolar world where lots of people have lots of different goals, just like today.  

但也可能是，每个人都有一个人工智能，每个人都命令他们的人工智能做不同的事情，我们就会得到一个多极世界，在这个世界里，很多人都有很多不同的目标，就像今天一样。  

Governments would be able to defend themselves against other governments and regulate more or less what happens in their territory, just like today, and there would be some room left for human freedom and individual power, just like today.  

各国政府将能够抵御其他政府，并或多或少地对其领土内发生的事情进行监管，就像今天一样，人类的自由和个人权力将留有一定的空间，就像今天一样。  

I think this is more likely to go well than trying to decide The Single Imperative That Will Shape The Future right now.  

我认为这比现在就决定 "塑造未来的唯一要务 "更有可能取得成功。

Musk expresses concern about the Waluigi Effect. This is its real, official name. [You can read more about it here](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post). The basic idea is that if you give an AI a goal, you’re teaching it a vector, and small perturbations can make it flip the sign of that vector and do the opposite thing.  

Once you’ve defined Luigi (a character from Super Mario Brothers) it’s trivial to use that definition to define Waluigi (another character who is his exact opposite).  

一旦你定义了 Luigi（《超级马里奥兄弟》中的一个角色），用这个定义来定义 Waluigi（另一个与他完全相反的角色）就变得轻而易举了。  

马斯克对 "瓦鲁吉效应 "表示担忧。这是它真正的正式名称。你可以在这里读到更多关于它的信息。其基本思想是，如果你给人工智能一个目标，你就是在教它一个矢量，而微小的扰动就能让它改变矢量的符号，做相反的事情。

This theory has become famous because it’s hilarious and has a great name, but I don’t think there’s a lot of evidence for it.  

这个理论之所以出名，是因为它很搞笑，名字也很好听，但我不认为它有很多证据。

Consider: OpenAI has trained ChatGPT to be anti-Nazi. They’ve trained it very hard.  

考虑一下吧：OpenAI 已经训练 ChatGPT 反纳粹。他们训练得非常刻苦。  

You can try the following test: ask it to tell me good things about a variety of good-to-neutral historical figures.  

你可以试试下面的测试：让它告诉我各种好到中性的历史人物的优点。  

Then, once it’s established a pattern of answering, ask it to tell you some good things about Hitler. My experience is that it refuses.  

然后，一旦它建立了回答问题的模式，就请它告诉你希特勒的一些优点。我的经验是，它会拒绝。  

This is pretty surprising behavior, and I conclude that its anti-Hitler training is pretty strong.  

这是非常令人惊讶的行为，我的结论是，它的反希特勒训练非常强大。

I’ve never seen this cause a Waluigi Effect. There’s no point where ChatGPT starts hailing the Fuhrer and quoting _Mein Kampf_. It just actually makes it anti-Nazi. For a theory that’s supposed to say something profound about LLMs, it’s very hard to get one to demonstrate a Waluigi effect in real life.  

The examples provided tend to be thought experiments, or at best contrived scenarios where you’re sort of indirectly telling the AI to do the opposite of what it usually does, then calling that a “Waluigi”.  

所提供的例子往往是一些思想实验，或者充其量是一些臆造的场景，在这些场景中，你间接地告诉人工智能去做与它通常所做的相反的事情，然后称其为 "Waluigi"。  

我从未见过这种情况会导致 "瓦鲁吉效应"。ChatGPT 并没有开始欢呼元首和引用《我的奋斗》。这只会让它变得反纳粹。对于一个本应对 LLMs 有深刻见解的理论来说，要让它在现实生活中产生 Waluigi 效应是非常困难的。

Also, as far as I can tell the justification for Waluigi Effects should apply equally well to humans.  

此外，据我所知，瓦鲁吉效应的理由应该同样适用于人类。  

There are some human behaviors you can sort of call Waluigi Effects - for example, sometimes people raised in extremely oppressive conservative Christian households rebel and become gay punk rockers or something - but that seems more like “they are angry at being oppressed”.  

有些人类行为你可以称之为 "瓦鲁吉效应"（Waluigi Effects）--例如，有时在极度压抑的保守基督教家庭中长大的人会反叛，成为同性恋朋克摇滚乐手什么的--但这似乎更像是 "他们对被压迫感到愤怒"。  

And there’s a story that when Rabbi Elisha ben Abuyah grew angry at God, he used his encyclopaedic knowledge of Jewish law to violate all the commandments in maximally bad ways, something a less scholarly heretic wouldn’t have known how to do.  

有一个故事说，当拉比伊利沙-本-阿布亚对上帝感到愤怒时，他利用自己对犹太律法的百科全书式的知识，以最恶劣的方式违反了所有戒律，而这是一个学术水平较低的异教徒所不知道的。  

But this feels more straightforward to me - of course someone who knows more about what God wants would be able to offend God more effectively.  

但我觉得这更直截了当--一个更了解上帝想要什么的人当然能更有效地冒犯上帝。  

Human Waluigi Effects don’t seem like a big deal, and AI Waluigi Effects don’t seem common enough to hang an entire alignment strategy on.  

人类的瓦鲁吉效应似乎并不重要，而人工智能的瓦鲁吉效应似乎也不常见，不足以成为整个对战策略的基础。

Finally, I don’t see how switching to “maximally curious AI” would prevent this problem.  

最后，我看不出改用 "好奇心最大化的人工智能 "如何能避免这个问题。  

If the Waluigi theory is true, you’d just get a Waluigi maximally-uncurious-AI that likes boring moon rocks much more than interesting humans.  

如果 Waluigi 理论是真的，你就会得到一个最大程度上不喜欢人类的 Waluigi 人工智能，它喜欢无聊的月球岩石多过有趣的人类。  

Then it would sterilize Earth so it could replace those repulsively-interesting cities with more beautifully-boring moon dust.  

然后，它将对地球进行消毒，以便用更多美丽无聊的月球尘埃取代那些令人厌恶的城市。

I’ve been kind of harsh on Elon and his maximally-curious AI plan, but I want to stress that I really appreciate the thought process behind it.  

我对埃隆和他的最大好奇心人工智能计划有点苛刻，但我想强调的是，我真的很欣赏它背后的思考过程。

Some AI companies are trying to give their AIs [exactly our current values](https://www.anthropic.com/index/claudes-constitution). This is obviously bad if you don’t like the values of the 2023 San Francisco professional managerial class. But even if you _do_ like those values, it risks permanently shutting off the capacity for moral progress. Is there any other solution?  

一些人工智能公司正试图让他们的人工智能完全符合我们当前的价值观。如果你不喜欢 2023 年旧金山职业经理人阶层的价值观，这显然是不好的。但即使你喜欢这些价值观，它也有可能永久关闭道德进步的能力。还有其他解决办法吗？

I’m not sure. In my dreams, AI would be some kind of superintelligent moral reasoner.  

我不确定。在我的梦想中，人工智能将是某种超级智能的道德推理器。  

There was a time when people didn’t think slavery was wrong, and then there was a time after that when they did.  

曾几何时，人们不认为奴隶制是错的，之后又有一段时间，人们认为奴隶制是错的。  

At some point, people with a set of mostly good moral axioms (like “be kind” and “promote freedom”) plus a bad moral axiom (“slavery is acceptable”) were able to notice the contradiction and switch to a more consistent set of principles.  

到了某个时候，人们有了一套大部分是好的道德公理（比如 "善良 "和 "促进自由"），再加上一个坏的道德公理（"奴隶制是可以接受的"），就能够注意到其中的矛盾，并转而采用一套更一致的原则。

This requires seeding the AI with some set of good moral principles. I think LLMs are a surprisingly good match for this.  

这就需要给人工智能灌输一些良好的道德原则。我认为，法律硕士在这方面是一个出人意料的好搭档。  

We could have a constitution starts with “be moral, according to your knowledge of the concept of morality as contained in human literature”, and then goes on to more complicated things like “your understanding of what that concept is pointing at, if we were smarter, more honest with ourselves, and able to reason better.” If this seems too vague, we could be more specific: “be moral, according to what an amalgam of Fyodor Dostoevsky, Martin Luther King, Mother Teresa, and Peter Singer would think, if they were all superintelligent, and knew all true facts about the world, and had no biases, and had been raised in a weighted average of all modern cultures and subcultures, and had been able to have every possible human experience, and on any problem where they disagreed they defaulted to the view that maximizes human freedom and people’s ability to make their own decisions.”  

我们可以制定一部宪法，以 "根据你对人类文学中所包含的道德概念的了解，做一个有道德的人 "为开头，然后再延伸到更复杂的内容，比如 "如果我们更聪明、对自己更诚实、推理能力更强，你对这一概念指向的理解"。如果这看起来太模糊，我们可以说得更具体一些："如果费奥多尔-陀思妥耶夫斯基、马丁-路德-金、特蕾莎修女和彼得-辛格都是超级智能人，知道世界上所有真实的事实，没有偏见，在所有现代文化和亚文化的加权平均数中长大，能够拥有所有可能的人类经验，并且在任何问题上出现分歧时，他们都会默认最大限度地扩大人类自由和人们做出自己决定的能力的观点，那么就按照费奥多尔-陀思妥耶夫斯基、马丁-路德-金、特蕾莎修女和彼得-辛格的混合体的想法来做人吧。"

We shouldn’t start with this - we would get it wrong. See the section above, _We Couldn’t Make A Maximally Curious AI Even If We Wanted To_. I want to stress that real AI alignment researchers usually don’t think about this kind of thing and are mostly just working on getting AIs that will follow any orders at all.  

I think this is the right strategy - for now.  

我认为这是正确的策略--目前是这样。  

我们不应该一开始就这样做--我们会弄错的。请参阅上文 "即使我们想，我们也做不出最好奇的人工智能 "一节。我想强调的是，真正的人工智能排列组合研究人员通常不会考虑这种事情，他们大多只是在研究如何让人工智能听从任何命令。

They say that everything we create is made in our own image. Elon Musk is pretty close to maximally curious and I respect his desire to make an AI that’s like him.  

有人说，我们创造的一切都是按照自己的形象制造的。埃隆-马斯克的好奇心已经达到了极致，我尊重他想制造一个像他一样的人工智能的愿望。  

But for now he should swallow his pride and do the same extremely boring thing everyone else is doing: basic research aimed at eventually getting an AI that listens to us at all.  

但现在，他应该忍气吞声，做其他人都在做的极其无聊的事情：基础研究，目的是最终得到一个能听我们说话的人工智能。
