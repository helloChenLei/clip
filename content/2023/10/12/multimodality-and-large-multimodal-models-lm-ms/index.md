---
title: "å¤šæ¨¡æ€å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ (LMM)"
date: 2023-10-12T05:50:23+08:00
updated: 2023-10-12T05:50:23+08:00
taxonomies:
  tags: []
extra:
  source: https://huyenchip.com/2023/10/10/multimodal.html
  hostname: huyenchip.com
  author: Chip Huyen
  original_title: "Multimodality and Large Multimodal Models (LMMs)"
  original_lang: zh
---

For a long time, each ML model operated in one data mode â€“ text (translation, language modeling), image (object detection, image classification), or audio (speech recognition).  

é•¿æœŸä»¥æ¥ï¼Œæ¯ä¸ª ML æ¨¡å‹éƒ½åœ¨ä¸€ç§æ•°æ®æ¨¡å¼ä¸‹è¿è¡Œ--æ–‡æœ¬ï¼ˆç¿»è¯‘ã€è¯­è¨€å»ºæ¨¡ï¼‰ã€å›¾åƒï¼ˆç‰©ä½“æ£€æµ‹ã€å›¾åƒåˆ†ç±»ï¼‰æˆ–éŸ³é¢‘ï¼ˆè¯­éŸ³è¯†åˆ«ï¼‰ã€‚

However, natural intelligence is not limited to just a single modality.  

ç„¶è€Œï¼Œè‡ªç„¶æ™ºèƒ½å¹¶ä¸å±€é™äºå•ä¸€çš„æ¨¡å¼ã€‚  

Humans can read and write text. We can see images and watch videos.  

äººç±»å¯ä»¥é˜…è¯»å’Œä¹¦å†™æ–‡å­—ã€‚æˆ‘ä»¬å¯ä»¥è§‚çœ‹å›¾åƒå’Œè§†é¢‘ã€‚  

We listen to music to relax and watch out for strange noises to detect danger.  

æˆ‘ä»¬å¬éŸ³ä¹æ¥æ”¾æ¾ï¼Œç•™æ„å¥‡æ€ªçš„å£°éŸ³æ¥å‘ç°å±é™©ã€‚  

Being able to work with multimodal data is essential for us or any AI to operate in the real world.  

èƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€æ•°æ®å¯¹äºæˆ‘ä»¬æˆ–ä»»ä½•äººå·¥æ™ºèƒ½åœ¨ç°å®ä¸–ç•Œä¸­çš„è¿è¡Œéƒ½è‡³å…³é‡è¦ã€‚

OpenAI noted in their [GPT-4V system card](https://cdn.openai.com/papers/GPTV_System_Card.pdf) that â€œ_incorporating additional modalities (such as image inputs) into LLMs is viewed by some as a key frontier in AI research and development_.â€  

OpenAI åœ¨å…¶ GPT-4V ç³»ç»Ÿå¡ä¸­æŒ‡å‡ºï¼Œ"å°†å…¶ä»–æ¨¡å¼ï¼ˆå¦‚å›¾åƒè¾“å…¥ï¼‰çº³å…¥ LLMï¼Œè¢«ä¸€äº›äººè§†ä¸ºäººå·¥æ™ºèƒ½ç ”å‘çš„å…³é”®å‰æ²¿"ã€‚

Incorporating additional modalities to LLMs (Large Language Models) creates LMMs (Large Multimodal Models).  

åœ¨ LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰ä¸­åŠ å…¥é¢å¤–çš„æ¨¡æ€ï¼Œå°±å½¢æˆäº† LMMï¼ˆå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼‰ã€‚  

In the last year, every week, a major research lab introduced a new LMM, e.g.  

åœ¨è¿‡å»çš„ä¸€å¹´é‡Œï¼Œæ¯å‘¨éƒ½æœ‰ä¸€ä¸ªä¸»è¦ç ”ç©¶å®éªŒå®¤æ¨å‡ºæ–°çš„ LMMã€‚  

DeepMindâ€™s Flamingo, Salesforceâ€™s BLIP, Microsoftâ€™s KOSMOS-1, Googleâ€™s PaLM-E, and Tencentâ€™s Macaw-LLM. Chatbots like [ChatGPT](https://openai.com/blog/chatgpt-can-now-see-hear-and-speak) and [Gemini](https://blog.google/technology/ai/google-io-2023-keynote-sundar-pichai/#palm-2-gemini) are LMMs.  

DeepMind çš„ Flamingoã€Salesforce çš„ BLIPã€å¾®è½¯çš„ KOSMOS-1ã€è°·æ­Œçš„ PaLM-E å’Œè…¾è®¯çš„ Macaw-LLMã€‚ChatGPT å’Œ Gemini ç­‰èŠå¤©æœºå™¨äººéƒ½å±äº LMMã€‚

Not all multimodal systems are LMMs. For example, text-to-image models like Midjourney, Stable Diffusion, and Dall-E are multimodal but donâ€™t have a language model component.  

å¹¶éæ‰€æœ‰å¤šæ¨¡æ€ç³»ç»Ÿéƒ½æ˜¯ LMMã€‚ä¾‹å¦‚ï¼ŒMidjourneyã€Stable Diffusion å’Œ Dall-E ç­‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ˜¯å¤šæ¨¡æ€çš„ï¼Œä½†æ²¡æœ‰è¯­è¨€æ¨¡å‹ç»„ä»¶ã€‚  

Multimodal can mean one or more of the following:  

å¤šæ¨¡å¼å¯ä»¥æŒ‡ä»¥ä¸‹ä¸€ç§æˆ–å¤šç§æ¨¡å¼ï¼š

1.  Input and output are of different modalities (e.g. text-to-image, image-to-text)  
    
    è¾“å…¥å’Œè¾“å‡ºä¸ºä¸åŒæ¨¡å¼ï¼ˆå¦‚æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°æ–‡æœ¬ï¼‰
2.  Inputs are multimodal (e.g. a system that can process both text and images)  
    
    è¾“å…¥æ˜¯å¤šæ¨¡æ€çš„ï¼ˆä¾‹å¦‚ï¼Œç³»ç»Ÿå¯ä»¥å¤„ç†æ–‡æœ¬å’Œå›¾åƒï¼‰
3.  Outputs are multimodal (e.g. a system that can generate both text and images)  
    
    è¾“å‡ºæ˜¯å¤šæ¨¡æ€çš„ï¼ˆä¾‹å¦‚ï¼Œç³»ç»Ÿå¯ä»¥ç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒï¼‰

This post covers multimodal systems in general, including LMMs. It consists of 3 parts.  

è¿™ç¯‡æ–‡ç« æ¶µç›–äº†åŒ…æ‹¬ LMM åœ¨å†…çš„ä¸€èˆ¬å¤šæ¨¡æ€ç³»ç»Ÿã€‚å®ƒç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆã€‚

-   Part 1 covers the context for multimodality, including why multimodal, different data modalities, and types of multimodal tasks.  
    
    ç¬¬ 1 éƒ¨åˆ†æ¶‰åŠå¤šæ¨¡æ€çš„èƒŒæ™¯ï¼ŒåŒ…æ‹¬ä¸ºä»€ä¹ˆè¦é‡‡ç”¨å¤šæ¨¡æ€ã€ä¸åŒçš„æ•°æ®æ¨¡æ€ä»¥åŠå¤šæ¨¡æ€ä»»åŠ¡çš„ç±»å‹ã€‚
-   Part 2 discusses the fundamentals of a multimodal system, using the examples of CLIP, which lays the foundation for many future multimodal systems, and Flamingo, whose impressive performance gave rise to LMMs.  
    
    ç¬¬ 2 éƒ¨åˆ†ä»¥ CLIP å’Œ Flamingo ä¸ºä¾‹ï¼Œè®¨è®ºäº†å¤šæ¨¡æ€ç³»ç»Ÿçš„åŸºæœ¬åŸç†ï¼Œå‰è€…ä¸ºè®¸å¤šæœªæ¥çš„å¤šæ¨¡æ€ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ï¼Œè€Œåè€…çš„å‡ºè‰²è¡¨ç°åˆ™å‚¬ç”Ÿäº† LMMã€‚
-   Part 3 discusses some active research areas for LMMs, including generating multimodal outputs and adapters for more efficient multimodal training, covering newer multimodal systems such as BLIP-2, LLaVA, LLaMA-Adapter V2, LAVIN, etc.  
    
    ç¬¬ 3 éƒ¨åˆ†è®¨è®ºäº† LMM çš„ä¸€äº›æ´»è·ƒç ”ç©¶é¢†åŸŸï¼ŒåŒ…æ‹¬ç”Ÿæˆå¤šæ¨¡æ€è¾“å‡ºå’Œé€‚é…å™¨ä»¥å®ç°æ›´é«˜æ•ˆçš„å¤šæ¨¡æ€è®­ç»ƒï¼Œæ¶µç›–äº† BLIP-2ã€LLaVAã€LLaMA-Adapter V2ã€LAVIN ç­‰è¾ƒæ–°çš„å¤šæ¨¡æ€ç³»ç»Ÿã€‚

The post is long. Feel free to skip to the sections most interesting to you.  

å¸–å­å¾ˆé•¿ã€‚è¯·éšæ„è·³åˆ°æ‚¨æœ€æ„Ÿå…´è¶£çš„éƒ¨åˆ†ã€‚

**âš  Ambiguous terminology âš   

æœ¯è¯­ä¸æ˜ç¡® âš **  

Multimodal data can also refer to multimodal distributions, e.g.  

å¤šæ¨¡æ€æ•°æ®ä¹Ÿå¯æŒ‡å¤šæ¨¡æ€åˆ†å¸ƒï¼Œä¾‹å¦‚  

bimodal distribution, which is different from multimodal data in this post.  

åŒæ¨¡æ€åˆ†å¸ƒï¼Œä¸æœ¬å¸–ä¸­çš„å¤šæ¨¡æ€æ•°æ®ä¸åŒã€‚


## Part 1. Understanding Multimodal  

ç¬¬ 1 éƒ¨åˆ†.äº†è§£å¤šæ¨¡å¼

Many use cases are impossible without multimodality, especially those in industries that deal with a mixture of data modalities such as healthcare, robotics, e-commerce, retail, gaming, etc.  

è®¸å¤šä½¿ç”¨æ¡ˆä¾‹éƒ½ç¦»ä¸å¼€å¤šæ¨¡æ€æŠ€æœ¯ï¼Œå°¤å…¶æ˜¯é‚£äº›éœ€è¦å¤„ç†å¤šç§æ•°æ®æ¨¡æ€çš„è¡Œä¸šï¼Œå¦‚åŒ»ç–—ä¿å¥ã€æœºå™¨äººã€ç”µå­å•†åŠ¡ã€é›¶å”®ã€æ¸¸æˆç­‰ã€‚

![Multimodal AI in healthcare](26-healthcare.png)

An example of how multimodality can be used in healthcare.  

å¤šæ¨¡æ€æŠ€æœ¯åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„åº”ç”¨å®ä¾‹ã€‚  

Image from Multimodal biomedical AI (Acosta et al., Nature Medicine 2022)  

å›¾ç‰‡æ¥è‡ªå¤šæ¨¡æ€ç”Ÿç‰©åŒ»å­¦äººå·¥æ™ºèƒ½ï¼ˆAcosta ç­‰äººï¼Œã€Šè‡ªç„¶åŒ»å­¦ã€‹ï¼Œ2022 å¹´ï¼‰

Not only that, incorporating data from other modalities can help boost model performance.  

ä¸ä»…å¦‚æ­¤ï¼Œç»“åˆå…¶ä»–æ¨¡å¼çš„æ•°æ®ä¹Ÿæœ‰åŠ©äºæé«˜æ¨¡å‹æ€§èƒ½ã€‚  

Shouldnâ€™t a model that can learn from both text and images perform better than a model that can learn from only text or only image?  

ä¸€ä¸ªæ—¢èƒ½ä»æ–‡æœ¬åˆèƒ½ä»å›¾åƒä¸­å­¦ä¹ çš„æ¨¡å‹ï¼Œéš¾é“ä¸åº”è¯¥æ¯”ä¸€ä¸ªåªèƒ½ä»æ–‡æœ¬æˆ–å›¾åƒä¸­å­¦ä¹ çš„æ¨¡å‹è¡¨ç°æ›´å¥½å—ï¼Ÿ

Multimodal systems can provide a more flexible interface, allowing you to interact with them in whichever way works best for you at the moment.  

å¤šæ¨¡æ€ç³»ç»Ÿå¯ä»¥æä¾›æ›´çµæ´»çš„ç•Œé¢ï¼Œè®©æ‚¨ä»¥æœ€é€‚åˆè‡ªå·±çš„æ–¹å¼ä¸ç³»ç»Ÿäº’åŠ¨ã€‚  

Imagine you can ask a question by typing, talking, or just pointing your camera at something.  

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ å¯ä»¥é€šè¿‡æ‰“å­—ã€è¯´è¯æˆ–å°†æ‘„åƒå¤´å¯¹å‡†æŸç‰©æ¥æé—®ã€‚

One use case that Iâ€™m especially excited about, is that multimodality can also enable visually impaired people to browse the Internet and also navigate the real world.  

è®©æˆ‘æ„Ÿåˆ°ç‰¹åˆ«å…´å¥‹çš„ä¸€ä¸ªç”¨ä¾‹æ˜¯ï¼Œå¤šæ¨¡æ€æŠ€æœ¯è¿˜èƒ½è®©è§†éšœäººå£«æµè§ˆäº’è”ç½‘ï¼Œå¹¶æµè§ˆç°å®ä¸–ç•Œã€‚

![Some cool multimodal use cases from GPT-4V](1-gpt-4v-use-cases.png)

Some cool multimodal use cases from GPT-4V  

æ¥è‡ª GPT-4V çš„ä¸€äº›å¾ˆé…·çš„å¤šæ¨¡å¼ä½¿ç”¨æ¡ˆä¾‹

## Data modalities  

æ•°æ®æ¨¡å¼

Different data modes are text, image, audio, tabular data, etc. One data mode can be represented or _approximated_ in another data mode. For example:  

ä¸åŒçš„æ•°æ®æ¨¡å¼åŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è¡¨æ ¼æ•°æ®ç­‰ã€‚ä¸€ç§æ•°æ®æ¨¡å¼å¯ä»¥ç”¨å¦ä¸€ç§æ•°æ®æ¨¡å¼è¡¨ç¤ºæˆ–è¿‘ä¼¼ã€‚ä¾‹å¦‚

-   Audio can be represented as images (mel spectrograms).  
    
    éŸ³é¢‘å¯ä»¥ç”¨å›¾åƒï¼ˆæ¢…å°”é¢‘è°±å›¾ï¼‰è¡¨ç¤ºã€‚
-   Speech can be transcribed into text, though its text-only representation loses information such as volume, intonation, pauses, etc.  
    
    è™½ç„¶è¯­éŸ³å¯ä»¥è½¬å½•æˆæ–‡æœ¬ï¼Œä½†åªç”¨æ–‡æœ¬è¡¨ç¤ºä¼šä¸¢å¤±éŸ³é‡ã€è¯­è°ƒã€åœé¡¿ç­‰ä¿¡æ¯ã€‚
-   An image can be represented as a vector, which, in turn, can be flattened and represented as a sequence of text tokens.  
    
    å›¾åƒå¯ä»¥è¡¨ç¤ºä¸ºçŸ¢é‡ï¼Œè€ŒçŸ¢é‡åˆå¯ä»¥æ‰å¹³åŒ–å¹¶è¡¨ç¤ºä¸ºæ–‡æœ¬æ ‡è®°åºåˆ—ã€‚
-   A video is a sequence of images plus audio.  
    
    è§†é¢‘æ˜¯ä¸€è¿ä¸²å›¾åƒå’ŒéŸ³é¢‘ã€‚  
    
    ML models today mostly treat videos as sequences of images.  
    
    å¦‚ä»Šçš„ ML æ¨¡å‹å¤§å¤šå°†è§†é¢‘è§†ä¸ºå›¾åƒåºåˆ—ã€‚  
    
    This is a severe limitation, as sounds have proved to be just as important as visuals for videos. [88% of TikTok users shared that sound is essential for their TikTok experience](https://www.kantar.com/uki/inspiration/advertising-media/the-power-of-tiktok).  
    
    è¿™æ˜¯ä¸€ä¸ªä¸¥é‡çš„é™åˆ¶ï¼Œå› ä¸ºäº‹å®è¯æ˜ï¼Œå£°éŸ³ä¸è§†é¢‘çš„è§†è§‰æ•ˆæœåŒæ ·é‡è¦ã€‚88% çš„ TikTok ç”¨æˆ·è®¤ä¸ºå£°éŸ³å¯¹ä»–ä»¬çš„ TikTok ä½“éªŒè‡³å…³é‡è¦ã€‚
-   A text can be represented as an image if you simply take a picture of it.  
    
    åªéœ€æ‹æ‘„ä¸€å¼ ç…§ç‰‡ï¼Œæ–‡å­—å°±èƒ½ä»¥å›¾åƒçš„å½¢å¼è¡¨ç°å‡ºæ¥ã€‚
-   A data table can be converted into a chart, which is an image.  
    
    æ•°æ®è¡¨å¯ä»¥è½¬æ¢æˆå›¾è¡¨ï¼Œä¹Ÿå°±æ˜¯å›¾åƒã€‚

___

**How about other data modalities?  

å…¶ä»–æ•°æ®æ¨¡å¼å¦‚ä½•ï¼Ÿ**

All digital data formats can be represented using bitstrings (strings of 0 and 1) or bytestrings.  

æ‰€æœ‰æ•°å­—æ•°æ®æ ¼å¼éƒ½å¯ä»¥ç”¨ä½å­—ç¬¦ä¸²ï¼ˆ0 å’Œ 1 çš„å­—ç¬¦ä¸²ï¼‰æˆ–å­—èŠ‚å­—ç¬¦ä¸²æ¥è¡¨ç¤ºã€‚  

A model that can effectively learn from bitstrings or bytestrings will be very powerful, and it can learn from any data mode.  

èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ ä½å­—ç¬¦ä¸²æˆ–å­—èŠ‚å­—ç¬¦ä¸²çš„æ¨¡å‹å°†éå¸¸å¼ºå¤§ï¼Œå®ƒå¯ä»¥å­¦ä¹ ä»»ä½•æ•°æ®æ¨¡å¼ã€‚  

There are other data modalities we havenâ€™t touched on, such as graphs and 3D assets.  

æˆ‘ä»¬è¿˜æ²¡æœ‰æ¶‰åŠå…¶ä»–æ•°æ®æ¨¡å¼ï¼Œå¦‚å›¾è¡¨å’Œ 3D èµ„äº§ã€‚  

We also havenâ€™t touched on the formats used to represent smell and touch (haptics).  

æˆ‘ä»¬è¿˜æ²¡æœ‰æ¶‰åŠç”¨äºè¡¨ç¤ºå—…è§‰å’Œè§¦è§‰ï¼ˆè§¦è§‰ï¼‰çš„æ ¼å¼ã€‚

___

  

In ML today, audio is still largely treated as a voice-based alternative to text.  

åœ¨å½“ä»Šçš„ ML ä¸­ï¼ŒéŸ³é¢‘åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»è¢«è§†ä¸ºæ–‡æœ¬çš„è¯­éŸ³æ›¿ä»£å“ã€‚  

The most common use cases for audio are still speech recognition (speech-to-text) and speech synthesis (text-to-speech).  

éŸ³é¢‘æœ€å¸¸è§çš„ä½¿ç”¨æƒ…å†µä»ç„¶æ˜¯è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³åˆ°æ–‡æœ¬ï¼‰å’Œè¯­éŸ³åˆæˆï¼ˆæ–‡æœ¬åˆ°è¯­éŸ³ï¼‰ã€‚  

Non-speech audio use cases, e.g. music generation, are still pretty niche.  

éè¯­éŸ³éŸ³é¢‘ä½¿ç”¨æ¡ˆä¾‹ï¼Œå¦‚éŸ³ä¹ç”Ÿæˆï¼Œä»ç„¶éå¸¸å°ä¼—ã€‚  

See the fake Drake & Weeknd song and [MusicGen model on HuggingFace](https://huggingface.co/spaces/facebook/MusicGen).  

åœ¨ HuggingFace ä¸ŠæŸ¥çœ‹å‡å†’çš„ Drake & Weeknd æ­Œæ›²å’Œ MusicGen æ¨¡å‹ã€‚

<iframe width="560" height="315" src="https://www.youtube.com/embed/7HZ2ie2ErFI?si=3RTESVN4kfFRw_WR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>

Image is perhaps the most versatile format for model inputs, as it can be used to represent text, tabular data, audio, and to some extent, videos.  

å›¾åƒå¯èƒ½æ˜¯æœ€é€šç”¨çš„æ¨¡å‹è¾“å…¥æ ¼å¼ï¼Œå› ä¸ºå®ƒå¯ä»¥ç”¨æ¥è¡¨ç¤ºæ–‡æœ¬ã€è¡¨æ ¼æ•°æ®ã€éŸ³é¢‘ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šè¿˜å¯ä»¥è¡¨ç¤ºè§†é¢‘ã€‚  

Thereâ€™s also so much more visual data than text data.  

å¯è§†åŒ–æ•°æ®ä¹Ÿæ¯”æ–‡æœ¬æ•°æ®å¤šå¾—å¤šã€‚  

We have phones/webcams that constantly take pictures and videos today.  

å¦‚ä»Šï¼Œæˆ‘ä»¬çš„æ‰‹æœº/ç½‘ç»œæ‘„åƒå¤´å¯ä»¥ä¸æ–­æ‹ç…§å’Œå½•åƒã€‚

Text is a much more powerful mode for model outputs.  

æ–‡æœ¬æ˜¯ä¸€ç§åŠŸèƒ½æ›´å¼ºå¤§çš„æ¨¡å‹è¾“å‡ºæ¨¡å¼ã€‚  

A model that can generate images can only be used for image generation, whereas a model that can generate text can be used for many tasks: summarization, translation, reasoning, question answering, etc.  

èƒ½ç”Ÿæˆå›¾åƒçš„æ¨¡å‹åªèƒ½ç”¨äºç”Ÿæˆå›¾åƒï¼Œè€Œèƒ½ç”Ÿæˆæ–‡æœ¬çš„æ¨¡å‹åˆ™å¯ç”¨äºå¤šç§ä»»åŠ¡ï¼šæ‘˜è¦ã€ç¿»è¯‘ã€æ¨ç†ã€é—®é¢˜è§£ç­”ç­‰ã€‚

For simplicity, weâ€™ll focus on 2 modalities: images and text.  

ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†é‡ç‚¹è®¨è®ºä¸¤ç§æ¨¡å¼ï¼šå›¾åƒå’Œæ–‡æœ¬ã€‚  

The learnings can be somewhat generalized to other modalities.  

è¿™äº›ç»éªŒåœ¨æŸç§ç¨‹åº¦ä¸Šå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–æ¨¡å¼ã€‚

## Multimodal tasks  

å¤šæ¨¡å¼ä»»åŠ¡

To understand multimodal systems, itâ€™s helpful to look at the tasks they are built to solve.  

è¦äº†è§£å¤šæ¨¡æ€ç³»ç»Ÿï¼Œæœ€å¥½å…ˆäº†è§£ä¸€ä¸‹è¿™äº›ç³»ç»Ÿæ˜¯ç”¨æ¥è§£å†³ä»€ä¹ˆä»»åŠ¡çš„ã€‚  

There are many tasks and many possible ways to organize them.  

æœ‰è®¸å¤šä»»åŠ¡ï¼Œä¹Ÿæœ‰è®¸å¤šå¯èƒ½çš„ç»„ç»‡æ–¹å¼ã€‚  

In literature, I commonly see vision-language tasks divided into two groups: **generation** and **vision-language understanding** (VLU), which is the umbrella term for all tasks that donâ€™t require generation.  

åœ¨æ–‡çŒ®ä¸­ï¼Œæˆ‘é€šå¸¸çœ‹åˆ°çš„è§†è§‰è¯­è¨€ä»»åŠ¡åˆ†ä¸ºä¸¤ç±»ï¼šç”Ÿæˆå’Œè§†è§‰è¯­è¨€ç†è§£ï¼ˆVLUï¼‰ï¼Œåè€…æ˜¯æ‰€æœ‰ä¸éœ€è¦ç”Ÿæˆçš„ä»»åŠ¡çš„æ€»ç§°ã€‚  

The line between these two groups is blurred, as being able to generate answers requires understanding too.  

è¿™ä¸¤ç±»äººä¹‹é—´çš„ç•Œé™æ˜¯æ¨¡ç³Šçš„ï¼Œå› ä¸ºèƒ½å¤Ÿäº§ç”Ÿç­”æ¡ˆä¹Ÿéœ€è¦ç†è§£ã€‚

### Generation  

ä¸€ä»£äºº

For generative tasks, the output can be unimodal (e.g. text, image, 3D rendering) or multimodal.  

å¯¹äºç”Ÿæˆä»»åŠ¡æ¥è¯´ï¼Œè¾“å‡ºå¯ä»¥æ˜¯å•æ¨¡æ€çš„ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€ä¸‰ç»´æ¸²æŸ“ï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯å¤šæ¨¡æ€çš„ã€‚  

While unimodal outputs are common today, multimodal outputs are still shaping up.  

è™½ç„¶å•æ¨¡å¼è¾“å‡ºåœ¨ä»Šå¤©å¾ˆå¸¸è§ï¼Œä½†å¤šæ¨¡å¼è¾“å‡ºä»åœ¨å½¢æˆä¹‹ä¸­ã€‚  

Weâ€™ll discuss multimodal outputs at the end of this post.  

æˆ‘ä»¬å°†åœ¨æœ¬å¸–æœ€åè®¨è®ºå¤šæ¨¡å¼è¾“å‡ºã€‚

#### Image generation (text-to-image synthesis)  

å›¾åƒç”Ÿæˆï¼ˆæ–‡æœ¬åˆ°å›¾åƒçš„åˆæˆï¼‰

This task category is straightforward. Examples: Dall-E, Stable Diffusion, and Midjourney.  

è¯¥ä»»åŠ¡ç±»åˆ«ç®€å•æ˜äº†ã€‚ä¾‹å¦‚Dall-Eã€Stable Diffusion å’Œ Midjourneyã€‚

#### Text generation  

æ–‡æœ¬ç”Ÿæˆ

A common text generation task is visual question answering.  

å¸¸è§çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡æ˜¯è§†è§‰é—®é¢˜è§£ç­”ã€‚  

Instead of relying only on text for the context, you can give the model both text and images.  

æ‚¨å¯ä»¥åŒæ—¶ä¸ºæ¨¡å‹æä¾›æ–‡å­—å’Œå›¾åƒï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾é æ–‡å­—æ¥æä¾›ä¸Šä¸‹æ–‡ã€‚  

Imagine you can point your camera to anything and ask questions like: â€œMy car wonâ€™t start.  

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ å¯ä»¥æŠŠç›¸æœºå¯¹å‡†ä»»ä½•ä¸œè¥¿ï¼Œç„¶åæé—®ï¼Œæ¯”å¦‚ï¼š"æˆ‘çš„è½¦å‘åŠ¨ä¸äº†ã€‚  

Whatâ€™s wrong with it?â€, â€œHow to make this dish?â€, or â€œWhat is this meme about?â€.  

æœ‰ä»€ä¹ˆé—®é¢˜å—ï¼Ÿ"ã€"è¿™é“èœæ€ä¹ˆåšï¼Ÿ"æˆ– "è¿™ä¸ªå¤‡å¿˜å½•æ˜¯å…³äºä»€ä¹ˆçš„ï¼Ÿ"ã€‚

Another common use case is image captioning, which can be used as part of a text-based image retrieval system.  

å¦ä¸€ä¸ªå¸¸è§ç”¨ä¾‹æ˜¯å›¾åƒæ ‡é¢˜ï¼Œå®ƒå¯ç”¨ä½œåŸºäºæ–‡æœ¬çš„å›¾åƒæ£€ç´¢ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†ã€‚  

An organization might have millions, if not billions, of images: product images, graphs, designs, team pictures, promotional materials, etc.  

ä¸€ä¸ªç»„ç»‡å¯èƒ½æœ‰æ•°ç™¾ä¸‡ç”šè‡³æ•°åäº¿å¼ å›¾ç‰‡ï¼šäº§å“å›¾ç‰‡ã€å›¾è¡¨ã€è®¾è®¡ã€å›¢é˜Ÿå›¾ç‰‡ã€å®£ä¼ ææ–™ç­‰ã€‚  

AI can automatically generate captions and metadata for them, making it easier to find the exact images you want.  

äººå·¥æ™ºèƒ½å¯ä»¥ä¸ºå®ƒä»¬è‡ªåŠ¨ç”Ÿæˆæ ‡é¢˜å’Œå…ƒæ•°æ®ï¼Œä»è€Œæ›´å®¹æ˜“å‡†ç¡®æ‰¾åˆ°æ‚¨æƒ³è¦çš„å›¾ç‰‡ã€‚

### Vision-language understanding  

è§†è§‰è¯­è¨€ç†è§£

Weâ€™ll zoom into two task types: classification and text-based image retrieval (TBIR).  

æˆ‘ä»¬å°†æ·±å…¥ç ”ç©¶ä¸¤ç§ä»»åŠ¡ç±»å‹ï¼šåˆ†ç±»å’ŒåŸºäºæ–‡æœ¬çš„å›¾åƒæ£€ç´¢ (TBIR)ã€‚

#### Classification  

åˆ†ç±»

Classification models can only generate outputs that belong to a pre-determined list of classes.  

åˆ†ç±»æ¨¡å‹åªèƒ½ç”Ÿæˆå±äºé¢„å®šç±»åˆ«åˆ—è¡¨çš„è¾“å‡ºç»“æœã€‚  

This works when you only care about a fixed number of outcomes.  

å½“ä½ åªå…³å¿ƒå›ºå®šæ•°é‡çš„ç»“æœæ—¶ï¼Œè¿™ç§æ–¹æ³•å°±ä¼šå¥æ•ˆã€‚  

For example, an OCR system only needs to predict if a visual is one of the known characters (e.g.  

ä¾‹å¦‚ï¼ŒOCR ç³»ç»Ÿåªéœ€é¢„æµ‹è§†è§‰æ•ˆæœæ˜¯å¦ä¸ºå·²çŸ¥å­—ç¬¦ä¹‹ä¸€ï¼ˆå¦‚ï¼š"......"ï¼‰ã€‚  

a digit or a letter).  

æ•°å­—æˆ–å­—æ¯ï¼‰ã€‚

**Side note**: An OCR system processes data at the character level.  

é¢˜å¤–è¯ï¼šOCR ç³»ç»Ÿåœ¨å­—ç¬¦çº§åˆ«å¤„ç†æ•°æ®ã€‚  

When used together with a system that can understand the broader context, it can improve use cases such as allowing you to â€œtalkâ€ to any textbook, contract, assembly instructions, etc.  

ä¸èƒ½å¤Ÿç†è§£æ›´å¹¿æ³›èƒŒæ™¯çš„ç³»ç»Ÿä¸€èµ·ä½¿ç”¨æ—¶ï¼Œå®ƒå¯ä»¥æ”¹è¿›ä½¿ç”¨æƒ…å†µï¼Œä¾‹å¦‚è®©æ‚¨ä¸ä»»ä½•æ•™ç§‘ä¹¦ã€åˆåŒã€ç»„è£…è¯´æ˜ç­‰ "å¯¹è¯"ã€‚

![Document processing with GPT-4V](2-gpt-4v-ocr.png)

Document processing with GPT-4V. The model's mistake is highlighted in red.  

ä½¿ç”¨ GPT-4V å¤„ç†æ–‡ä»¶ã€‚æ¨¡å‹çš„é”™è¯¯ç”¨çº¢è‰²æ ‡å‡ºã€‚

One related task to classification is **image-to-text retrieval**: given an image and a pool of pre-defined texts, find the text thatâ€™s most likely to accompany the image.  

ä¸åˆ†ç±»ç›¸å…³çš„ä¸€é¡¹ä»»åŠ¡æ˜¯å›¾åƒåˆ°æ–‡æœ¬æ£€ç´¢ï¼šç»™å®šä¸€å¹…å›¾åƒå’Œä¸€ç»„é¢„å…ˆå®šä¹‰çš„æ–‡æœ¬ï¼Œæ‰¾å‡ºæœ€æœ‰å¯èƒ½ä¸å›¾åƒæ­é…çš„æ–‡æœ¬ã€‚  

This can be helpful for product image search, i.e. retrieving product reviews from a picture.  

è¿™æœ‰åŠ©äºäº§å“å›¾ç‰‡æœç´¢ï¼Œå³ä»å›¾ç‰‡ä¸­æ£€ç´¢äº§å“è¯„è®ºã€‚

#### Text-based image retrieval (image search)  

åŸºäºæ–‡æœ¬çš„å›¾åƒæ£€ç´¢ï¼ˆå›¾åƒæœç´¢ï¼‰

Image search matters not only for search engines but also for enterprises to be able to search through all their internal images and documents.  

å›¾åƒæœç´¢ä¸ä»…å¯¹æœç´¢å¼•æ“å¾ˆé‡è¦ï¼Œè€Œä¸”å¯¹ä¼ä¸šæ¥è¯´ï¼Œèƒ½å¤Ÿæœç´¢æ‰€æœ‰å†…éƒ¨å›¾åƒå’Œæ–‡ä»¶ä¹Ÿå¾ˆé‡è¦ã€‚  

Some people call text-based image retrieval â€œtext-to-image retrievalâ€.  

æœ‰äººå°†åŸºäºæ–‡æœ¬çš„å›¾åƒæ£€ç´¢ç§°ä¸º "æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢"ã€‚

There are several approaches to text-based image retrieval. Two of them are:  

åŸºäºæ–‡æœ¬çš„å›¾åƒæ£€ç´¢æœ‰å¤šç§æ–¹æ³•ã€‚å…¶ä¸­ä¸¤ç§æ˜¯

1.  Generate captions and metadata for each image, either manually or automatically (see image captioning in **Text generation**). Given a text query, find images whose captions/metadata are closest to this text query.  
    
    æ‰‹åŠ¨æˆ–è‡ªåŠ¨ä¸ºæ¯å¼ å›¾ç‰‡ç”Ÿæˆæ ‡é¢˜å’Œå…ƒæ•°æ®ï¼ˆè¯·å‚é˜…æ–‡æœ¬ç”Ÿæˆä¸­çš„å›¾ç‰‡æ ‡é¢˜ï¼‰ã€‚ç»™å®šæ–‡æœ¬æŸ¥è¯¢ï¼ŒæŸ¥æ‰¾å…¶æ ‡é¢˜/å…ƒæ•°æ®æœ€æ¥è¿‘è¯¥æ–‡æœ¬æŸ¥è¯¢çš„å›¾ç‰‡ã€‚
2.  Train a joint embedding space for both images and text.  
    
    è®­ç»ƒå›¾åƒå’Œæ–‡æœ¬çš„è”åˆåµŒå…¥ç©ºé—´ã€‚  
    
    Given a text query, generate an embedding for this query, and find all images whose embeddings are closest to this embedding.  
    
    ç»™å®šä¸€ä¸ªæ–‡æœ¬æŸ¥è¯¢ï¼Œä¸ºè¯¥æŸ¥è¯¢ç”Ÿæˆä¸€ä¸ªåµŒå…¥å¼ï¼Œå¹¶æ‰¾å‡ºåµŒå…¥å¼ä¸è¯¥åµŒå…¥å¼æœ€æ¥è¿‘çš„æ‰€æœ‰å›¾åƒã€‚

The second approach is more flexible, and I believe will be more widely used.  

ç¬¬äºŒç§æ–¹æ³•æ›´åŠ çµæ´»ï¼Œæˆ‘ç›¸ä¿¡ä¼šå¾—åˆ°æ›´å¹¿æ³›çš„åº”ç”¨ã€‚  

This approach requires having a strong joint embedding space for both vision and language, like the one that OpenAIâ€™s [CLIP](https://arxiv.org/abs/2103.00020) developed.  

è¿™ç§æ–¹æ³•è¦æ±‚è§†è§‰å’Œè¯­è¨€éƒ½æœ‰ä¸€ä¸ªå¼ºå¤§çš„è”åˆåµŒå…¥ç©ºé—´ï¼Œå°±åƒ OpenAI çš„ CLIP æ‰€å¼€å‘çš„é‚£æ ·ã€‚

## Part 2. Fundamentals of Multimodal Training  

ç¬¬ 2 éƒ¨åˆ†.å¤šæ¨¡å¼åŸ¹è®­åŸºç¡€

Given the existence of so many amazing multimodal systems, a challenge of writing this post is choosing which systems to focus on.  

é‰´äºå­˜åœ¨å¦‚æ­¤å¤šä»¤äººæƒŠå¹çš„å¤šæ¨¡æ€ç³»ç»Ÿï¼Œæ’°å†™è¿™ç¯‡æ–‡ç« çš„ä¸€ä¸ªæŒ‘æˆ˜å°±æ˜¯é€‰æ‹©å“ªäº›ç³»ç»Ÿä½œä¸ºé‡ç‚¹ã€‚  

In the end, I decided to focus on two models: [CLIP](https://arxiv.org/abs/2103.00020) (2021) and [Flamingo](https://arxiv.org/abs/2204.14198) (2022) both for their significance as well as availability and clarity of public details.  

æœ€åï¼Œæˆ‘å†³å®šæŠŠé‡ç‚¹æ”¾åœ¨ä¸¤ä¸ªå‹å·ä¸Šï¼šCLIPï¼ˆ2021 å¹´ï¼‰å’Œ Flamingoï¼ˆ2022 å¹´ï¼‰çš„é‡è¦æ€§ä»¥åŠå…¬å¼€ç»†èŠ‚çš„å¯ç”¨æ€§å’Œæ¸…æ™°åº¦ã€‚

-   CLIP was the first model that could generalize to multiple **image classification tasks** with zero- and few-shot learning.  
    
    CLIP æ˜¯ç¬¬ä¸€ä¸ªå¯ä»¥é€šè¿‡é›¶é•œå¤´å’Œå°‘é•œå¤´å­¦ä¹ å¯¹å¤šç§å›¾åƒåˆ†ç±»ä»»åŠ¡è¿›è¡Œæ³›åŒ–çš„æ¨¡å‹ã€‚
-   Flamingo wasnâ€™t the first large multimodal model that could **generate open-ended responses** ([Salesforceâ€™s BLIP](https://arxiv.org/abs/2201.12086) came out 3 months prior). However, Flamingoâ€™s strong performance prompted some to consider it [the GPT-3 moment in the multimodal domain](https://arxiv.org/abs/2304.08485).  
    
    Flamingo å¹¶ä¸æ˜¯ç¬¬ä¸€ä¸ªå¯ä»¥ç”Ÿæˆå¼€æ”¾å¼å›ç­”çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆSalesforce çš„ BLIP åœ¨ 3 ä¸ªæœˆå‰é—®ä¸–ï¼‰ã€‚ç„¶è€Œï¼ŒFlamingo çš„å¼ºåŠ²è¡¨ç°ä¿ƒä½¿ä¸€äº›äººå°†å…¶è§†ä¸ºå¤šæ¨¡æ€é¢†åŸŸçš„ GPT-3 æ—¶åˆ»ã€‚

Even though these two models are older, many techniques they use are still relevant today.  

å°½ç®¡è¿™ä¸¤ç§æ¨¡å¼çš„å†å²è¾ƒé•¿ï¼Œä½†å®ƒä»¬ä½¿ç”¨çš„è®¸å¤šæŠ€æœ¯åœ¨ä»Šå¤©ä»ç„¶é€‚ç”¨ã€‚  

I hope they serve as the foundation to understanding newer models.  

æˆ‘å¸Œæœ›å®ƒä»¬èƒ½ä½œä¸ºç†è§£æ›´æ–°æ¨¡å‹çš„åŸºç¡€ã€‚  

The multimodal space is evolving repaidly, with many new ideas being developed.  

å¤šå¼è”è¿é¢†åŸŸçš„å‘å±•æ—¥æ–°æœˆå¼‚ï¼Œè®¸å¤šæ–°ç†å¿µä¸æ–­æ¶Œç°ã€‚  

Weâ€™ll go over these newer models in [Part 3](https://huyenchip.com/2023/10/10/multimodal.html#part_3_research_directions_for_lmms).  

æˆ‘ä»¬å°†åœ¨ç¬¬ 3 éƒ¨åˆ†ä»‹ç»è¿™äº›æ–°å‹å·ã€‚

At a high level, a multimodal system consists of the following components:  

åœ¨é«˜å±‚æ¬¡ä¸Šï¼Œå¤šæ¨¡å¼ç³»ç»Ÿç”±ä»¥ä¸‹éƒ¨åˆ†ç»„æˆï¼š

1.  An **encoder** for each data modality to generate the embeddings for data of that modality.  
    
    æ¯ä¸ªæ•°æ®æ¨¡æ€éƒ½æœ‰ä¸€ä¸ªç¼–ç å™¨ï¼Œç”¨äºç”Ÿæˆè¯¥æ¨¡æ€æ•°æ®çš„åµŒå…¥ã€‚
2.  A way to **align embeddings** of different modalities into the same **multimodal embedding space**.  
    
    ä¸€ç§å°†ä¸åŒæ¨¡æ€çš„åµŒå…¥æ•°æ®æ•´åˆåˆ°åŒä¸€å¤šæ¨¡æ€åµŒå…¥ç©ºé—´çš„æ–¹æ³•ã€‚
3.  \[Generative models only\] A **language model to generate text responses**.  
    
    \[ä»…é™ç”Ÿæˆæ¨¡å‹\] ç”Ÿæˆæ–‡æœ¬å›å¤çš„è¯­è¨€æ¨¡å‹ã€‚  
    
    Since inputs can contain both text and visuals, new techniques need to be developed to allow the language model to condition its responses on not just text, but also visuals.  
    
    ç”±äºè¾“å…¥å¯èƒ½åŒæ—¶åŒ…å«æ–‡æœ¬å’Œè§†è§‰æ•ˆæœï¼Œå› æ­¤éœ€è¦å¼€å‘æ–°çš„æŠ€æœ¯ï¼Œä½¿è¯­è¨€æ¨¡å‹ä¸ä»…èƒ½æ ¹æ®æ–‡æœ¬ï¼Œè¿˜èƒ½æ ¹æ®è§†è§‰æ•ˆæœåšå‡ºååº”ã€‚

Ideally, as many of these components should be pretrained and reusable as possible.  

ç†æƒ³æƒ…å†µä¸‹ï¼Œè¿™äº›ç»„ä»¶ä¸­åº”å°½å¯èƒ½å¤šåœ°è¿›è¡Œé¢„åŸ¹è®­å’Œé‡å¤ä½¿ç”¨ã€‚

## CLIP: Contrastive Language-Image Pre-training  

CLIPï¼šå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„åŸ¹è®­

CLIPâ€™s key contribution is its ability to map data of different modalities, text and images, into a shared embedding space.  

CLIP çš„ä¸»è¦è´¡çŒ®åœ¨äºå®ƒèƒ½å¤Ÿå°†æ–‡æœ¬å’Œå›¾åƒç­‰ä¸åŒæ¨¡å¼çš„æ•°æ®æ˜ å°„åˆ°ä¸€ä¸ªå…±äº«çš„åµŒå…¥ç©ºé—´ã€‚  

This shared multimodal embedding space makes text-to-image and image-to-text tasks so much easier.  

è¿™ç§å…±äº«çš„å¤šæ¨¡æ€åµŒå…¥ç©ºé—´ä½¿æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒåˆ°æ–‡æœ¬çš„ä»»åŠ¡å˜å¾—æ›´åŠ å®¹æ˜“ã€‚

Training this multimodal embedding space also produced a strong image encoder, which allows CLIP to achieve **competitive zero-shot performance on many image classification tasks**.  

å¯¹è¿™ä¸€å¤šæ¨¡æ€åµŒå…¥ç©ºé—´çš„è®­ç»ƒè¿˜äº§ç”Ÿäº†ä¸€ä¸ªå¼ºå¤§çš„å›¾åƒç¼–ç å™¨ï¼Œè¿™ä½¿å¾— CLIP åœ¨è®¸å¤šå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­éƒ½èƒ½å®ç°å…·æœ‰ç«äº‰åŠ›çš„é›¶é•œå¤´æ€§èƒ½ã€‚  

This strong image encoder can be used for many other tasks: image generation, visual question answering, and text-based image retrieval.  

è¿™ç§å¼ºå¤§çš„å›¾åƒç¼–ç å™¨è¿˜å¯ç”¨äºè®¸å¤šå…¶ä»–ä»»åŠ¡ï¼šå›¾åƒç”Ÿæˆã€è§†è§‰é—®é¢˜è§£ç­”å’ŒåŸºäºæ–‡æœ¬çš„å›¾åƒæ£€ç´¢ã€‚  

Flamingo and LLaVa use CLIP as their image encoder. DALL-E uses CLIP to rerank generated images.  

Flamingo å’Œ LLaVa ä½¿ç”¨ CLIP ä½œä¸ºå›¾åƒç¼–ç å™¨ã€‚DALL-E ä½¿ç”¨ CLIP å¯¹ç”Ÿæˆçš„å›¾åƒè¿›è¡Œé‡æ’ã€‚  

Itâ€™s unclear if GPT-4V uses CLIP.  

ç›®å‰è¿˜ä¸æ¸…æ¥š GPT-4V æ˜¯å¦ä½¿ç”¨ CLIPã€‚

![Zero-shot image classification with CLIP](3-CLIP-image-classification.png)

Zero-shot image classification with CLIP  

åˆ©ç”¨ CLIP è¿›è¡Œé›¶é•œå¤´å›¾åƒåˆ†ç±»

CLIP leveraged **natural language supervision** and **contrastive learning**, which allowed CLIP to both scale up their data and make training more efficient.  

CLIP åˆ©ç”¨è‡ªç„¶è¯­è¨€ç›‘ç£å’Œå¯¹æ¯”å­¦ä¹ ï¼Œæ—¢æ‰©å¤§äº†æ•°æ®è§„æ¨¡ï¼Œåˆæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚  

Weâ€™ll go over why/how these two techniques work.  

æˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»è¿™ä¸¤ç§æŠ€æœ¯çš„åŸå› /å·¥ä½œåŸç†ã€‚

### CLIP's high-level architecture  

CLIP çš„é«˜çº§æ¶æ„

![Architecture of OpenAI's CLIP](4-CLIP-architecture.png)

CLIP's architecture.  

CLIP çš„ç»“æ„ã€‚  

Both encoders and projection matrices are jointly trained together from scratch.  

ç¼–ç å™¨å’ŒæŠ•å½±çŸ©é˜µéƒ½æ˜¯ä»é›¶å¼€å§‹è”åˆè®­ç»ƒçš„ã€‚  

The training goal is to maximize the similarity scores of the right (image, text) pairings while minimizing the similarity scores of the wrong pairings (contrastive learning).  

è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–æ­£ç¡®é…å¯¹ï¼ˆå›¾åƒã€æ–‡æœ¬ï¼‰çš„ç›¸ä¼¼åº¦å¾—åˆ†ï¼ŒåŒæ—¶æœ€å°åŒ–é”™è¯¯é…å¯¹çš„ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰ã€‚

For the **image encoder**, the authors experimented with both ResNet and ViT. Their best-performing model is `ViT-L/14@336px`:  

å¯¹äºå›¾åƒç¼–ç å™¨ï¼Œä½œè€…ä½¿ç”¨ ResNet å’Œ ViT è¿›è¡Œäº†å®éªŒã€‚è¡¨ç°æœ€å¥½çš„æ¨¡å‹æ˜¯ `ViT-L/14@336px` ï¼š

-   Large vision transformer (ViT-L)  
    
    å¤§å‹è§†è§‰å˜å‹å™¨ (ViT-L)
-   14 patches (each image is divided into 14 sub-images)  
    
    14 ä¸ªè¡¥ä¸ï¼ˆæ¯å¹…å›¾åƒåˆ†ä¸º 14 ä¸ªå­å›¾åƒï¼‰
-   on 336x336 pixel input  
    
    è¾“å…¥ 336x336 åƒç´ æ—¶

For the **text encoder**, CLIP uses a Transformer model similar to [GPT-2](https://openai.com/research/better-language-models) but smaller. Their base model has only 63M parameters with 8 attention heads.  

åœ¨æ–‡æœ¬ç¼–ç å™¨æ–¹é¢ï¼ŒCLIP ä½¿ç”¨ä¸ GPT-2 ç±»ä¼¼çš„ Transformer æ¨¡å‹ï¼Œä½†ä½“ç§¯æ›´å°ã€‚ä»–ä»¬çš„åŸºæœ¬æ¨¡å‹åªæœ‰ 6300 ä¸‡ä¸ªå‚æ•°å’Œ 8 ä¸ªæ³¨æ„å¤´ã€‚  

The authors found CLIPâ€™s performance to be less sensitive to the capacity of the text encoder.  

ä½œè€…å‘ç° CLIP çš„æ€§èƒ½å¯¹æ–‡æœ¬ç¼–ç å™¨çš„å®¹é‡ä¸å¤ªæ•æ„Ÿã€‚

Embeddings generated by the image encoder and text encoder are projected into the same embedding space using two projection matriceså›¾åƒç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆçš„åµŒå…¥ä¿¡æ¯é€šè¿‡ä¸¤ä¸ªæŠ•å½±çŸ©é˜µ \\(W\_v\\) å’Œ \\(W\_l\\) æŠ•å°„åˆ°åŒä¸€ä¸ªåµŒå…¥ç©ºé—´ã€‚ and .

-   Given an image embeddingç»™å®šä¸€ä¸ªå›¾åƒåµŒå…¥å€¼ \\(V\_i\\)ï¼Œç›¸åº”çš„å¤šæ¨¡æ€åµŒå…¥å€¼çš„è®¡ç®—å…¬å¼ä¸º\\W\_vV\_i\\). , the corresponding multimodal embedding is computed as: .
-   Given a text embeddingç»™å®šä¸€ä¸ªæ–‡æœ¬åµŒå…¥ï¼ˆ\\(L\_i\\)ï¼‰ï¼Œç›¸åº”çš„å¤šæ¨¡æ€åµŒå…¥è®¡ç®—å¦‚ä¸‹ï¼š\\W\_lL\_i\\). , the corresponding multimodal embedding is computed as: .

When people say CLIP embeddings, they either refer to these multimodal embeddings or the embeddings generated by CLIPâ€™s image encoder.  

å½“äººä»¬æåˆ° CLIP åµŒå…¥æ—¶ï¼Œä»–ä»¬è¦ä¹ˆæŒ‡çš„æ˜¯è¿™äº›å¤šæ¨¡æ€åµŒå…¥ï¼Œè¦ä¹ˆæŒ‡çš„æ˜¯ CLIP å›¾åƒç¼–ç å™¨ç”Ÿæˆçš„åµŒå…¥ã€‚

### Natural language supervision  

è‡ªç„¶è¯­è¨€ç›‘ç£

For many years, image models were trained with manually annotated (image, text) datasets (e.g.  

å¤šå¹´æ¥ï¼Œå›¾åƒæ¨¡å‹éƒ½æ˜¯é€šè¿‡äººå·¥æ ‡æ³¨çš„ï¼ˆå›¾åƒã€æ–‡æœ¬ï¼‰æ•°æ®é›†ï¼ˆä¾‹å¦‚ï¼Œ"å›¾åƒæ¨¡å‹"ï¼‰è¿›è¡Œè®­ç»ƒçš„ã€‚  

ImageNet, MS COCO). This isnâ€™t scalable. Manual annotation is time-consuming and expensive.  

ImageNetã€MS COCOï¼‰ã€‚è¿™æ— æ³•æ‰©å±•ã€‚äººå·¥æ ‡æ³¨è´¹æ—¶è´¹åŠ›ã€‚

The CLIP paper noted that none of the then-available (image, text) datasets was big and high quality enough.  

CLIP è®ºæ–‡æŒ‡å‡ºï¼Œå½“æ—¶å¯ç”¨çš„ï¼ˆå›¾åƒã€æ–‡æœ¬ï¼‰æ•°æ®é›†éƒ½ä¸å¤Ÿå¤§ï¼Œè´¨é‡ä¹Ÿä¸å¤Ÿé«˜ã€‚  

They created their own dataset â€“ 400M (image, text) pairs â€“ as follows.  

ä»–ä»¬åˆ›å»ºäº†è‡ªå·±çš„æ•°æ®é›†--4 äº¿ï¼ˆå›¾åƒã€æ–‡æœ¬ï¼‰å¯¹ï¼Œå…·ä½“å¦‚ä¸‹ã€‚

1.  Construct a list of 500,000 queries.  
    
    æ„å»ºä¸€ä¸ªåŒ…å« 500,000 æ¡æŸ¥è¯¢çš„åˆ—è¡¨ã€‚  
    
    Queries are common words, bigrams, and titles of popular Wikipedia articles.  
    
    æŸ¥è¯¢çš„å†…å®¹æ˜¯ç»´åŸºç™¾ç§‘çƒ­é—¨æ–‡ç« çš„å¸¸ç”¨è¯ã€åŒéŸ³èŠ‚è¯å’Œæ ‡é¢˜ã€‚
2.  Find images matching these queries (string and substring match).  
    
    æŸ¥æ‰¾ä¸è¿™äº›æŸ¥è¯¢åŒ¹é…çš„å›¾åƒï¼ˆå­—ç¬¦ä¸²å’Œå­ä¸²åŒ¹é…ï¼‰ã€‚  
    
    The paper mentioned this search did NOT happen on search engines but didnâ€™t specify where.  
    
    è®ºæ–‡æåˆ°è¿™ç§æœç´¢å¹¶ä¸æ˜¯åœ¨æœç´¢å¼•æ“ä¸Šè¿›è¡Œçš„ï¼Œä½†æ²¡æœ‰å…·ä½“è¯´æ˜æ˜¯åœ¨å“ªé‡Œè¿›è¡Œçš„ã€‚  
    
    My theory is that since OpenAI already scraped the entire Internet for their GPT models, they probably just queried their internal database.  
    
    æˆ‘çš„ç†è®ºæ˜¯ï¼Œæ—¢ç„¶ OpenAI å·²ç»åœ¨æ•´ä¸ªäº’è”ç½‘ä¸Šæœç´¢äº†ä»–ä»¬çš„ GPT æ¨¡å‹ï¼Œé‚£ä¹ˆä»–ä»¬å¯èƒ½åªæ˜¯æŸ¥è¯¢äº†ä»–ä»¬çš„å†…éƒ¨æ•°æ®åº“ã€‚
3.  Each image is paired with a text that co-occurs with it (e.g.  
    
    æ¯å¹…å›¾åƒéƒ½ä¸ä¸€ä¸ªå…±åŒå‡ºç°çš„æ–‡æœ¬é…å¯¹ï¼ˆä¾‹å¦‚  
    
    captions, comments) instead of the query since queries are too short to be descriptive.  
    
    æ ‡é¢˜ã€æ³¨é‡Šï¼‰ï¼Œè€Œä¸æ˜¯æŸ¥è¯¢ï¼Œå› ä¸ºæŸ¥è¯¢å¤ªçŸ­ï¼Œæ— æ³•è¿›è¡Œæè¿°ã€‚

Because some queries are more popular than others, to avoid data imbalance, they used at most 20K images for a query.  

ç”±äºæœ‰äº›æŸ¥è¯¢æ¯”å…¶ä»–æŸ¥è¯¢æ›´å—æ¬¢è¿ï¼Œä¸ºé¿å…æ•°æ®ä¸å¹³è¡¡ï¼Œä»–ä»¬æœ€å¤šä½¿ç”¨ 20K å¼ å›¾ç‰‡è¿›è¡ŒæŸ¥è¯¢ã€‚

### Contrastive learning  

å¯¹æ¯”å­¦ä¹ 

Pre-CLIP, most vision-language models were trained using a classifier or language model objectives.  

åœ¨ CLIP ä¹‹å‰ï¼Œå¤§å¤šæ•°è§†è§‰è¯­è¨€æ¨¡å‹éƒ½æ˜¯ä½¿ç”¨åˆ†ç±»å™¨æˆ–è¯­è¨€æ¨¡å‹ç›®æ ‡æ¥è®­ç»ƒçš„ã€‚  

Contrastive objective is a clever technique that allows CLIP to scale and generalize to multiple tasks.  

å¯¹æ¯”ç›®æ ‡æ˜¯ä¸€ç§å·§å¦™çš„æŠ€æœ¯ï¼Œå®ƒä½¿ CLIP èƒ½å¤Ÿæ‰©å±•å¹¶é€‚ç”¨äºå¤šç§ä»»åŠ¡ã€‚

Weâ€™ll show why the constrastive objective works better for CLIP using an example task of image captioning: given an image, generate a text that describes it.  

æˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªä¸ºå›¾ç‰‡é…å­—å¹•çš„ç¤ºä¾‹ä»»åŠ¡æ¥è¯´æ˜ä¸ºä»€ä¹ˆæ”¶ç¼©ç›®æ ‡æ›´é€‚åˆ CLIPï¼šç»™å®šä¸€å¼ å›¾ç‰‡ï¼Œç”Ÿæˆä¸€æ®µæè¿°è¯¥å›¾ç‰‡çš„æ–‡å­—ã€‚

#### Classifier objective  

åˆ†ç±»å™¨ç›®æ ‡

A classifier predicts the correct class among a predetermined list of classes.  

åˆ†ç±»å™¨å¯ä»é¢„å®šçš„ç±»åˆ«åˆ—è¡¨ä¸­é¢„æµ‹å‡ºæ­£ç¡®çš„ç±»åˆ«ã€‚  

This works when the output space is finite.  

å½“è¾“å‡ºç©ºé—´æœ‰é™æ—¶ï¼Œè¿™ç§æ–¹æ³•å°±ä¼šå¥æ•ˆã€‚  

Previous models that work with (image, text) pair datasets all had this limitation.  

ä»¥å‰ä½¿ç”¨ï¼ˆå›¾åƒã€æ–‡æœ¬ï¼‰é…å¯¹æ•°æ®é›†çš„æ¨¡å‹éƒ½æœ‰è¿™ç§å±€é™æ€§ã€‚  

For example, models working with [ILSVRC-2012](https://www.image-net.org/challenges/LSVRC/2012/) limited themselves to 1,000 classes, and [JFT-300M](https://arxiv.org/abs/1707.02968) to 18,291 classes.  

ä¾‹å¦‚ï¼Œä½¿ç”¨ ILSVRC-2012 çš„æ¨¡å‹ä»…é™äº 1,000 ä¸ªç­‰çº§ï¼Œè€Œä½¿ç”¨ JFT-300M çš„æ¨¡å‹ä»…é™äº 18,291 ä¸ªç­‰çº§ã€‚

This objective limits not only the modelâ€™s capacity to output meaningful responses but also its capacity for zero-shot learning.  

è¿™ä¸€ç›®æ ‡ä¸ä»…é™åˆ¶äº†æ¨¡å‹è¾“å‡ºæœ‰æ„ä¹‰å“åº”çš„èƒ½åŠ›ï¼Œä¹Ÿé™åˆ¶äº†å…¶é›¶ç‚¹å­¦ä¹ çš„èƒ½åŠ›ã€‚  

Say, if the model was trained to predict among 10 classes, it wonâ€™t work for a task that has 100 classes.  

æ¯”æ–¹è¯´ï¼Œå¦‚æœæ¨¡å‹æ˜¯ä¸ºé¢„æµ‹ 10 ä¸ªç±»åˆ«è€Œè®­ç»ƒçš„ï¼Œé‚£ä¹ˆå®ƒå°±ä¸èƒ½ç”¨äºæœ‰ 100 ä¸ªç±»åˆ«çš„ä»»åŠ¡ã€‚

#### Language model objective  

è¯­è¨€æ¨¡å¼ç›®æ ‡

If a classifier outputs only one class for each input, a language model outputs a sequence of classes.  

å¦‚æœåˆ†ç±»å™¨å¯¹æ¯ä¸ªè¾“å…¥åªè¾“å‡ºä¸€ä¸ªç±»åˆ«ï¼Œé‚£ä¹ˆè¯­è¨€æ¨¡å‹åˆ™ä¼šè¾“å‡ºä¸€ç³»åˆ—ç±»åˆ«ã€‚  

Each generated class is called a token.  

æ¯ä¸ªç”Ÿæˆçš„ç±»ç§°ä¸ºä¸€ä¸ªæ ‡è®°ã€‚  

Each token is from a predetermined list, the vocabulary, of the language model.  

æ¯ä¸ªæ ‡è®°éƒ½æ¥è‡ªè¯­è¨€æ¨¡å‹çš„é¢„å®šåˆ—è¡¨ï¼Œå³è¯æ±‡è¡¨ã€‚

![Classifier vs. language model objectives](5-classifier-vs-language-model-objectives.png)

Classifier vs. language model objectives  

åˆ†ç±»å™¨ä¸è¯­è¨€æ¨¡å‹çš„ç›®æ ‡

#### Contrastive objective  

å¯¹æ¯”ç›®æ ‡

While the language model objective allows for vastly more flexible outputs, CLIP authors noted this objective made the training difficult.  

è™½ç„¶è¯­è¨€æ¨¡å‹çš„ç›®æ ‡å…è®¸æ›´çµæ´»çš„è¾“å‡ºï¼Œä½† CLIP çš„ä½œè€…æŒ‡å‡ºï¼Œè¿™ä¸€ç›®æ ‡ç»™è®­ç»ƒå¸¦æ¥äº†å›°éš¾ã€‚  

They hypothesized that this is because the model tries to generate _exactly_ the text accompanying each image, while many possible texts can accompany an image: alt-text, caption, comments, etc.  

ä»–ä»¬å‡è®¾ï¼Œè¿™æ˜¯å› ä¸ºè¯¥æ¨¡å‹è¯•å›¾å‡†ç¡®ç”Ÿæˆæ¯å¼ å›¾ç‰‡çš„é™„å¸¦æ–‡æœ¬ï¼Œè€Œä¸€å¼ å›¾ç‰‡å¯èƒ½é™„å¸¦å¤šç§æ–‡æœ¬ï¼šalt-textã€æ ‡é¢˜ã€æ³¨é‡Šç­‰ã€‚

For example, in the [Flickr30K dataset](https://arxiv.org/abs/1509.04942), each image has 5 captions provided by human annotators, and the captions for the same image can be very different.  

ä¾‹å¦‚ï¼Œåœ¨ Flickr30K æ•°æ®é›†ä¸­ï¼Œæ¯å¼ å›¾ç‰‡éƒ½æœ‰ 5 ä¸ªç”±äººç±»æ³¨é‡Šè€…æä¾›çš„æ ‡é¢˜ï¼Œè€ŒåŒä¸€å›¾ç‰‡çš„æ ‡é¢˜å¯èƒ½å¤§ç›¸å¾„åº­ã€‚

![Multiple captions for the same image](6-multiple-captions.png)

Contrastive learning is to overcome this challenge.  

å¯¹æ¯”å­¦ä¹ å°±æ˜¯è¦å…‹æœè¿™ä¸€æŒ‘æˆ˜ã€‚  

Instead of predicting the exact text of each image, CLIP was trained to predict whether a text is more likely to accompany an image than other texts.  

CLIP ä¸é¢„æµ‹æ¯å¼ å›¾ç‰‡çš„ç¡®åˆ‡æ–‡å­—ï¼Œè€Œæ˜¯é€šè¿‡è®­ç»ƒæ¥é¢„æµ‹æŸæ®µæ–‡å­—æ˜¯å¦æ¯”å…¶ä»–æ–‡å­—æ›´æœ‰å¯èƒ½ä¸å›¾ç‰‡ä¸€èµ·å‡ºç°ã€‚

For each batch ofå¯¹äºæ¯ä¸€æ‰¹ ï¼ˆå›¾åƒã€æ–‡æœ¬ï¼‰å¯¹ï¼Œæ¨¡å‹ä¼šç”Ÿæˆ N ä¸ªæ–‡æœ¬åµŒå…¥å’Œ N ä¸ªå›¾åƒåµŒå…¥ã€‚ (image, text) pairs, the model generates N text embeddings and N image embeddings.

-   Letè®© \\(V\_1, V\_2, ..., V\_n\\) æˆä¸º \\(N\\) å›¾åƒçš„åµŒå…¥ã€‚ be the embeddings for the images.
-   Letè®© \\(L\_1, L\_2, ..., L\_n\\) æˆä¸º \\(N\\) æ–‡æœ¬çš„åµŒå…¥ã€‚ be the embeddings for the texts.

CLIP computes the cosine similarity scores of theCLIP è®¡ç®—å¯èƒ½çš„ (\\(V\_i, L\_j\\))é…å¯¹çš„ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†ã€‚ possible () pairings.  

The model is trained to maximize the similarity scores of theè®­ç»ƒæ¨¡å‹çš„ç›®çš„æ˜¯æœ€å¤§åŒ–ï¼ˆNï¼‰æ­£ç¡®é…å¯¹çš„ç›¸ä¼¼åº¦å¾—åˆ†ï¼ŒåŒæ—¶æœ€å°åŒ–ï¼ˆN^2 - Nï¼‰é”™è¯¯é…å¯¹çš„å¾—åˆ†ã€‚ correct pairings while minimizing the scores of the incorrect pairings.  

For CLIP,  

å¯¹äº CLIPï¼Œï¼ˆN = 32 768ï¼‰ã€‚.

![How CLIP works](7-clip.png)

Another way to look at this is that each training batch of CLIP is two classification tasks.  

å¦ä¸€ç§çœ‹æ³•æ˜¯ï¼ŒCLIP çš„æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡éƒ½æ˜¯ä¸¤ä¸ªåˆ†ç±»ä»»åŠ¡ã€‚

1.  Each image can be paired with N possible texts, and the model tries to predict the correct one.  
    
    æ¯å¼ å›¾ç‰‡å¯ä»¥ä¸ N ä¸ªå¯èƒ½çš„æ–‡æœ¬é…å¯¹ï¼Œæ¨¡å‹ä¼šå°è¯•é¢„æµ‹æ­£ç¡®çš„æ–‡æœ¬ã€‚  
    
    This is the same setup as image-to-text retrieval.  
    
    è¿™ä¸å›¾åƒåˆ°æ–‡æœ¬æ£€ç´¢çš„è®¾ç½®ç›¸åŒã€‚
    
2.  Each text can be paired with N possible images, and the model tries to predict the correct image.  
    
    æ¯ä¸ªæ–‡æœ¬å¯ä¸ N å¹…å¯èƒ½çš„å›¾åƒé…å¯¹ï¼Œæ¨¡å‹ä¼šå°è¯•é¢„æµ‹æ­£ç¡®çš„å›¾åƒã€‚  
    
    This is the same setup as text-to-image retrieval.  
    
    è¿™ä¸æ–‡æœ¬åˆ°å›¾åƒçš„æ£€ç´¢è®¾ç½®ç›¸åŒã€‚
    

The sum of these two losses is minimized. ğ›½ is a trainable inverse temperature parameter.  

è¿™ä¸¤é¡¹æŸå¤±ä¹‹å’Œæœ€å°ã€‚åŒ¡æ˜¯ä¸€ä¸ªå¯è®­ç»ƒçš„åæ¸©åº¦å‚æ•°ã€‚

This is what it all looks like in pseudocode.  

è¿™å°±æ˜¯ä¼ªä»£ç çš„å…¨éƒ¨å†…å®¹ã€‚

![CLIP pseudocode](8-clip-pseudocode.png)

CLIP authors found that the contrastive objective provided a 12x improvement in efficiency compared to the language model objective baseline while producing higher-quality image embeddings.  

CLIP çš„ä½œè€…å‘ç°ï¼Œä¸è¯­è¨€æ¨¡å‹ç›®æ ‡åŸºçº¿ç›¸æ¯”ï¼Œå¯¹æ¯”ç›®æ ‡çš„æ•ˆç‡æé«˜äº† 12 å€ï¼ŒåŒæ—¶ç”Ÿæˆçš„å›¾åƒåµŒå…¥è´¨é‡æ›´é«˜ã€‚

![CLIP constrastive learning](9-contrastive-learning-efficiency.png)

### CLIP applications  

CLIP åº”ç”¨

#### Classification  

åˆ†ç±»

Today, for many image classification tasks, CLIP is still a strong out-of-the-box baseline to be used as-is or fine-tuned.  

å¦‚ä»Šï¼Œå¯¹äºè®¸å¤šå›¾åƒåˆ†ç±»ä»»åŠ¡æ¥è¯´ï¼ŒCLIP ä»ç„¶æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€ç®±å³ç”¨çš„åŸºå‡†å·¥å…·ï¼Œæ—¢å¯ä»¥æŒ‰åŸæ ·ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥è¿›è¡Œå¾®è°ƒã€‚

![CLIP as a strong baseline for image classification](10-clip-perf.png)

#### Text-based image retrieval  

åŸºäºæ–‡æœ¬çš„å›¾åƒæ£€ç´¢

Since CLIPâ€™s training process was conceptually similar to image-to-text retrieval and text-to-image retrieval, CLIP â€œ_displays significant promise for widely-applicable tasks like image retrieval or search_.â€ However, â€œ_on image retrieval, CLIPâ€™s performance relative to the overall state of the art is noticeably lower._â€  

ç”±äº CLIP çš„è®­ç»ƒè¿‡ç¨‹åœ¨æ¦‚å¿µä¸Šç±»ä¼¼äºå›¾åƒåˆ°æ–‡æœ¬æ£€ç´¢å’Œæ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢ï¼ŒCLIP "åœ¨å›¾åƒæ£€ç´¢æˆ–æœç´¢ç­‰å¹¿æ³›åº”ç”¨çš„ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„å‰æ™¯"ã€‚ç„¶è€Œï¼Œ"åœ¨å›¾åƒæ£€ç´¢æ–¹é¢ï¼ŒCLIP çš„æ€§èƒ½ç›¸å¯¹äºæ•´ä½“æŠ€æœ¯æ°´å¹³æ˜æ˜¾è¾ƒä½"ã€‚

There are attempts to use CLIP for image retrieval. For example, [clip-retrieval](https://github.com/rom1504/clip-retrieval) package works as follows:  

æœ‰äººå°è¯•ä½¿ç”¨ CLIP è¿›è¡Œå›¾åƒæ£€ç´¢ã€‚ä¾‹å¦‚ï¼Œå‰ªè¾‘æ£€ç´¢è½¯ä»¶åŒ…çš„å·¥ä½œåŸç†å¦‚ä¸‹ï¼š

1.  Generate CLIP embeddings for all your images and store them in a vector database.  
    
    ä¸ºæ‰€æœ‰å›¾åƒç”Ÿæˆ CLIP åµŒå…¥ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨çŸ¢é‡æ•°æ®åº“ä¸­ã€‚
2.  For each text query, generate a CLIP embedding for this text.  
    
    å¯¹äºæ¯ä¸ªæ–‡æœ¬æŸ¥è¯¢ï¼Œä¸ºè¯¥æ–‡æœ¬ç”Ÿæˆ CLIP åµŒå…¥ã€‚
3.  Query in the vector database for all images whose embeddings are close to this text query embedding.  
    
    åœ¨çŸ¢é‡æ•°æ®åº“ä¸­æŸ¥è¯¢åµŒå…¥å¼ä¸è¯¥æ–‡æœ¬æŸ¥è¯¢åµŒå…¥å¼ç›¸è¿‘çš„æ‰€æœ‰å›¾åƒã€‚

#### Image generation  

å›¾åƒç”Ÿæˆ

CLIPâ€™s joint image-text embeddings are useful for image generation. Given a text prompt, [DALL-E](https://openai.com/research/dall-e) (2021) generates many different visuals and uses CLIP to rerank these visuals before showing the top visuals to users.  

CLIP çš„å›¾åƒ-æ–‡æœ¬è”åˆåµŒå…¥å¯¹å›¾åƒç”Ÿæˆéå¸¸æœ‰ç”¨ã€‚DALL-E (2021) ç»™å‡ºæ–‡æœ¬æç¤ºåï¼Œä¼šç”Ÿæˆè®¸å¤šä¸åŒçš„è§†è§‰æ•ˆæœï¼Œå¹¶ä½¿ç”¨ CLIP å¯¹è¿™äº›è§†è§‰æ•ˆæœè¿›è¡Œé‡æ–°æ’åºï¼Œç„¶åå†å‘ç”¨æˆ·å±•ç¤ºæœ€ä½³è§†è§‰æ•ˆæœã€‚

In 2022, OpenAI introduced [unCLIP](https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents), a text-to-image synthesis model conditioned on CLIP latents. It consists of two main components:  

2022 å¹´ï¼ŒOpenAI æ¨å‡ºäº† unCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥ CLIP æ½œåœ¨å˜é‡ä¸ºæ¡ä»¶çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ¨¡å‹ã€‚å®ƒç”±ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼š

1.  CLIP is trained and frozen.  
    
    CLIP å·²æ¥å—åŸ¹è®­å¹¶å†»ç»“ã€‚  
    
    The pretrained CLIP model can generate embeddings for both text and images in the same embedding space.  
    
    ç»è¿‡é¢„è®­ç»ƒçš„ CLIP æ¨¡å‹å¯ä»¥åœ¨åŒä¸€åµŒå…¥ç©ºé—´ä¸­ç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒçš„åµŒå…¥ã€‚
2.  Two things happen at image generation:  
    
    å›¾åƒç”Ÿæˆæ—¶ä¼šå‘ç”Ÿä¸¤ä»¶äº‹ï¼š
    -   Use CLIP to generate embedding for this text.  
        
        ä½¿ç”¨ CLIP ä¸ºè¯¥æ–‡æœ¬ç”ŸæˆåµŒå…¥ã€‚
    -   Use a diffusion decoder to generate images conditioned on this embedding.  
        
        ä½¿ç”¨æ‰©æ•£è§£ç å™¨ç”Ÿæˆä»¥è¿™ç§åµŒå…¥ä¸ºæ¡ä»¶çš„å›¾åƒã€‚

![unCLIP](11-unCLIP.png)

#### Text generation: visual question answering, captioning  

æ–‡æœ¬ç”Ÿæˆï¼šè§†è§‰é—®é¢˜è§£ç­”ã€å­—å¹•

CLIP authors did attempt to create a model for text generation.  

CLIP çš„ä½œè€…ç¡®å®è¯•å›¾åˆ›å»ºä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€‚  

One version they experimented with is called LM RN50. Though this model could generate text responses, its performance was consistently around 10% below CLIPâ€™s best-performing model on all the vision-language understanding tasks that CLIP was evaluated on.  

ä»–ä»¬è¯•éªŒçš„ä¸€ä¸ªç‰ˆæœ¬ç§°ä¸º LM RN50ã€‚è™½ç„¶è¿™ä¸ªæ¨¡å‹å¯ä»¥ç”Ÿæˆæ–‡æœ¬å›å¤ï¼Œä½†åœ¨ CLIP è¯„ä¼°çš„æ‰€æœ‰è§†è§‰è¯­è¨€ç†è§£ä»»åŠ¡ä¸­ï¼Œå®ƒçš„è¡¨ç°å§‹ç»ˆæ¯” CLIP è¡¨ç°æœ€å¥½çš„æ¨¡å‹ä½ 10% å·¦å³ã€‚

While today CLIP isnâ€™t used directly for text generation, its image encoder is often the backbone for LMMs that can generate texts.  

è™½ç„¶ CLIP å¦‚ä»Šå¹¶ä¸ç›´æ¥ç”¨äºæ–‡æœ¬ç”Ÿæˆï¼Œä½†å…¶å›¾åƒç¼–ç å™¨å¾€å¾€æ˜¯å¯ç”Ÿæˆæ–‡æœ¬çš„ LMM çš„æ”¯æŸ±ã€‚

## Flamingo: the dawns of LMMs  

ç«çƒˆé¸Ÿï¼šLMM çš„æ›™å…‰

Unlike CLIP, Flamingo can generate text responses.  

ä¸ CLIP ä¸åŒï¼ŒFlamingo å¯ä»¥ç”Ÿæˆæ–‡æœ¬å›å¤ã€‚  

In a reductive view, Flamingo is CLIP + a language model, with added techniques to make it possible for the language model to generate text tokens conditioned on both visual and text inputs.  

ä»è¿˜åŸçš„è§’åº¦çœ‹ï¼ŒFlamingo å°±æ˜¯ CLIP + è¯­è¨€æ¨¡å‹ï¼Œå¹¶å¢åŠ äº†ä¸€äº›æŠ€æœ¯ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ç”Ÿæˆæ–‡æœ¬æ ‡è®°ã€‚

![Conversations with Flamingo LMMs](12-flamingo-chatbots.png)

Flamingo can generate text responses conditioned on both text and images  

Flamingo å¯æ ¹æ®æ–‡å­—å’Œå›¾åƒç”Ÿæˆæ–‡å­—ååº”

### Flamingo's high-level architecture  

Flamingo çš„é«˜çº§æ¶æ„

At a high level, Flamingo consists of 2 parts:  

åœ¨é«˜å±‚æ¬¡ä¸Šï¼Œç«çƒˆé¸Ÿç”±ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆï¼š

1.  **Vision encoder**: a CLIP-like model is trained using contrastive learning.  
    
    è§†è§‰ç¼–ç å™¨ï¼šåˆ©ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒç±»ä¼¼ CLIP çš„æ¨¡å‹ã€‚  
    
    The text encoder of this model is then discarded.  
    
    ç„¶åï¼Œè¯¥æ¨¡å‹çš„æ–‡æœ¬ç¼–ç å™¨å°†è¢«ä¸¢å¼ƒã€‚  
    
    The vision encoder is frozen to be used in the main model.  
    
    è§†è§‰ç¼–ç å™¨å·²å†»ç»“ï¼Œå°†åœ¨ä¸»æ¨¡å‹ä¸­ä½¿ç”¨ã€‚
2.  **Language model**: Flamingo finetunes Chinchilla to generate text tokens, conditioned on visuals and text, using language model loss, with two additional components Perceiver Resampler and GATED XATTN-DENSE layers.  
    
    è¯­è¨€æ¨¡å‹ï¼šFlamingo å¯¹ Chinchilla è¿›è¡Œäº†å¾®è°ƒï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹æŸè€—ç”Ÿæˆä»¥è§†è§‰å’Œæ–‡æœ¬ä¸ºæ¡ä»¶çš„æ–‡æœ¬æ ‡è®°ï¼Œå¹¶å¢åŠ äº†ä¸¤ä¸ªé™„åŠ ç»„ä»¶ Perceiver Resampler å’Œ GATED XATTN-DENSE å±‚ã€‚  
    
    Weâ€™ll discuss them later in this blog.  
    
    æˆ‘ä»¬ç¨åå°†åœ¨æœ¬åšå®¢ä¸­è®¨è®ºå®ƒä»¬ã€‚

![Flamingo high level architecture](13-flamingo-architecture.png)

### Data  

æ•°æ®

Flamingo used 4 datasets: 2 (image, text) pair datasets, 1 (video, text) pair dataset, and 1 interleaved image and text dataset.  

Flamingo ä½¿ç”¨äº† 4 ä¸ªæ•°æ®é›†ï¼š2 ä¸ªï¼ˆå›¾åƒã€æ–‡æœ¬ï¼‰é…å¯¹æ•°æ®é›†ã€1 ä¸ªï¼ˆè§†é¢‘ã€æ–‡æœ¬ï¼‰é…å¯¹æ•°æ®é›†å’Œ 1 ä¸ªäº¤é”™å›¾åƒå’Œæ–‡æœ¬æ•°æ®é›†ã€‚

![Flamingo's 4 datasets](14-flamingo-data.png)

<table data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb"><tbody data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb"><tr data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb"><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb"><strong data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">Dataset<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">æ•°æ®é›†</span></span></span></strong></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb"><strong data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">Type<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">ç±»å‹</span></span></span></strong></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb"><strong data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">Size<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">å°ºå¯¸</span></span></span></strong></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb"><strong data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">How</strong></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb"><strong data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">Training weight<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">è®­ç»ƒé‡é‡</span></span></span></strong></td></tr><tr data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb"><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">M3W</td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">Interleaved image and text dataset<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">äº¤é”™å›¾åƒå’Œæ–‡æœ¬æ•°æ®é›†</span></span></span></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">43M webpages<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">4 300 ä¸‡ç½‘é¡µ</span></span></span></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">For each webpage, they sample a random subsequence of 256 tokens and take up to the first 5 images included in the sampled sequence.<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">å¯¹äºæ¯ä¸ªç½‘é¡µï¼Œä»–ä»¬ä¼šéšæœºæŠ½å– 256 ä¸ªæ ‡è®°çš„å­åºåˆ—ï¼Œå¹¶å–æ ·åºåˆ—ä¸­çš„å‰ 5 ä¸ªå›¾åƒã€‚</span></span></span></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">1.0</td></tr><tr data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb"><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">ALIGN<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">å¯¹é½</span></span></span></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">(Image, text) pairs<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">(å›¾åƒã€æ–‡æœ¬ï¼‰å¯¹</span></span></span></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">1.8B pairs<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">18 äº¿å¯¹</span></span></span></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">Texts are alt-texts, averaging 12 tokens/text.<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">æ–‡æœ¬ä¸ºalt-textsï¼Œå¹³å‡æ¯ä¸ªæ–‡æœ¬æœ‰12ä¸ªæ ‡è®°ã€‚</span></span></span></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">0.2</td></tr><tr data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb"><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">LTIP</td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">(Image, text) pairs<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">(å›¾åƒã€æ–‡æœ¬ï¼‰å¯¹</span></span></span></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">312M pairs<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">3.12 äº¿å¯¹</span></span></span></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">Texts are long descriptions, averaging 20.5 tokens/text.<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">æ–‡æœ¬æ˜¯é•¿ç¯‡æè¿°ï¼Œå¹³å‡æ¯ç¯‡æ–‡æœ¬æœ‰ 20.5 ä¸ªæ ‡è®°ã€‚</span></span></span></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">0.2</td></tr><tr data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb"><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">VTP</td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">(Video, text) pairs<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">(è§†é¢‘ã€æ–‡å­—ï¼‰å¯¹</span></span></span></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">27M short videos<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">2,700 ä¸‡ä¸ªçŸ­è§†é¢‘</span></span></span></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">~22 seconds/video on average<span lang="zh-CN" data-immersive-translate-translation-element-mark="1"><br><span data-immersive-translate-translation-element-mark="1"><span data-immersive-translate-translation-element-mark="1">å¹³å‡æ¯æ®µè§†é¢‘ ~22 ç§’</span></span></span></td><td data-immersive-translate-effect="1" data-immersive_translate_walked="de41cc8b-cb6f-4426-9c68-52a1cc6cd3cb">0.03</td></tr></tbody></table>

### Flamingo's vision encoder  

ç«çƒˆé¸Ÿè§†è§‰ç¼–ç å™¨

Flamingo first trains a CLIP-like model from scratch using contrastive learning.  

Flamingo é¦–å…ˆåˆ©ç”¨å¯¹æ¯”å­¦ä¹ ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªç±»ä¼¼ CLIP çš„æ¨¡å‹ã€‚  

This component only uses the 2 (image, text) pair datasets, ALIGN and LTIP, totaling 2.1M (image, text) pairs.  

è¯¥ç»„ä»¶ä»…ä½¿ç”¨ ALIGN å’Œ LTIP è¿™ä¸¤ä¸ªï¼ˆå›¾åƒã€æ–‡æœ¬ï¼‰é…å¯¹æ•°æ®é›†ï¼Œå…±è®¡ 210 ä¸‡ä¸ªï¼ˆå›¾åƒã€æ–‡æœ¬ï¼‰é…å¯¹ã€‚  

This is 5x larger than the dataset CLIP was trained on.  

è¿™æ¯” CLIP æ‰€è®­ç»ƒçš„æ•°æ®é›†å¤§ 5 å€ã€‚

-   For the text encoder, Flamingo uses BERT instead of GPT-2.  
    
    åœ¨æ–‡æœ¬ç¼–ç å™¨æ–¹é¢ï¼ŒFlamingo ä½¿ç”¨ BERT è€Œä¸æ˜¯ GPT-2ã€‚
-   For the vision encoder, Flamingo uses a NormalizerFree ResNet (NFNet) F6 model.  
    
    åœ¨è§†è§‰ç¼–ç å™¨æ–¹é¢ï¼ŒFlamingo ä½¿ç”¨äº† NormalizerFree ResNet (NFNet) F6 æ¨¡å‹ã€‚
-   Text and vision embeddings are meanpooled before being projected to the joint embedding space.  
    
    åœ¨æŠ•å°„åˆ°è”åˆåµŒå…¥ç©ºé—´ä¹‹å‰ï¼Œå¯¹æ–‡æœ¬å’Œè§†è§‰åµŒå…¥è¿›è¡Œå‡å€¼æ± åŒ–å¤„ç†ã€‚

### Flamingo's language model  

ç«çƒˆé¸Ÿè¯­è¨€æ¨¡å‹

Flamingo uses Chinchilla as their language model.  

Flamingo ä½¿ç”¨ Chinchilla ä½œä¸ºè¯­è¨€æ¨¡å‹ã€‚  

More specifically, they freeze the 9 pretrained Chinchilla LM layers.  

æ›´å…·ä½“åœ°è¯´ï¼Œä»–ä»¬å†»ç»“äº† 9 ä¸ªç»è¿‡é¢„è®­ç»ƒçš„é’¦å¥‡æ‹‰ LM å›¾å±‚ã€‚  

A traditional language model predicts the next text token based on the preceding text tokens.  

ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹ä¼šæ ¹æ®å‰é¢çš„æ–‡æœ¬æ ‡è®°é¢„æµ‹ä¸‹ä¸€ä¸ªæ–‡æœ¬æ ‡è®°ã€‚  

Flamingo predicts the next text token based on both the preceding text and visual tokens.  

Flamingo ä¼šæ ¹æ®å‰é¢çš„æ–‡å­—å’Œè§†è§‰æ ‡è®°é¢„æµ‹ä¸‹ä¸€ä¸ªæ–‡å­—æ ‡è®°ã€‚

![Flamingo's 4 datasets](15-lmm-text-generation.png)

Next token generation is conditioned on both text and visual tokens.  

ä¸‹ä¸€ä¸ªæ ‡è®°çš„ç”Ÿæˆä»¥æ–‡æœ¬å’Œè§†è§‰æ ‡è®°ä¸ºæ¡ä»¶ã€‚  

Illustration taken from Chunyuan Li's CVPR 2023 tutorial: Large Multimodal Models.  

æ’å›¾æ‘˜è‡ªææ˜¥å…ƒçš„ CVPR 2023 æ•™ç¨‹ï¼šå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ã€‚

To be able to generate text conditioned on both text and visual inputs, Flamingo relied on Perceiver Resampler and GATED XATTN-DENSE layers.  

Flamingo ä¾é  Perceiver Resampler å’Œ GATED XATTN-DENSE å±‚æ¥ç”Ÿæˆä»¥æ–‡å­—å’Œè§†è§‰è¾“å…¥ä¸ºæ¡ä»¶çš„æ–‡æœ¬ã€‚

#### Perceiver Resampler  

æ¥æ”¶å™¨é‡é‡‡æ ·å™¨

As the visual inputs can be both images and videos, the vision encoder can produce a variable number of image or video features.  

ç”±äºè§†è§‰è¾“å…¥æ—¢å¯ä»¥æ˜¯å›¾åƒä¹Ÿå¯ä»¥æ˜¯è§†é¢‘ï¼Œå› æ­¤è§†è§‰ç¼–ç å™¨å¯ä»¥ç”Ÿæˆä¸åŒæ•°é‡çš„å›¾åƒæˆ–è§†é¢‘ç‰¹å¾ã€‚  

Perceiver Resampler converts these variable features into a consistent 64 visual outputs.  

Perceiver Resampler å¯å°†è¿™äº›å¯å˜ç‰¹å¾è½¬æ¢ä¸º 64 ç§ä¸€è‡´çš„è§†è§‰è¾“å‡ºã€‚

Interestingly enough, while training the vision encoder, the resolution used was 288 x 288. However, at this phase, visual inputs are resized to 320 Ã— 320. Itâ€™s been shown that [a higher test-time resolution can lead to improved performance when using CNNs](https://arxiv.org/abs/1906.06423).  

æœ‰è¶£çš„æ˜¯ï¼Œåœ¨è®­ç»ƒè§†è§‰ç¼–ç å™¨æ—¶ï¼Œä½¿ç”¨çš„åˆ†è¾¨ç‡æ˜¯ 288 x 288ã€‚ç„¶è€Œï¼Œåœ¨è¿™ä¸€é˜¶æ®µï¼Œè§†è§‰è¾“å…¥è¢«è°ƒæ•´ä¸º 320 Ã— 320ã€‚äº‹å®è¯æ˜ï¼Œåœ¨ä½¿ç”¨ CNN æ—¶ï¼Œæµ‹è¯•æ—¶é—´åˆ†è¾¨ç‡è¶Šé«˜ï¼Œæ€§èƒ½è¶Šå¥½ã€‚

![Flamingo's Perceiver Resampler](16-flamingo-perceiver-resampler.png)

#### GATED XATTN-DENSE layers  

é—¨æ§ XATTN-DENSE å±‚

GATED XATTN-DENSE layers are inserted between existing and frozen LM layers to allow the language model to attend more efficiently to the visual tokens when generating text tokens.  

GATED XATTN-DENSE å±‚è¢«æ’å…¥ç°æœ‰å’Œå†»ç»“çš„ LM å±‚ä¹‹é—´ï¼Œä»¥ä¾¿è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ ‡è®°æ—¶æ›´æœ‰æ•ˆåœ°å…³æ³¨è§†è§‰æ ‡è®°ã€‚  

Without these layers, Flamingo authors noted a drop of 4.2% in the overall score.  

å¦‚æœæ²¡æœ‰è¿™äº›å±‚ï¼ŒFlamingo ä½œè€…æ³¨æ„åˆ°æ€»åˆ†ä¸‹é™äº† 4.2%ã€‚

![Flamingo's GATED ATTN-DENSE layers](17-gated20xattn-dense.png)

#### Loss function  

æŸå¤±å‡½æ•°

Flamingo computes the likelihood of textFlamingo æ ¹æ®äº¤é”™å›¾åƒå’Œè§†é¢‘ \\(x\\)è®¡ç®—æ–‡æœ¬ \\(y\\)çš„å¯èƒ½æ€§ã€‚ conditioned on the interleaved images and videos .

  

\\\[p(y|x)=prod\_{l=1}^N p(y\_l|y\_{<l}, x\_{\\leq l})/\]ã€‚

The training loss function was a weighted sum of expected negative log-likelihoods of generated text across all 4 datasets, withè®­ç»ƒæŸå¤±å‡½æ•°æ˜¯æ‰€æœ‰ 4 ä¸ªæ•°æ®é›†ç”Ÿæˆæ–‡æœ¬çš„é¢„æœŸè´Ÿå¯¹æ•°ä¼¼ç„¶å€¼çš„åŠ æƒå’Œï¼Œå…¶ä¸­ \\(\\lambda\_m\\) æ˜¯æ•°æ®é›† \\(m\\) çš„è®­ç»ƒæƒé‡ã€‚ being the training weight of dataset .

  

\\\[ï¼ˆsum\_{m=1}^M \\lambda\_m E\_{(x, y)\\sim D\_m} \[ -\\sum\_{l=1}^L \\log p(y|x)\]\\\]\].

#### Training  

åŸ¹è®­

While the Chinchilla LM layers are finetuned and frozen, the additional components are trained from scratch, using all 4 Flamingo datasets, with different weights. _Finding the right per-dataset weights was key to performance._ The weight for each dataset is in the **Training weight** column in the dataset table above.  

åœ¨å¯¹ Chinchilla LM å±‚è¿›è¡Œå¾®è°ƒå’Œå†»ç»“çš„åŒæ—¶ï¼Œä½¿ç”¨å…¨éƒ¨ 4 ä¸ª Flamingo æ•°æ®é›†ï¼Œä»¥ä¸åŒçš„æƒé‡ï¼Œä»å¤´å¼€å§‹è®­ç»ƒé™„åŠ ç»„ä»¶ã€‚æ‰¾åˆ°åˆé€‚çš„æ•°æ®é›†æƒé‡æ˜¯æé«˜æ€§èƒ½çš„å…³é”®ã€‚æ¯ä¸ªæ•°æ®é›†çš„æƒé‡éƒ½åœ¨ä¸Šè¡¨æ•°æ®é›†çš„è®­ç»ƒæƒé‡æ ä¸­ã€‚

VTPâ€™s weight is much smaller than other datasets (0.03 compared to 0.2 and 1), so its contribution to the training should be minimal.  

VTP çš„æƒé‡æ¯”å…¶ä»–æ•°æ®é›†è¦å°å¾—å¤šï¼ˆ0.03ï¼Œè€Œ 0.2 å’Œ 1ï¼‰ï¼Œå› æ­¤å®ƒå¯¹è®­ç»ƒçš„è´¡çŒ®åº”è¯¥æ˜¯æœ€å°çš„ã€‚  

However, the authors noted that removing this dataset negatively affects performance on all video tasks.  

ä¸è¿‡ï¼Œä½œè€…æŒ‡å‡ºï¼Œç§»é™¤è¯¥æ•°æ®é›†ä¼šå¯¹æ‰€æœ‰è§†é¢‘ä»»åŠ¡çš„æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚

While Flamingo isnâ€™t open-sourced, there are many open-source replications of Flamingo.  

è™½ç„¶ Flamingo æ²¡æœ‰å¼€æºï¼Œä½†æœ‰è®¸å¤š Flamingo çš„å¼€æºå¤åˆ¶å“ã€‚

-   [IDEFICS](https://huggingface.co/spaces/HuggingFaceM4/idefics_playground) (HuggingFace)  
    
    IDEFICS (æ‹¥æŠ±è„¸)
-   [mlfoundations/open\_flamingo](https://github.com/mlfoundations/open_flamingo/issues)

## TL;DR: CLIP vs. Flamingo  

ç®€è¦è¯´æ˜ï¼šCLIP vs. Flamingo

![Flamingo's 4 datasets](18-clip-flamingo.png)

## Part 3. Research Directions for LMMs  

ç¬¬ 3 éƒ¨åˆ†.LMM çš„ç ”ç©¶æ–¹å‘

CLIP is 3 years old and Flamingo is almost 2. While their architectures serve as a good foundation for us to understand how LMMs are built, there have been many new progresses in the space.  

CLIP å·²æœ‰ 3 å¹´å†å²ï¼ŒFlamingo ä¹Ÿå°†è¿‘ 2 å¹´ã€‚è™½ç„¶å®ƒä»¬çš„æ¶æ„ä¸ºæˆ‘ä»¬äº†è§£ LMM çš„æ„å»ºæ–¹å¼å¥ å®šäº†è‰¯å¥½åŸºç¡€ï¼Œä½†è¯¥é¢†åŸŸä»å–å¾—äº†è®¸å¤šæ–°è¿›å±•ã€‚

Here are a few directions that Iâ€™m excited about.  

ä»¥ä¸‹æ˜¯å‡ ä¸ªè®©æˆ‘æ„Ÿåˆ°å…´å¥‹çš„æ–¹å‘ã€‚  

This is far from an exhaustive list, both because this post has been long and because Iâ€™m still learning about the space too.  

è¿™è¿œä¸æ˜¯ä¸€ä»½è¯¦å°½æ— é—çš„æ¸…å•ï¼Œå› ä¸ºè¿™ç¯‡æ–‡ç« å¾ˆé•¿ï¼Œè€Œä¸”æˆ‘ä¹Ÿè¿˜åœ¨äº†è§£è¿™ä¸ªç©ºé—´ã€‚  

If you have any pointers or suggestions, please let me know!  

å¦‚æœæ‚¨æœ‰ä»»ä½•æŒ‡ç‚¹æˆ–å»ºè®®ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼

## Incorporating more data modalities  

çº³å…¥æ›´å¤šæ•°æ®æ¨¡å¼

Today, most multimodal systems work with text and images.  

å¦‚ä»Šï¼Œå¤§å¤šæ•°å¤šæ¨¡æ€ç³»ç»Ÿéƒ½ä½¿ç”¨æ–‡æœ¬å’Œå›¾åƒã€‚  

Itâ€™s only a matter of time before we need systems that can incorporate other modalities such as videos, music, and 3D.  

æˆ‘ä»¬è¿Ÿæ—©ä¼šéœ€è¦èƒ½å¤Ÿæ•´åˆè§†é¢‘ã€éŸ³ä¹å’Œ 3D ç­‰å…¶ä»–æ¨¡å¼çš„ç³»ç»Ÿã€‚  

Wouldnâ€™t it be amazing to have one shared embedding space for ALL data modalities?  

å¦‚æœæ‰€æœ‰æ•°æ®æ¨¡å¼éƒ½æœ‰ä¸€ä¸ªå…±äº«çš„åµŒå…¥ç©ºé—´ï¼Œå²‚ä¸å¦™å“‰ï¼Ÿ

Examples of works in this space:  

è¯¥é¢†åŸŸçš„ä½œå“ä¸¾ä¾‹

-   [ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding](https://arxiv.org/abs/2212.05171) (Xue et al., Dec 2022)  
    
    ULIPï¼šå­¦ä¹ è¯­è¨€ã€å›¾åƒå’Œç‚¹äº‘çš„ç»Ÿä¸€è¡¨ç¤ºï¼Œå®ç°ä¸‰ç»´ç†è§£ï¼ˆXue ç­‰äººï¼Œ2022 å¹´ 12 æœˆï¼‰
-   [ImageBind: One Embedding Space To Bind Them All](https://browse.arxiv.org/abs/2305.05665) (Girdhar et al., May 2023)  
    
    å›¾åƒç»‘å®šï¼šä¸€ä¸ªåµŒå…¥ç©ºé—´ç»‘å®šæ‰€æœ‰å›¾åƒï¼ˆGirdhar ç­‰äººï¼Œ2023 å¹´ 5 æœˆï¼‰
-   Jeff Deanâ€™s ambitious [Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/) project (2021): its vision is to â€œ_enable multimodal models that encompass vision, auditory, and language understanding simultaneously_.â€  
    
    æ°å¤«-è¿ªæ©é›„å¿ƒå‹ƒå‹ƒçš„ Pathways é¡¹ç›®ï¼ˆ2021 å¹´ï¼‰ï¼šå…¶æ„¿æ™¯æ˜¯ "å®ç°åŒæ—¶åŒ…å«è§†è§‰ã€å¬è§‰å’Œè¯­è¨€ç†è§£çš„å¤šæ¨¡æ€æ¨¡å‹"ã€‚

![Imagebind](19-imagebind.png)

## Multimodal systems for instruction-following  

å¤šæ¨¡å¼æ•™å­¦ç³»ç»Ÿ

Flamingo was trained for completion, but not for dialogue or for following instructions.  

ç«çƒˆé¸Ÿæ¥å—çš„æ˜¯å®Œæˆä»»åŠ¡çš„è®­ç»ƒï¼Œè€Œä¸æ˜¯å¯¹è¯æˆ–å¬ä»æŒ‡ä»¤çš„è®­ç»ƒã€‚  

(If youâ€™re not familiar with completion vs. dialogue, check out my post on [RLHF](https://huyenchip.com/2023/05/02/rlhf.html)).  

(å¦‚æœæ‚¨è¿˜ä¸äº†è§£ "å®Œæˆ "ä¸ "å¯¹è¯"ï¼Œè¯·æŸ¥çœ‹æˆ‘åœ¨ RLHF ä¸Šå‘è¡¨çš„æ–‡ç« ï¼‰ã€‚  

Many people are working on building LMMs that can follow instructions and have conversations, such as:  

è®¸å¤šäººéƒ½åœ¨ç ”ç©¶å¦‚ä½•åˆ¶é€ èƒ½å¤Ÿå¬ä»æŒ‡ä»¤å¹¶è¿›è¡Œå¯¹è¯çš„ LMMï¼Œä¾‹å¦‚

-   [MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning](https://arxiv.org/abs/2212.10773) (Xu et al., Dec 2022)  
    
    MultiInstructï¼šé€šè¿‡æŒ‡ä»¤è°ƒæ•´æ”¹è¿›å¤šæ¨¡å¼é›¶ç‚¹å­¦ä¹ ï¼ˆXu ç­‰äººï¼Œ2022 å¹´ 12 æœˆï¼‰
-   [LLaVA: Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) (Liu et al., Apr 28, 2023)  
    
    LLaVAï¼šè§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆLiu ç­‰äººï¼Œ2023 å¹´ 4 æœˆ 28 æ—¥ï¼‰
-   [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) (Salesforce, May 11, 2023)  
    
    InstructBLIPï¼šåˆ©ç”¨æŒ‡ä»¤è°ƒæ•´å®ç°é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆSalesforceï¼Œ2023 å¹´ 5 æœˆ 11 æ—¥ï¼‰
-   LaVIN: [Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.15023) (Luo et al., May 24, 2023)  
    
    LaVINï¼šä¾¿å®œåˆå¿«é€Ÿï¼šé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆè§†è§‰è¯­è¨€æŒ‡ä»¤è°ƒæ•´ï¼ˆLuo ç­‰äººï¼Œ2023 å¹´ 5 æœˆ 24 æ—¥ï¼‰

![LaVIN](20-LaVIN.png)

Examples of LaVIN's outputs compared to other LMMs, shown in LaVIN's paper  

LaVIN çš„äº§å‡ºä¸å…¶ä»– LMM ç›¸æ¯”çš„ä¾‹å­ï¼Œè§ LaVIN çš„è®ºæ–‡

## Adapters for more efficient multimodal training  

æ›´é«˜æ•ˆçš„å¤šæ¨¡å¼åŸ¹è®­é€‚é…å™¨

While Flamingo used 9 pretrained and frozen layers from Chinchilla, it had to pretrain its vision encoder, Perceiver Resampler, and GATED XATTN-DENSE layers from scratch.  

Flamingo ä½¿ç”¨äº† Chinchilla çš„ 9 ä¸ªé¢„è®­ç»ƒå’Œå†»ç»“å±‚ï¼Œä½†å®ƒå¿…é¡»ä»å¤´å¼€å§‹é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ã€æ„ŸçŸ¥å™¨é‡é‡‡æ ·å™¨å’Œ GATED XATTN-DENSE å±‚ã€‚  

These train-from-scratch modules could be compute-intensive.  

è¿™äº› "ä»é›¶å¼€å§‹ "çš„æ¨¡å—å¯èƒ½æ˜¯è®¡ç®—å¯†é›†å‹çš„ã€‚  

Many works focus on more efficient ways to bootstrap multimodal systems using less training from scratch.  

è®¸å¤šç ”ç©¶éƒ½é›†ä¸­åœ¨å¦‚ä½•æ›´æœ‰æ•ˆåœ°å¼•å¯¼å¤šæ¨¡æ€ç³»ç»Ÿï¼Œå‡å°‘ä»å¤´å¼€å§‹çš„è®­ç»ƒã€‚

Some works are quite promising.  

æœ‰äº›ä½œå“å¾ˆæœ‰å¸Œæœ›ã€‚  

BLIP-2, for example, outperformed Flamingo-80B by 8.7% on zero-shot VQA-v2 with 54x fewer trainable parameters.  

ä¾‹å¦‚ï¼ŒBLIP-2 åœ¨é›¶é•œå¤´ VQA-v2 ä¸Šçš„è¡¨ç°æ¯” Flamingo-80B é«˜å‡º 8.7%ï¼Œè€Œå¯è®­ç»ƒå‚æ•°å´å°‘äº† 54 å€ã€‚

Works in this space include:  

è¯¥é¢†åŸŸçš„ä½œå“åŒ…æ‹¬

-   [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models  
    
    BLIP-2ï¼šåˆ©ç”¨å†»ç»“å›¾åƒç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹å¼•å¯¼è¯­è¨€å›¾åƒé¢„è®­ç»ƒ](https://arxiv.org/abs/2301.12597)
-   \[LAVIN\] [Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.15023)  
    
    \[LAVIN\]ä¾¿å®œåˆå¿«é€Ÿï¼šé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆè§†è§‰è¯­è¨€æŒ‡ä»¤è°ƒæ•´
-   [LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model  
    
    LLaMA-Adapter V2ï¼šå‚æ•°æ•ˆç‡å¯è§†åŒ–æŒ‡ä»¤æ¨¡å‹](https://arxiv.org/abs/2304.15010)

The two images below are from Chunyuan Liâ€™s [Large Multimodal Models](https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf) tutorial at CVPR 2023, which is, btw, an excellent tutorial.  

ä¸‹é¢çš„ä¸¤å¼ å›¾ç‰‡æ¥è‡ªææ˜¥å…ƒåœ¨ CVPR 2023 ä¸Šçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ•™ç¨‹ã€‚

![Adapters for LMMs](21-adapters-1.png)

![Adapters for LMMs](22-adapters-2.png)

## Generating multimodal outputs  

ç”Ÿæˆå¤šæ¨¡å¼è¾“å‡º

While models that can process multimodal inputs are becoming the norm, multimodal output is still lagging.  

è™½ç„¶èƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€è¾“å…¥çš„æ¨¡å‹æ­£åœ¨æˆä¸ºè§„èŒƒï¼Œä½†å¤šæ¨¡æ€è¾“å‡ºä»ç„¶æ»åã€‚  

Many use cases require multimodal outputs.  

è®¸å¤šç”¨ä¾‹éœ€è¦å¤šæ¨¡å¼è¾“å‡ºã€‚  

For example, if we ask ChatGPT to explain RLHF, an effective explanation might require graphs, equations, and even simple animations.  

ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬è¦æ±‚ ChatGPT è§£é‡Š RLHFï¼Œæœ‰æ•ˆçš„è§£é‡Šå¯èƒ½éœ€è¦å›¾å½¢ã€æ–¹ç¨‹ç”šè‡³ç®€å•çš„åŠ¨ç”»ã€‚

To generate multimodal outputs, a model would first need to generate a shared intermediate output.  

è¦ç”Ÿæˆå¤šæ¨¡å¼è¾“å‡ºï¼Œæ¨¡å‹é¦–å…ˆéœ€è¦ç”Ÿæˆä¸€ä¸ªå…±äº«çš„ä¸­é—´è¾“å‡ºã€‚  

One key question is what the intermediate output would look like.  

ä¸€ä¸ªå…³é”®é—®é¢˜æ˜¯ä¸­é—´äº§å‡ºæ˜¯ä»€ä¹ˆæ ·å­çš„ã€‚

One option for intermediate output is text, which will then be used to generate/synthesize other actions.  

ä¸­é—´è¾“å‡ºçš„ä¸€ä¸ªé€‰é¡¹æ˜¯æ–‡æœ¬ï¼Œç„¶åå°†ç”¨äºç”Ÿæˆ/åˆæˆå…¶ä»–æ“ä½œã€‚

For example, [CM3](https://arxiv.org/abs/2201.07520) (Aghajanyan et al., 2022) outputs HTML markup which can be compiled into webpages that contain not only text but also formattings, links, and images.  

ä¾‹å¦‚ï¼ŒCM3ï¼ˆAghajanyan ç­‰äººï¼Œ2022 å¹´ï¼‰å¯è¾“å‡º HTML æ ‡è®°ï¼Œå°†å…¶ç¼–è¯‘æˆä¸ä»…åŒ…å«æ–‡æœ¬è¿˜åŒ…å«æ ¼å¼ã€é“¾æ¥å’Œå›¾åƒçš„ç½‘é¡µã€‚  

GPT-4V generates Latex code, which can then be reconstructed as data tables.  

GPT-4V ç”Ÿæˆ Latex ä»£ç ï¼Œç„¶åå¯å°†å…¶é‡æ„ä¸ºæ•°æ®è¡¨ã€‚

![CM3](23-cm3.png)

Sampled outputs from CM3  

CM3 çš„é‡‡æ ·è¾“å‡º

![GPT-4V generating LaTeX](24-gpt-4v-latex.png)

GPT-4V generates Latex code, which can then be reconstructed as a data table  

GPT-4V ç”Ÿæˆ Latex ä»£ç ï¼Œç„¶åå¯å°†å…¶é‡æ„ä¸ºæ•°æ®è¡¨

Another option for intermediate output would be multimodal tokens. This is the option that [Caiming Xiong](https://www.linkedin.com/in/caiming-xiong-150a1417), whose team at Salesforce has done a lot of awesome work on multimodality, told me.  

ä¸­é—´è¾“å‡ºçš„å¦ä¸€ç§é€‰æ‹©æ˜¯å¤šæ¨¡æ€æ ‡è®°ã€‚è¿™å°±æ˜¯ç†Šæ‰æ˜å‘Šè¯‰æˆ‘çš„æ–¹æ¡ˆï¼Œä»–åœ¨ Salesforce çš„å›¢é˜Ÿåœ¨å¤šæ¨¡æ€æ–¹é¢åšäº†å¾ˆå¤šäº†ä¸èµ·çš„å·¥ä½œã€‚  

Each token will have a tag to denote whether itâ€™s a text token or an image token.  

æ¯ä¸ªæ ‡è®°ç¬¦éƒ½æœ‰ä¸€ä¸ªæ ‡ç­¾ï¼Œç”¨æ¥è¡¨ç¤ºæ˜¯æ–‡æœ¬æ ‡è®°ç¬¦è¿˜æ˜¯å›¾åƒæ ‡è®°ç¬¦ã€‚  

Image tokens will then be input into an image model like Diffusion to generate images.  

ç„¶åï¼Œå›¾åƒæ ‡è®°å°†è¢«è¾“å…¥åˆ°åƒ Diffusion è¿™æ ·çš„å›¾åƒæ¨¡å‹ä¸­ï¼Œä»¥ç”Ÿæˆå›¾åƒã€‚  

Text tokens will then be input into a language model.  

ç„¶åå°†æ–‡æœ¬æ ‡è®°è¾“å…¥è¯­è¨€æ¨¡å‹ã€‚

[Generating Images with Multimodal Language Models](https://arxiv.org/abs/2305.17216) (Koh et al., Jun 2023) is an awesome paper that shows how LMMs can generate and retrieve images together with generating texts. See below.  

åˆ©ç”¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ç”Ÿæˆå›¾åƒã€‹ï¼ˆKoh ç­‰äººï¼Œ2023 å¹´ 6 æœˆï¼‰æ˜¯ä¸€ç¯‡å‡ºè‰²çš„è®ºæ–‡ï¼Œå®ƒå±•ç¤ºäº† LMM å¦‚ä½•åœ¨ç”Ÿæˆæ–‡æœ¬çš„åŒæ—¶ç”Ÿæˆå’Œæ£€ç´¢å›¾åƒã€‚è¯·å‚è§ä¸‹æ–‡ã€‚

![LMMs generating text and images](27-lmms-generating-images.png)

## Conclusion  

ç»“è®º

Itâ€™s been a lot of fun going over so many multimodal papers as well as talking to people doing awesome work and trying to summarize the key patterns in one blog post.  

é˜…è¯»è¿™ä¹ˆå¤šçš„å¤šæ¨¡æ€è®ºæ–‡ï¼Œä¸å·¥ä½œå‡ºè‰²çš„äººäº¤è°ˆï¼Œå¹¶å°è¯•åœ¨ä¸€ç¯‡åšæ–‡ä¸­æ€»ç»“å…³é”®æ¨¡å¼ï¼Œè¿™çœŸæ˜¯ä¸€ä»¶éå¸¸æœ‰è¶£çš„äº‹æƒ…ã€‚  

Thereâ€™s so much about multimodality that Iâ€™m sure there are many things that Iâ€™ve missed, but I hope that this post provides the core patterns that will help you develop multimodal systems and apply them to your work.  

å…³äºå¤šæ¨¡æ€çš„å†…å®¹å¤ªå¤šäº†ï¼Œæˆ‘è‚¯å®šè¿˜æœ‰å¾ˆå¤šé—æ¼çš„åœ°æ–¹ï¼Œä½†æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« æä¾›çš„æ ¸å¿ƒæ¨¡å¼èƒ½å¸®åŠ©ä½ å¼€å‘å¤šæ¨¡æ€ç³»ç»Ÿï¼Œå¹¶å°†å…¶åº”ç”¨åˆ°ä½ çš„å·¥ä½œä¸­ã€‚

As you see in part 3 of this post, weâ€™re still in the early days of multimodal systems (so early that a friend told me heâ€™s not sure if the LMM abbreviation would catch on).  

æ­£å¦‚æœ¬æ–‡ç¬¬ä¸‰éƒ¨åˆ†æ‰€è¿°ï¼Œæˆ‘ä»¬ä»å¤„äºå¤šæ¨¡æ€ç³»ç»Ÿçš„æ—©æœŸé˜¶æ®µï¼ˆä»¥è‡³äºä¸€ä½æœ‹å‹å‘Šè¯‰æˆ‘ï¼Œä»–ä¸ç¡®å®š LMM ç¼©å†™æ˜¯å¦ä¼šæµè¡Œèµ·æ¥ï¼‰ã€‚  

Yes, in most of my conversations, thereâ€™s little doubt that multimodal systems in general, and LMMs in particular, will be even more impactful than large language models.  

æ˜¯çš„ï¼Œåœ¨æˆ‘çš„å¤§å¤šæ•°è°ˆè¯ä¸­ï¼Œæ¯«æ— ç–‘é—®ï¼Œå¤šæ¨¡æ€ç³»ç»Ÿï¼Œå°¤å…¶æ˜¯ LMMï¼Œå°†æ¯”å¤§å‹è¯­è¨€æ¨¡å‹æ›´å…·å½±å“åŠ›ã€‚  

However, keep in mind that LMMs do not make LLMs obsolete.  

ä¸è¿‡ï¼Œè¯·è®°ä½ï¼ŒLMM ä¸ä¼šä½¿ LLM è¿‡æ—¶ã€‚  

As LMMs extend upon LLMs, the performance of an LMM relies on the performance of its base LLM.  

ç”±äº LMM æ˜¯åœ¨ LLM çš„åŸºç¡€ä¸Šæ‰©å±•çš„ï¼Œå› æ­¤ LMM çš„æ€§èƒ½å–å†³äºå…¶åŸºç¡€ LLM çš„æ€§èƒ½ã€‚  

Many labs that work on multimodal systems work on LLMs in parallel.  

è®¸å¤šç ”ç©¶å¤šæ¨¡æ€ç³»ç»Ÿçš„å®éªŒå®¤åŒæ—¶ç ”ç©¶ LLMã€‚

## Early reviewers  

æ—©æœŸå®¡æŸ¥å‘˜

Iâ€™d like to thank the amazing early reviewers who gave me plenty of pointers and suggestions to make this post better: [Han-chung Lee](https://www.linkedin.com/in/hanchunglee/), [Sam Reiswig](https://www.linkedin.com/in/samreiswig/), and [Luke Metz](https://twitter.com/Luke_Metz).  

æˆ‘è¦æ„Ÿè°¢é‚£äº›äº†ä¸èµ·çš„æ—©æœŸå®¡ç¨¿äººï¼Œä»–ä»¬ç»™äº†æˆ‘å¾ˆå¤šæŒ‡ç‚¹å’Œå»ºè®®ï¼Œè®©è¿™ç¯‡æ–‡ç« å˜å¾—æ›´å¥½ï¼šHan-chung Leeã€Sam Reiswig å’Œ Luke Metzã€‚

## Resources  

èµ„æº

### Models  

æœºå‹

An incomplete list of multimodal systems by time to give you a sense of how fast the space is moving!  

æŒ‰æ—¶é—´åˆ†åˆ—çš„å¤šæ¨¡å¼ç³»ç»Ÿä¸å®Œæ•´æ¸…å•ï¼Œè®©æ‚¨äº†è§£è¯¥é¢†åŸŸçš„å‘å±•é€Ÿåº¦ï¼

-   [Microsoft COCO Captions: Data Collection and Evaluation Server](https://arxiv.org/abs/1504.00325) (Apr 2015)  
    
    Microsoft COCO å­—å¹•ï¼šæ•°æ®æ”¶é›†å’Œè¯„ä¼°æœåŠ¡å™¨ï¼ˆ2015 å¹´ 4 æœˆï¼‰
-   [VQA: Visual Question Answering](https://arxiv.org/abs/1505.00468) (May 2015)  
    
    VQAï¼šå¯è§†åŒ–é—®é¢˜è§£ç­”ï¼ˆ2015 å¹´ 5 æœˆï¼‰
-   [VideoBERT: A Joint Model for Video and Language Representation Learning](https://arxiv.org/abs/1904.01766) (Google, Apr 3, 2019)  
    
    VideoBERTï¼šè§†é¢‘å’Œè¯­è¨€è¡¨å¾å­¦ä¹ çš„è”åˆæ¨¡å‹ï¼ˆè°·æ­Œï¼Œ2019å¹´4æœˆ3æ—¥ï¼‰
-   [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490) (UNC Chapel Hill, Aug 20, 2019)  
    
    LXMERTï¼šä»å˜æ¢å™¨å­¦ä¹ è·¨æ¨¡æ€ç¼–ç å™¨è¡¨å¾ï¼ˆ2019å¹´8æœˆ20æ—¥ï¼ŒåŒ—å¡ç½—æ¥çº³å·ç«‹å¤§å­¦æ•™å ‚å±±åˆ†æ ¡ï¼‰
-   [\[CLIP\] Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (OpenAI, 2021)  
    
    \[CLIP\] ä»è‡ªç„¶è¯­è¨€ç›‘ç£ä¸­å­¦ä¹ å¯è½¬ç§»çš„è§†è§‰æ¨¡å‹ï¼ˆOpenAIï¼Œ2021 å¹´ï¼‰
-   [Unifying Vision-and-Language Tasks via Text Generation](https://arxiv.org/abs/2102.02779) (UNC Chapel Hill, May 2021)  
    
    é€šè¿‡æ–‡æœ¬ç”Ÿæˆç»Ÿä¸€è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ï¼ˆ2021 å¹´ 5 æœˆï¼Œè”åˆå›½å¤§å­¦æ•™å ‚å±±åˆ†æ ¡ï¼‰
-   [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) (Salesforce, Jan 28, 2022)  
    
    BLIPï¼šå¼•å¯¼è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼Œå®ç°ç»Ÿä¸€çš„è§†è§‰è¯­è¨€ç†è§£å’Œç”Ÿæˆï¼ˆSalesforceï¼Œ2022 å¹´ 1 æœˆ 28 æ—¥ï¼‰
-   [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198) (DeepMind, April 29, 2022)  
    
    ç«çƒˆé¸Ÿï¼šç”¨äºå¿«é€Ÿå­¦ä¹ çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆDeepMindï¼Œ2022 å¹´ 4 æœˆ 29 æ—¥ï¼‰
-   [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) (Microsoft, May 2, 2022)  
    
    GITï¼šè§†è§‰å’Œè¯­è¨€çš„å›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆè½¬æ¢å™¨ï¼ˆå¾®è½¯ï¼Œ2022 å¹´ 5 æœˆ 2 æ—¥ï¼‰
-   [MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning](https://arxiv.org/abs/2212.10773) (Xu et al., Dec 2022)  
    
    MultiInstructï¼šé€šè¿‡æŒ‡ä»¤è°ƒæ•´æ”¹è¿›å¤šæ¨¡å¼é›¶ç‚¹å­¦ä¹ ï¼ˆXu ç­‰äººï¼Œ2022 å¹´ 12 æœˆï¼‰
-   [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) (Salesforce, Jan 30, 2023)  
    
    BLIP-2ï¼šåˆ©ç”¨å†»ç»“å›¾åƒç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹å¼•å¯¼è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆSalesforceï¼Œ2023 å¹´ 1 æœˆ 30 æ—¥ï¼‰
-   [Cross-Modal Fine-Tuning: Align then Refine](https://arxiv.org/abs/2302.05738) (Shen et al., Feb 11, 2023)  
    
    è·¨æ¨¡å¼å¾®è°ƒï¼šå…ˆå¯¹é½å†å®Œå–„ï¼ˆShen ç­‰ï¼Œ2023 å¹´ 2 æœˆ 11 æ—¥ï¼‰
-   [KOSMOS-1: Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045) (Microsoft, Feb 27, 2023)  
    
    KOSMOS-1ï¼šè¯­è¨€å¹¶éä½ æ‰€éœ€è¦çš„å…¨éƒ¨ï¼šå°†æ„ŸçŸ¥ä¸è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼ˆå¾®è½¯ï¼Œ2023 å¹´ 2 æœˆ 27 æ—¥ï¼‰
-   [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378) (Google, Mar 10, 2023)  
    
    PaLM-Eï¼šåµŒå…¥å¼å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆè°·æ­Œï¼Œ2023 å¹´ 3 æœˆ 10 æ—¥ï¼‰
-   [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/abs/2303.16199) (Zhang et al., Mar 28, 2023)  
    
    LLaMA-Adapterï¼šåˆ©ç”¨é›¶åˆå§‹æ³¨æ„é«˜æ•ˆå¾®è°ƒè¯­è¨€æ¨¡å‹ï¼ˆZhang ç­‰äººï¼Œ2023 å¹´ 3 æœˆ 28 æ—¥ï¼‰
-   [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality](https://arxiv.org/abs/2304.14178) (Ye et al., Apr 2, 2023)  
    
    mPLUG-Owlï¼šæ¨¡å—åŒ–èµ‹äºˆå¤§å‹è¯­è¨€æ¨¡å‹ä»¥å¤šæ¨¡æ€æ€§ï¼ˆYe ç­‰äººï¼Œ2023 å¹´ 4 æœˆ 2 æ—¥ï¼‰
-   [LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model](https://arxiv.org/abs/2304.15010) (Gao et al., Apr 28, 2023)  
    
    LLaMA-Adapter V2ï¼šå‚æ•°æ•ˆç‡å¯è§†åŒ–æŒ‡ä»¤æ¨¡å‹ï¼ˆGao ç­‰äººï¼Œ2023 å¹´ 4 æœˆ 28 æ—¥ï¼‰
-   [LLaVA: Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) (Liu et al., Apr 28, 2023)  
    
    LLaVAï¼šè§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆLiu ç­‰äººï¼Œ2023 å¹´ 4 æœˆ 28 æ—¥ï¼‰
-   [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) (Salesforce, May 11, 2023)  
    
    InstructBLIPï¼šåˆ©ç”¨æŒ‡ä»¤è°ƒæ•´å®ç°é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆSalesforceï¼Œ2023 å¹´ 5 æœˆ 11 æ—¥ï¼‰
-   [Towards Expert-Level Medical Question Answering with Large Language Models](https://arxiv.org/abs/2305.09617) (Singhal et al., May 16, 2023)  
    
    åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å®ç°ä¸“å®¶çº§åŒ»å­¦é—®é¢˜è§£ç­”ï¼ˆSinghal ç­‰äººï¼Œ2023 å¹´ 5 æœˆ 16 æ—¥ï¼‰
-   [Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.15023) (Luo et al., May 24, 2023)  
    
    ä¾¿å®œåˆå¿«é€Ÿï¼šé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆè§†è§‰è¯­è¨€æŒ‡ä»¤è°ƒæ•´ï¼ˆLuo ç­‰äººï¼Œ2023 å¹´ 5 æœˆ 24 æ—¥ï¼‰
-   [Shikra: Unleashing Multimodal LLMâ€™s Referential Dialogue Magic](https://arxiv.org/abs/2306.15195) (SenseTime, Jun 3, 2023)  
    
    Shikraï¼šé‡Šæ”¾å¤šæ¨¡æ€ LLM çš„å‚è€ƒå¯¹è¯é­”åŠ›ï¼ˆSenseTimeï¼Œ2023 å¹´ 6 æœˆ 3 æ—¥ï¼‰
-   [Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration](https://arxiv.org/abs/2306.09093) (Tencent, Jun 15, 2023)  
    
    Macaw-LLMï¼šæ•´åˆå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘å’Œæ–‡æœ¬çš„å¤šæ¨¡æ€è¯­è¨€å»ºæ¨¡ï¼ˆè…¾è®¯ï¼Œ2023 å¹´ 6 æœˆ 15 æ—¥ï¼‰

### Other resources  

å…¶ä»–èµ„æº

-   \[CVPR2023 Tutorial Talk\] [Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4](https://www.youtube.com/watch?v=mkI7EPD1vp8)  
    
    \[CVPR2023æ•™ç¨‹è®²åº§\]å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼šæ„å»ºå’Œè¶…è¶Šå¤šæ¨¡æ€ GPT-4
    -   Slides: [Large Multimodal Models](https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf)  
        
        å¹»ç¯ç‰‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹
-   \[CMU course\] [11-777 MMML](https://cmu-multicomp-lab.github.io/mmml-course/fall2022/)  
    
    \[CMUè¯¾ç¨‹\] 11-777 MMML
-   \[Open source\] [Salesforceâ€™s LAVIS](https://github.com/salesforce/LAVIS)  
    
    \[å¼€æº\] Salesforce çš„ LAVIS
