---
title: "æ¨ç†æ¨¡å‹çš„ç°çŠ¶"
date: 2025-03-09T19:00:31+08:00
updated: 2025-03-19T19:00:31+08:00
taxonomies:
  tags: []
extra:
  source: https://magazine.sebastianraschka.com/p/state-of-llm-reasoning-and-inference-scaling
  hostname: magazine.sebastianraschka.com
  author: Sebastian Raschka, PhD
  original_title: "The State of LLM Reasoning Models"
  original_lang: en
---

Improving the reasoning abilities of large language models (LLMs) has become one of the hottest topics in 2025, and for good reason. Stronger reasoning skills allow LLMs to tackle more complex problems, making them more capable across a wide range of tasks users care about.  

æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ› (LLMs) å·²æˆä¸º 2025 å¹´æœ€çƒ­é—¨çš„è¯é¢˜ä¹‹ä¸€ï¼Œè¿™æ˜¯æœ‰å……åˆ†ç†ç”±çš„ã€‚æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ä½¿ LLMs èƒ½å¤Ÿè§£å†³æ›´å¤æ‚çš„é—®é¢˜ï¼Œä½¿å…¶æ›´æœ‰èƒ½åŠ›å®Œæˆç”¨æˆ·å…³å¿ƒçš„å¹¿æ³›ä»»åŠ¡ã€‚

In the last few weeks, researchers have shared a large number of new strategies to improve reasoning, including scaling inference-time compute, reinforcement learning, supervised fine-tuning, and distillation. And many approaches combine these techniques for greater effect.Â   

åœ¨è¿‡å»å‡ å‘¨ä¸­ï¼Œç ”ç©¶äººå‘˜åˆ†äº«äº†å¤§é‡æ”¹è¿›æ¨ç†çš„æ–°ç­–ç•¥ï¼ŒåŒ…æ‹¬æ‰©å±•æ¨ç†æ—¶é—´è®¡ç®—ã€å¼ºåŒ–å­¦ä¹ ã€ç›‘ç£å¾®è°ƒå’Œæç‚¼ã€‚è®¸å¤šæ–¹æ³•å°†è¿™äº›æŠ€æœ¯ç»“åˆèµ·æ¥ä»¥è·å¾—æ›´å¤§çš„æ•ˆæœã€‚

This article explores recent research advancements in reasoning-optimized LLMs, with a particular focus on inference-time compute scaling that have emerged since the release of DeepSeek R1.  

æœ¬æ–‡æ¢è®¨äº†æ¨ç†ä¼˜åŒ–æ–¹é¢çš„æœ€æ–°ç ”ç©¶è¿›å±•ï¼Œç‰¹åˆ«å…³æ³¨è‡ª DeepSeek R1 å‘å¸ƒä»¥æ¥å‡ºç°çš„æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•ã€‚

![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2Fc1f749e4-4167-4013-b1c9-651c83bf8d3b_1504x756.jpeg)

_The four main categories of implementing reasoning models I explained in [Understanding Reasoning LLMs](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms)_. This article focuses on inference-time-scaling methods.  

_æˆ‘åœ¨[ã€Šç†è§£æ¨ç†ã€‹](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms)ä¸­è§£é‡Šäº†å®ç°æ¨ç†æ¨¡å‹çš„å››ä¸ªä¸»è¦ç±»åˆ«_ã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚

Since most readers are likely already familiar with LLM reasoning models, I will keep the definition short: An LLM-based reasoning model is an LLM designed to solve multi-step problems by generating intermediate steps or structured "thought" processes. Unlike simple question-answering LLMs that just share the final answer, reasoning models either explicitly display their thought process or handle it internally, which helps them to perform better at complex tasks such as puzzles, coding challenges, and mathematical problems.  

ç”±äºå¤§å¤šæ•°è¯»è€…å¯èƒ½å·²ç»ç†Ÿæ‚‰æ¨ç†æ¨¡å‹ï¼Œå› æ­¤æˆ‘å°†ç®€çŸ­åœ°ä»‹ç»å…¶å®šä¹‰ï¼šåŸºäºæ¨ç†çš„æ¨¡å‹æ—¨åœ¨é€šè¿‡ç”Ÿæˆä¸­é—´æ­¥éª¤æˆ–ç»“æ„åŒ–çš„â€œæ€ç»´â€è¿‡ç¨‹æ¥è§£å†³å¤šæ­¥éª¤é—®é¢˜ã€‚ä¸ä»…å…±äº«æœ€ç»ˆç­”æ¡ˆçš„ç®€å•é—®ç­”ä¸åŒï¼Œæ¨ç†æ¨¡å‹è¦ä¹ˆæ˜ç¡®æ˜¾ç¤ºå…¶æ€ç»´è¿‡ç¨‹ï¼Œè¦ä¹ˆåœ¨å†…éƒ¨è¿›è¡Œå¤„ç†ï¼Œè¿™æœ‰åŠ©äºå®ƒä»¬åœ¨å¤æ‚ä»»åŠ¡ï¼ˆä¾‹å¦‚è°œé¢˜ã€ç¼–ç æŒ‘æˆ˜å’Œæ•°å­¦é—®é¢˜ï¼‰ä¸­è¡¨ç°æ›´å¥½ã€‚

![](17417772544031.jpg)

_Side-by-side comparison of a basic LLMâ€™s one-line answer and a reasoning LLMâ€™s explanatory response.  

å¹¶æ’æ¯”è¾ƒåŸºæœ¬LLMçš„å•è¡Œç­”æ¡ˆå’Œæ¨ç†LLMçš„è§£é‡Šæ€§å“åº”ã€‚_

In general, there are two main strategies to improve reasoning: (1) increasing _training_ compute or (2) increasing _inference_ compute, also known as _inference-time scaling_ or _test-time scalin_g. (Inference compute refers to the processing power required to generate model outputs in response to a user query after training.)  

ä¸€èˆ¬æ¥è¯´ï¼Œæœ‰ä¸¤ç§ä¸»è¦ç­–ç•¥å¯ä»¥æé«˜æ¨ç†èƒ½åŠ›ï¼šï¼ˆ1ï¼‰å¢åŠ _è®­ç»ƒ_è®¡ç®—æˆ–ï¼ˆ2ï¼‰å¢åŠ _æ¨ç†_è®¡ç®—ï¼Œä¹Ÿç§°ä¸º_æ¨ç†æ—¶é—´æ‰©å±•_æˆ–_æµ‹è¯•æ—¶é—´æ‰©å±•_ã€‚ï¼ˆæ¨ç†è®¡ç®—æ˜¯æŒ‡è®­ç»ƒåå“åº”ç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆæ¨¡å‹è¾“å‡ºæ‰€éœ€çš„å¤„ç†èƒ½åŠ›ã€‚ï¼‰

![](17417772544059.jpg)

_Accuracy improvements can be achieved through increased training or test-time compute, where test-time compute is synonymous with inference-time compute and inference-time scaling. Source: Annotated figure from https://openai.com/index/learning-to-reason-with-llms/  

å¯ä»¥é€šè¿‡å¢åŠ è®­ç»ƒæˆ–æµ‹è¯•æ—¶é—´è®¡ç®—æ¥æé«˜å‡†ç¡®ç‡ï¼Œå…¶ä¸­æµ‹è¯•æ—¶é—´è®¡ç®—ä¸æ¨ç†æ—¶é—´è®¡ç®—å’Œæ¨ç†æ—¶é—´æ‰©å±•åŒä¹‰ã€‚æ¥æºï¼šå¸¦æ³¨é‡Šçš„å›¾è¡¨æ¥è‡ª https://openai.com/index/learning-to-reason-with-llms/_

Note that the plots shown above make it look like we improve reasoning either via train-time compute OR test-time compute. However, LLMs are usually designed to improve reasoning by **combining** heavy train-time compute (extensive training or fine-tuning, often with reinforcement learning or specialized data) **and** increased test-time compute (allowing the model to "think longer" or perform extra computation during inference).  

è¯·æ³¨æ„ï¼Œä¸Šå›¾æ˜¾ç¤ºæˆ‘ä»¬ä¼¼ä¹é€šè¿‡è®­ç»ƒæ—¶é—´è®¡ç®—æˆ–æµ‹è¯•æ—¶é—´è®¡ç®—æ¥æ”¹è¿›æ¨ç†ã€‚ç„¶è€Œï¼ŒLLMs é€šå¸¸æ—¨åœ¨é€šè¿‡**ç»“åˆ**å¤§é‡è®­ç»ƒæ—¶é—´è®¡ç®—ï¼ˆå¤§é‡è®­ç»ƒæˆ–å¾®è°ƒï¼Œé€šå¸¸ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æˆ–ä¸“é—¨æ•°æ®ï¼‰**å’Œ**å¢åŠ æµ‹è¯•æ—¶é—´è®¡ç®—ï¼ˆå…è®¸æ¨¡å‹â€œæ€è€ƒæ›´é•¿æ—¶é—´â€æˆ–åœ¨æ¨ç†æœŸé—´æ‰§è¡Œé¢å¤–è®¡ç®—ï¼‰æ¥æ”¹è¿›æ¨ç†ã€‚

![](17417772544078.jpg)

The many terms that are used synonymously with inference-time scaling.  

ä¸æ¨ç†æ—¶é—´ç¼©æ”¾åŒä¹‰ä½¿ç”¨çš„è®¸å¤šæœ¯è¯­ã€‚

To understand how reasoning models are being developed and improved, I think it remains useful to look at the different techniques separately. In my previous article, [Understanding Reasoning LLMs](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms), I discussed a finer categorization into four categories, as summarized in the figure below.  

ä¸ºäº†ç†è§£æ¨ç†æ¨¡å‹æ˜¯å¦‚ä½•å‘å±•å’Œæ”¹è¿›çš„ï¼Œæˆ‘è®¤ä¸ºåˆ†åˆ«ç ”ç©¶ä¸åŒçš„æŠ€æœ¯ä»ç„¶å¾ˆæœ‰ç”¨ã€‚åœ¨æˆ‘ä¹‹å‰çš„æ–‡ç« [ã€Šç†è§£æ¨ç†ã€‹](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms)ä¸­ï¼Œæˆ‘è®¨è®ºäº†æ›´ç²¾ç»†çš„å››ç±»åˆ†ç±»ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](17417772544096.jpg)

Methods 2-4 in the figure above typically produce models that generate longer responses because they include intermediate steps and explanations in their outputs. Since inference costs scale with response length (e.g., a response twice as long requires twice the compute), these training approaches are inherently linked to inference scaling. However, in this section on inference-time compute scaling, I focus specifically on techniques that explicitly regulate the number of generated tokens, whether through additional sampling strategies, self-correction mechanisms, or other methods.  

ä¸Šå›¾ä¸­çš„æ–¹æ³• 2-4 é€šå¸¸ä¼šç”Ÿæˆç”Ÿæˆè¾ƒé•¿å“åº”çš„æ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬çš„è¾“å‡ºä¸­åŒ…å«ä¸­é—´æ­¥éª¤å’Œè§£é‡Šã€‚ç”±äºæ¨ç†æˆæœ¬ä¼šéšå“åº”é•¿åº¦è€Œå˜åŒ–ï¼ˆä¾‹å¦‚ï¼Œä¸¤å€é•¿çš„å“åº”éœ€è¦ä¸¤å€çš„è®¡ç®—é‡ï¼‰ï¼Œå› æ­¤è¿™äº›è®­ç»ƒæ–¹æ³•æœ¬è´¨ä¸Šä¸æ¨ç†æ‰©å±•ç›¸å…³ã€‚ç„¶è€Œï¼Œåœ¨æœ¬èŠ‚å…³äºæ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•çš„å†…å®¹ä¸­ï¼Œæˆ‘å°†ç‰¹åˆ«å…³æ³¨æ˜ç¡®è°ƒèŠ‚ç”Ÿæˆæ ‡è®°æ•°é‡çš„æŠ€æœ¯ï¼Œæ— è®ºæ˜¯é€šè¿‡é¢å¤–çš„é‡‡æ ·ç­–ç•¥ã€è‡ªæˆ‘æ ¡æ­£æœºåˆ¶è¿˜æ˜¯å…¶ä»–æ–¹æ³•ã€‚

In this article, I focus on the interesting new research papers and model releases focused on scaling **inference-time compute scaling** that followed **after** the DeepSeek R1 release on January 22nd, 2025. (Originally, I wanted to cover methods from all categories in this article, but due to the excessive length, I decided to release a separate article focused on train-time compute methods in the future.)  

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†é‡ç‚¹ä»‹ç»2025 å¹´ 1 æœˆ 22 æ—¥ DeepSeek R1 å‘å¸ƒ**ä¹‹å**å‘å¸ƒçš„å…³äºæ‰©å±•**æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•**çš„æœ‰è¶£çš„æ–°ç ”ç©¶è®ºæ–‡å’Œæ¨¡å‹å‘å¸ƒã€‚ï¼ˆæœ€åˆï¼Œæˆ‘æƒ³åœ¨æœ¬æ–‡ä¸­æ¶µç›–æ‰€æœ‰ç±»åˆ«çš„æ–¹æ³•ï¼Œä½†ç”±äºç¯‡å¹…è¿‡é•¿ï¼Œæˆ‘å†³å®šåœ¨æœªæ¥å‘å¸ƒä¸€ç¯‡å•ç‹¬çš„æ–‡ç« ï¼Œé‡ç‚¹ä»‹ç»è®­ç»ƒæ—¶é—´è®¡ç®—æ–¹æ³•ã€‚ï¼‰

![](17417772544112.jpg)

_Development process of DeepSeek's reasoning models that I discussed in my previous article, Understanding Reasoning LLMs (https://magazine.sebastianraschka.com/p/understanding-reasoning-llms).  

æˆ‘åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ã€Šç†è§£æ¨ç†ã€‹(https://magazine.sebastianraschka.com/p/understanding-reasoning-llms) ä¸­è®¨è®ºè¿‡ DeepSeek æ¨ç†æ¨¡å‹çš„å¼€å‘è¿‡ç¨‹ã€‚_

Before we look into Inference-time compute scaling methods and the different areas of progress on the reasoning model with a focus on the inference-time compute scaling category, let me at least provide a brief overview of all the different categories.  

åœ¨æˆ‘ä»¬ç ”ç©¶æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•æ–¹æ³•å’Œæ¨ç†æ¨¡å‹çš„ä¸åŒè¿›å±•é¢†åŸŸï¼ˆé‡ç‚¹å…³æ³¨æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•ç±»åˆ«ï¼‰ä¹‹å‰ï¼Œè®©æˆ‘è‡³å°‘å¯¹æ‰€æœ‰ä¸åŒç±»åˆ«è¿›è¡Œç®€è¦æ¦‚è¿°ã€‚

**1\. Inference-time compute scaling  

1\. æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•**

This category includes methods that improve model reasoning capabilities at inference time without training or modifying the underlying model weights. The core idea is to trade off increased computational resources for improved performance, which helps with making even fixed models more capable through techniques such as chain-of-thought reasoning, and various sampling procedures.Â   

æ­¤ç±»åˆ«åŒ…æ‹¬åœ¨æ¨ç†æ—¶æé«˜æ¨¡å‹æ¨ç†èƒ½åŠ›è€Œæ— éœ€è®­ç»ƒæˆ–ä¿®æ”¹åº•å±‚æ¨¡å‹æƒé‡çš„æ–¹æ³•ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ä»¥å¢åŠ è®¡ç®—èµ„æºæ¢å–æé«˜æ€§èƒ½ï¼Œè¿™æœ‰åŠ©äºé€šè¿‡æ€è·¯é“¾æ¨ç†å’Œå„ç§é‡‡æ ·ç¨‹åºç­‰æŠ€æœ¯ä½¿å›ºå®šæ¨¡å‹å˜å¾—æ›´å¼ºå¤§ã€‚

While I categorize inference-time compute scaling separately to focus on methods in this context, it is important to note that this technique can be applied to any LLM. For example, OpenAI developed its o1 model using reinforcement learning, and then additionally leveraged inference-time compute scaling. Interestingly, as I discussed in my previous article on reasoning models ([Understanding Reasoning LLMs](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms)), the DeepSeek R1 paper explicitly mentioned that R1 did not use inference-time scaling. However, they acknowledged that it is something they could easily incorporate into the R1 deployment or application.  

è™½ç„¶æˆ‘å°†æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•å•ç‹¬åˆ†ç±»ä»¥å…³æ³¨æ­¤ä¸Šä¸‹æ–‡ä¸­çš„æ–¹æ³•ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ­¤æŠ€æœ¯å¯åº”ç”¨äºä»»ä½•é¢†åŸŸã€‚ä¾‹å¦‚ï¼ŒOpenAI ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¼€å‘äº†å…¶ o1 æ¨¡å‹ï¼Œç„¶ååˆåˆ©ç”¨äº†æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæ­£å¦‚æˆ‘åœ¨ä¸Šä¸€ç¯‡å…³äºæ¨ç†æ¨¡å‹çš„æ–‡ç« ï¼ˆ[ç†è§£æ¨ç†](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms)ï¼‰ä¸­æ‰€è®¨è®ºçš„é‚£æ ·ï¼ŒDeepSeek R1 è®ºæ–‡æ˜ç¡®æåˆ° R1 æ²¡æœ‰ä½¿ç”¨æ¨ç†æ—¶é—´æ‰©å±•ã€‚ä½†æ˜¯ï¼Œä»–ä»¬æ‰¿è®¤ï¼Œä»–ä»¬å¯ä»¥è½»æ¾åœ°å°†å…¶çº³å…¥ R1 éƒ¨ç½²æˆ–åº”ç”¨ç¨‹åºä¸­ã€‚

**2\. Pure reinforcement learning  

2.çº¯å¼ºåŒ–å­¦ä¹ **

This approach focuses solely on reinforcement learning (RL) to develop or improve reasoning capabilities. It typically involves training models with verifiable reward signals from math or coding domains. While RL allows models to develop more strategic thinking and self-improvement capabilities, it comes with challenges such as reward hacking, instability, and high computational costs.  

è¿™ç§æ–¹æ³•ä»…ä¸“æ³¨äºå¼ºåŒ–å­¦ä¹  (RL) æ¥å¼€å‘æˆ–æé«˜æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šå¸¸æ¶‰åŠä½¿ç”¨æ¥è‡ªæ•°å­¦æˆ–ç¼–ç é¢†åŸŸçš„å¯éªŒè¯å¥–åŠ±ä¿¡å·æ¥è®­ç»ƒæ¨¡å‹ã€‚è™½ç„¶ RL å…è®¸æ¨¡å‹å¼€å‘æ›´å…·æˆ˜ç•¥æ€§çš„æ€ç»´å’Œè‡ªæˆ‘æ”¹è¿›èƒ½åŠ›ï¼Œä½†å®ƒä¹Ÿå¸¦æ¥äº†å¥–åŠ±é»‘å®¢ã€ä¸ç¨³å®šå’Œé«˜è®¡ç®—æˆæœ¬ç­‰æŒ‘æˆ˜ã€‚

**3\. Reinforcement learning and supervised fine-tuning  

3.å¼ºåŒ–å­¦ä¹ å’Œç›‘ç£å¾®è°ƒ**

This hybrid approach combines RL with supervised fine-tuning (SFT) to achieve more stable and generalizable improvements than pure RL. Typically, a model is first trained with SFT on high-quality instruction data and then further refined using RL to optimize specific behaviors**.**  

è¿™ç§æ··åˆæ–¹æ³•å°† RL ä¸ç›‘ç£å¾®è°ƒ (SFT) ç›¸ç»“åˆï¼Œä»¥å®ç°æ¯”çº¯ RL æ›´ç¨³å®šã€æ›´é€šç”¨çš„æ”¹è¿›ã€‚é€šå¸¸ï¼Œé¦–å…ˆä½¿ç”¨ SFT åœ¨é«˜è´¨é‡æŒ‡ä»¤æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œç„¶åä½¿ç”¨ RL è¿›ä¸€æ­¥å®Œå–„ä»¥ä¼˜åŒ–ç‰¹å®šè¡Œä¸º**ã€‚**

**4\. Supervised fine-tuning and model distillation  

4\. ç›‘ç£å¾®è°ƒå’Œæ¨¡å‹è’¸é¦**

This method improves the reasoning capabilities of a model by instruction fine-tuning it on high-quality labeled datasets (SFT). If this high-quality dataset is generated by a larger LLM, then this methodology is also referred to as "knowledge distillation" or just "distillation" in LLM contexts. However, note that this differs slightly from traditional knowledge distillation in deep learning, which typically involves training a smaller model using not only the outputs (labels) but also the logits of a larger teacher model.  

è¯¥æ–¹æ³•é€šè¿‡åœ¨é«˜è´¨é‡æ ‡è®°æ•°æ®é›† (SFT) ä¸Šå¯¹æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒæ¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å¦‚æœæ­¤é«˜è´¨é‡æ•°æ®é›†æ˜¯ç”±æ›´å¤§çš„æ•°æ®é›†ç”Ÿæˆçš„ï¼Œåˆ™è¯¥æ–¹æ³•åœ¨ä¸Šä¸‹æ–‡ä¸­ä¹Ÿç§°ä¸ºâ€œçŸ¥è¯†æç‚¼â€æˆ–ç®€ç§°ä¸ºâ€œæç‚¼â€ã€‚ä½†è¯·æ³¨æ„ï¼Œè¿™ä¸æ·±åº¦å­¦ä¹ ä¸­çš„ä¼ ç»ŸçŸ¥è¯†æç‚¼ç•¥æœ‰ä¸åŒï¼Œåè€…é€šå¸¸æ¶‰åŠä¸ä»…ä½¿ç”¨è¾“å‡ºï¼ˆæ ‡ç­¾ï¼‰è€Œä¸”è¿˜ä½¿ç”¨è¾ƒå¤§æ•™å¸ˆæ¨¡å‹çš„é€»è¾‘æ¥è®­ç»ƒè¾ƒå°çš„æ¨¡å‹ã€‚

The previous section already briefly summarized inference-time compute scaling. Before discussing the recent research in this category, let me describe the inference-time scaling in a bit more detail.  

ä¸Šä¸€èŠ‚å·²ç»ç®€è¦æ€»ç»“äº†æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•ã€‚åœ¨è®¨è®ºæ­¤ç±»åˆ«çš„æœ€æ–°ç ”ç©¶ä¹‹å‰ï¼Œè®©æˆ‘æ›´è¯¦ç»†åœ°æè¿°ä¸€ä¸‹æ¨ç†æ—¶é—´æ‰©å±•ã€‚

Inference-time scaling improves an LLM's reasoning by increasing computational resources ("compute") during inference. The idea why this can improve reasoning can be given with a simple analogy: humans give better responses when given more time to think, and similarly, LLMs can improve with techniques that encourage more "thought" during generation.  

æ¨ç†æ—¶é—´æ‰©å±•é€šè¿‡å¢åŠ æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—èµ„æºï¼ˆâ€œè®¡ç®—â€ï¼‰æ¥æ”¹å–„æ¨ç†èƒ½åŠ›ã€‚è¿™å¯ä»¥æ”¹å–„æ¨ç†èƒ½åŠ›çš„åŸå› å¯ä»¥ç”¨ä¸€ä¸ªç®€å•çš„ç±»æ¯”æ¥è¯´æ˜ï¼šå½“ç»™äºˆäººç±»æ›´å¤šæ—¶é—´æ€è€ƒæ—¶ï¼Œä»–ä»¬ä¼šåšå‡ºæ›´å¥½çš„ååº”ï¼ŒåŒæ ·ï¼Œé€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é¼“åŠ±æ›´å¤šâ€œæ€è€ƒâ€çš„æŠ€æœ¯ï¼Œä¹Ÿå¯ä»¥æ”¹å–„æ¨ç†èƒ½åŠ›ã€‚

One approach here is prompt engineering, such as chain-of-thought (CoT) prompting, where phrases like "think step by step" guide the model to generate intermediate reasoning steps. This improves accuracy on complex problems but is unnecessary for simple factual queries. Since CoT prompts generate more tokens, they effectively make inference more expensive.  

ä¸€ç§æ–¹æ³•æ˜¯æç¤ºå·¥ç¨‹ï¼Œä¾‹å¦‚æ€è·¯é“¾ (CoT) æç¤ºï¼Œå…¶ä¸­â€œé€æ­¥æ€è€ƒâ€ç­‰çŸ­è¯­ä¼šå¼•å¯¼æ¨¡å‹ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ã€‚è¿™å¯ä»¥æé«˜å¤æ‚é—®é¢˜çš„å‡†ç¡®æ€§ï¼Œä½†å¯¹äºç®€å•çš„äº‹å®æŸ¥è¯¢è€Œè¨€åˆ™æ²¡æœ‰å¿…è¦ã€‚ç”±äº CoT æç¤ºä¼šç”Ÿæˆæ›´å¤šæ ‡è®°ï¼Œå› æ­¤å®ƒä»¬å®é™…ä¸Šä½¿æ¨ç†æ›´åŠ æ˜‚è´µã€‚

![](17417772544130.jpg)

_An example of classic CoT prompting from the 2022 Large Language Models are Zero-Shot Reasoners paper (https://arxiv.org/abs/2205.11916).  

2022 å¹´å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ç»å…¸ CoT æç¤ºçš„ä¸€ä¸ªç¤ºä¾‹æ˜¯é›¶æ ·æœ¬æ¨ç†å™¨è®ºæ–‡ (https://arxiv.org/abs/2205.11916)ã€‚_

Another method involves voting and search strategies, such as majority voting or beam search, which refine responses by selecting the best output.  

å¦ä¸€ç§æ–¹æ³•æ¶‰åŠæŠ•ç¥¨å’Œæœç´¢ç­–ç•¥ï¼Œä¾‹å¦‚å¤šæ•°æŠ•ç¥¨æˆ–é›†æŸæœç´¢ï¼Œé€šè¿‡é€‰æ‹©æœ€ä½³è¾“å‡ºæ¥æ”¹è¿›å“åº”ã€‚

![](17417772544156.jpg)

_Different search-based methods rely on a process-reward-based model to select the best answer. Annotated figure from the LLM Test-Time Compute paper, https://arxiv.org/abs/2408.03314  

ä¸åŒçš„åŸºäºæœç´¢çš„æ–¹æ³•ä¾èµ–äºåŸºäºè¿‡ç¨‹å¥–åŠ±çš„æ¨¡å‹æ¥é€‰æ‹©æœ€ä½³ç­”æ¡ˆã€‚æ³¨é‡Šå›¾æ¥è‡ª Test-Time Compute è®ºæ–‡ï¼Œhttps://arxiv.org/abs/2408.03314_

The remainder of this article will be focused on the recent research advances in the inference-time scaling category for improving reasoning capabilities of LLMs. Let me start with a more detailed discussion of a paper that serves as an example of inference-time scaling.  

æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†å°†é‡ç‚¹ä»‹ç»æ¨ç†æ—¶é—´æ‰©å±•ç±»åˆ«çš„æœ€æ–°ç ”ç©¶è¿›å±•ï¼Œä»¥æé«˜æ¨ç†èƒ½åŠ›ã€‚é¦–å…ˆï¼Œæˆ‘å°†æ›´è¯¦ç»†åœ°è®¨è®ºä¸€ç¯‡ä½œä¸ºæ¨ç†æ—¶é—´æ‰©å±•ç¤ºä¾‹çš„è®ºæ–‡ã€‚

So, one of the interesting recent research papers in this category is [s1: Simple Test-Time Scaling](https://arxiv.org/abs/2501.19393) (31 Jan, 2025), which introduces so-called "wait" tokens, which can be considered as a more modern version of the aforementioned "think step by step" prompt modification.  

å› æ­¤ï¼Œè¯¥ç±»åˆ«ä¸­æœ€è¿‘ä¸€ç¯‡æœ‰è¶£çš„ç ”ç©¶è®ºæ–‡æ˜¯[s1ï¼šç®€å•æµ‹è¯•æ—¶é—´ç¼©æ”¾](https://arxiv.org/abs/2501.19393)ï¼ˆ2025 å¹´ 1 æœˆ 31 æ—¥ï¼‰ï¼Œå®ƒå¼•å…¥äº†æ‰€è°“çš„â€œç­‰å¾…â€æ ‡è®°ï¼Œå¯ä»¥å°†å…¶è§†ä¸ºå‰é¢æåˆ°çš„â€œé€æ­¥æ€è€ƒâ€æç¤ºä¿®æ”¹çš„æ›´ç°ä»£ç‰ˆæœ¬ã€‚

Note that this involves supervised finetuning (SFT) to generate the initial model, so it's not a pure inference-time scaling approach. However, the end goal is actively controlling the reasoning behavior through inference-time scaling; hence, I considered this paper for the "1. Inference-time compute scaling" category.  

è¯·æ³¨æ„ï¼Œè¿™æ¶‰åŠç›‘ç£å¾®è°ƒ (SFT) æ¥ç”Ÿæˆåˆå§‹æ¨¡å‹ï¼Œå› æ­¤å®ƒä¸æ˜¯çº¯ç²¹çš„æ¨ç†æ—¶é—´æ‰©å±•æ–¹æ³•ã€‚ä½†æ˜¯ï¼Œæœ€ç»ˆç›®æ ‡æ˜¯é€šè¿‡æ¨ç†æ—¶é—´æ‰©å±•ä¸»åŠ¨æ§åˆ¶æ¨ç†è¡Œä¸ºï¼›å› æ­¤ï¼Œæˆ‘è®¤ä¸ºè¿™ç¯‡è®ºæ–‡å±äºâ€œ1. æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•â€ç±»åˆ«ã€‚

In short, their approach is twofold:  

ç®€è€Œè¨€ä¹‹ï¼Œä»–ä»¬çš„æ–¹æ³•æœ‰ä¸¤ä¸ªï¼š

1.  Create a curated SFT dataset with 1k training examples that include reasoning traces.  
    
    åˆ›å»ºåŒ…å« 1k ä¸ªåŒ…å«æ¨ç†ç—•è¿¹çš„è®­ç»ƒç¤ºä¾‹çš„ç²¾é€‰ SFT æ•°æ®é›†ã€‚
    
2.  Control the length of responses by:  
    
    é€šè¿‡ä»¥ä¸‹æ–¹å¼æ§åˆ¶å“åº”çš„é•¿åº¦ï¼š
    
3.  a) Appending "Wait" tokens to get the LLM to generate longer responses, self-verify, and correct itself, or  
    
    a) é™„åŠ â€œç­‰å¾…â€ä»¤ç‰Œï¼Œè®© LLM ç”Ÿæˆæ›´é•¿çš„å“åº”ã€è¿›è¡Œè‡ªæˆ‘éªŒè¯å’Œè‡ªæˆ‘çº æ­£ï¼Œæˆ–è€…
    
4.  b) Stopping generation by adding an end-of-thinking token delimiter ("Final Answer:"). They call this length control "budget forcing."  
    
    b) é€šè¿‡æ·»åŠ æ€è€ƒç»“æŸæ ‡è®°åˆ†éš”ç¬¦æ¥åœæ­¢ç”Ÿæˆï¼ˆâ€œæœ€ç»ˆç­”æ¡ˆï¼šâ€ï¼‰ã€‚ä»–ä»¬ç§°è¿™ç§é•¿åº¦æ§åˆ¶ä¸ºâ€œé¢„ç®—å¼ºåˆ¶â€ã€‚
    

![](17417772544182.jpg)

_Illustration of "wait" token insertion to control the length of the output. Annotated figure from https://arxiv.org/abs/2501.19393.  

æ’å…¥â€œwaitâ€æ ‡è®°ä»¥æ§åˆ¶è¾“å‡ºé•¿åº¦çš„å›¾ç¤ºã€‚æ³¨é‡Šå›¾æ¥è‡ª https://arxiv.org/abs/2501.19393ã€‚_

Budget forcing can be seen as a sequential inference scaling technique because it still generates one token at a time (but just more of it). In contrast, we have parallel techniques like majority voting, which aggregate multiple independent completions.  

é¢„ç®—å¼ºåˆ¶å¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§é¡ºåºæ¨ç†æ‰©å±•æŠ€æœ¯ï¼Œå› ä¸ºå®ƒä»ç„¶ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ª tokenï¼ˆä½†æ•°é‡æ›´å¤šï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬æœ‰å¤šæ•°è¡¨å†³ç­‰å¹¶è¡ŒæŠ€æœ¯ï¼Œå¯ä»¥èšåˆå¤šä¸ªç‹¬ç«‹çš„å®Œæˆã€‚

![](17417772544215.jpg)

_Correlation between response accuracy and length. Annotated figure from https://arxiv.org/abs/2501.19393.  

å“åº”å‡†ç¡®åº¦ä¸é•¿åº¦ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æ³¨é‡Šå›¾æ¥è‡ª https://arxiv.org/abs/2501.19393ã€‚_

They found their budget-forcing method more effective than other inference-scaling techniques I've discussed, like majority voting. If there's something to criticize or improve, I would've liked to see results for more sophisticated parallel inference-scaling methods, like beam search, lookahead search, or the best compute-optimal search described in Google's _[Scaling LLM Test-Time Compute Optimally Can Be More Effective Than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)_ paper last year. Or even a simple comparison with a classic sequential method like chain-of-thought prompting ("Think step by step").  

ä»–ä»¬å‘ç°ä»–ä»¬çš„å¼ºåˆ¶é¢„ç®—æ–¹æ³•æ¯”æˆ‘è®¨è®ºè¿‡çš„å…¶ä»–æ¨ç†æ‰©å±•æŠ€æœ¯ï¼ˆå¦‚å¤šæ•°è¡¨å†³ï¼‰æ›´æœ‰æ•ˆã€‚å¦‚æœæœ‰ä»€ä¹ˆéœ€è¦æ‰¹è¯„æˆ–æ”¹è¿›çš„åœ°æ–¹ï¼Œæˆ‘å¸Œæœ›çœ‹åˆ°æ›´å¤æ‚çš„å¹¶è¡Œæ¨ç†æ‰©å±•æ–¹æ³•çš„ç»“æœï¼Œå¦‚æŸæœç´¢ã€å‰ç»æœç´¢æˆ– Googleå»å¹´çš„è®ºæ–‡_[ã€Šæ‰©å±• LLM æµ‹è¯•æ—¶é—´è®¡ç®—ä¼˜åŒ–æ¯”æ‰©å±•æ¨¡å‹å‚æ•°æ›´æœ‰æ•ˆ](https://arxiv.org/abs/2408.03314)_ã€‹ä¸­æè¿°çš„æœ€ä½³è®¡ç®—ä¼˜åŒ–æœç´¢ã€‚æˆ–è€…ç”šè‡³ä¸ç»å…¸çš„é¡ºåºæ–¹æ³•ï¼ˆå¦‚æ€ç»´é“¾æç¤ºï¼ˆâ€œé€æ­¥æ€è€ƒâ€ï¼‰ï¼‰è¿›è¡Œç®€å•çš„æ¯”è¾ƒã€‚

Anyway, it's a really interesting paper and approach!  

æ— è®ºå¦‚ä½•ï¼Œè¿™æ˜¯ä¸€ç¯‡éå¸¸æœ‰è¶£çš„è®ºæ–‡å’Œæ–¹æ³•ï¼

**PS: Why "Wait" tokens?** My guess is the researchers were inspired by the "Aha moment" figure in the DeepSeek-R1 paper, where researchers saw LLMs coming up with something like "_Wait, wait. Wait. That's an aha moment I can flag here._" which showed that pure reinforcement learning can induce reasoning behavior in LLMs.  

**é™„è¨€ï¼šä¸ºä»€ä¹ˆæ˜¯â€œç­‰å¾…â€æ ‡è®°ï¼Ÿ**æˆ‘çŒœç ”ç©¶äººå‘˜å—åˆ°äº† DeepSeek-R1 è®ºæ–‡ä¸­â€œé¡¿æ‚Ÿæ—¶åˆ»â€å›¾çš„å¯å‘ï¼Œç ”ç©¶äººå‘˜çœ‹åˆ° LLMs å‡ºç°äº†ç±»ä¼¼â€œ_ç­‰ä¸€ä¸‹ï¼Œç­‰ä¸€ä¸‹ã€‚ç­‰ä¸€ä¸‹ã€‚è¿™æ˜¯ä¸€ä¸ªé¡¿æ‚Ÿæ—¶åˆ»ï¼Œæˆ‘å¯ä»¥åœ¨è¿™é‡Œæ ‡è®°å‡ºæ¥ã€‚_ â€ çš„ä¸œè¥¿ã€‚è¿™è¡¨æ˜çº¯å¼ºåŒ–å­¦ä¹ å¯ä»¥è¯±å¯¼æ¨ç†è¡Œä¸ºã€‚

Interestingly, they also tried other tokens like "_Hmm_" but found that "_Wait_" performed slightly better.  

æœ‰è¶£çš„æ˜¯ï¼Œä»–ä»¬è¿˜å°è¯•äº†å…¶ä»–æ ‡è®°ï¼Œä¾‹å¦‚â€œ_å—¯_â€ï¼Œä½†å‘ç°â€œ_ç­‰å¾…_â€çš„è¡¨ç°ç•¥å¥½ä¸€äº›ã€‚

![](17417772544239.jpg)

"_Wait"_ vs "_Hmm"_ tokens. _Annotated figure from https://arxiv.org/abs/2501.19393._  

â€œ_ç­‰å¾…â€_ä¸â€œ_å—¯â€_æ ‡è®°ã€‚_æ³¨é‡Šå›¾æ¥è‡ª https://arxiv.org/abs/2501.19393ã€‚_

Since it's been a very active month on the reasoning model research front, I need to keep the summaries of other papers relatively brief to manage a reasonable length for this article. Hence, below are brief summaries of other interesting research articles related to inference-time compute scaling, sorted in ascending order by publication date.  

ç”±äºæœ¬æœˆåœ¨æ¨ç†æ¨¡å‹ç ”ç©¶æ–¹é¢éå¸¸æ´»è·ƒï¼Œæˆ‘éœ€è¦å¯¹å…¶ä»–è®ºæ–‡çš„æ€»ç»“ä¿æŒç›¸å¯¹ç®€çŸ­ï¼Œä»¥æ§åˆ¶æœ¬æ–‡çš„åˆç†é•¿åº¦ã€‚å› æ­¤ï¼Œä¸‹é¢æ˜¯ä¸æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•ç›¸å…³çš„å…¶ä»–æœ‰è¶£ç ”ç©¶æ–‡ç« çš„ç®€è¦æ€»ç»“ï¼ŒæŒ‰å‡ºç‰ˆæ—¥æœŸå‡åºæ’åˆ—ã€‚

As mentioned earlier, not all of these articles fall neatly into the inference-time compute scaling category, as some of them also involve specific training. However, these papers have in common that controlling inference-time compute is a specific mechanism of action. (Many distilled or SFT methods that I will cover in upcoming articles will lead to longer responses, which can be seen as a form of inference-time compute scaling. However, they do not actively control the length during inference, which makes these methods different from those covered here.)  

å¦‚å‰æ‰€è¿°ï¼Œå¹¶éæ‰€æœ‰è¿™äº›æ–‡ç« éƒ½å®Œå…¨å±äºæ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•ç±»åˆ«ï¼Œå› ä¸ºå…¶ä¸­ä¸€äº›æ–‡ç« è¿˜æ¶‰åŠç‰¹å®šè®­ç»ƒã€‚ä½†æ˜¯ï¼Œè¿™äº›è®ºæ–‡çš„å…±åŒç‚¹æ˜¯æ§åˆ¶æ¨ç†æ—¶é—´è®¡ç®—æ˜¯ä¸€ç§ç‰¹å®šçš„ä½œç”¨æœºåˆ¶ã€‚ï¼ˆæˆ‘å°†åœ¨åç»­æ–‡ç« ä¸­ä»‹ç»çš„è®¸å¤šæç‚¼æˆ– SFT æ–¹æ³•å°†å¯¼è‡´æ›´é•¿çš„å“åº”ï¼Œè¿™å¯ä»¥çœ‹ä½œæ˜¯æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•çš„ä¸€ç§å½¢å¼ã€‚ä½†æ˜¯ï¼Œå®ƒä»¬ä¸ä¼šåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸»åŠ¨æ§åˆ¶é•¿åº¦ï¼Œè¿™ä½¿å¾—è¿™äº›æ–¹æ³•ä¸æ­¤å¤„ä»‹ç»çš„æ–¹æ³•ä¸åŒã€‚ï¼‰

**ğŸ“„ 22 Jan,** _**Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback**_**, [https://arxiv.org/abs/2501.12895](https://arxiv.org/abs/2501.12895)**  

**ğŸ“„ 1 æœˆ 22 æ—¥ï¼Œ**_**æµ‹è¯•æ—¶é—´åå¥½ä¼˜åŒ–ï¼šé€šè¿‡è¿­ä»£æ–‡æœ¬åé¦ˆè¿›è¡ŒåŠ¨æ€å¯¹é½**_**ï¼Œ [https://arxiv.org/abs/2501.12895](https://arxiv.org/abs/2501.12895)**

Test-time Preference Optimization (TPO) is an iterative process that aligns LLM outputs with human preferences during inference (this is without altering its underlying model weights). In each iteration, the model:  

æµ‹è¯•æ—¶åå¥½ä¼˜åŒ– (TPO) æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œå¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†è¾“å‡ºä¸äººç±»åå¥½ä¿æŒä¸€è‡´ï¼ˆè¿™ä¸ä¼šæ”¹å˜å…¶åº•å±‚æ¨¡å‹æƒé‡ï¼‰ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæ¨¡å‹ï¼š

1.  Generates multiple responses for a given prompt.  
    
    é’ˆå¯¹ç»™å®šçš„æç¤ºç”Ÿæˆå¤šä¸ªå“åº”ã€‚
    
2.  Score the responses with a reward model to select the highest- and lowest-scoring ones as "chosen" and "rejected" responses  
    
    ä½¿ç”¨å¥–åŠ±æ¨¡å‹å¯¹ç­”æ¡ˆè¿›è¡Œè¯„åˆ†ï¼Œä»¥é€‰æ‹©å¾—åˆ†æœ€é«˜å’Œæœ€ä½çš„ç­”æ¡ˆä½œä¸ºâ€œé€‰å®šâ€å’Œâ€œæ‹’ç»â€çš„ç­”æ¡ˆ
    
3.  Prompt the model to compare and critique the "chosen" and "rejected" responses  
    
    æç¤ºæ¨¡å‹æ¯”è¾ƒå’Œæ‰¹è¯„â€œé€‰æ‹©â€å’Œâ€œæ‹’ç»â€çš„ç­”æ¡ˆ
    
4.  Refine the output by converting the critiques into textual suggestions to update the original model responses  
    
    é€šè¿‡å°†æ‰¹è¯„è½¬æ¢ä¸ºæ–‡æœ¬å»ºè®®æ¥ä¼˜åŒ–è¾“å‡ºï¼Œä»¥æ›´æ–°åŸå§‹æ¨¡å‹å“åº”
    

By doing steps 1-4 iteratively, the model refines its original responses.  

é€šè¿‡è¿­ä»£æ‰§è¡Œæ­¥éª¤ 1-4ï¼Œæ¨¡å‹å¯ä»¥æ”¹è¿›å…¶åŸå§‹å“åº”ã€‚

![](17417772544264.jpg)

_Annotated figure from "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback", https://arxiv.org/abs/2501.12895  

æ³¨é‡Šå›¾æ¥è‡ªâ€œæµ‹è¯•æ—¶é—´åå¥½ä¼˜åŒ–ï¼šé€šè¿‡è¿­ä»£æ–‡æœ¬åé¦ˆè¿›è¡ŒåŠ¨æ€å¯¹é½â€ï¼Œhttps://arxiv.org/abs/2501.12895_

**ğŸ“„ 30 Jan,** _**Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs**_**, [https://arxiv.org/abs/2501.18585](https://arxiv.org/abs/2501.18585)**  

**ğŸ“„ 1 æœˆ 30 æ—¥ï¼Œ**_**æ€ç»ªä¸‡åƒï¼šè®º o1-Like LLMs çš„æ·±æ€ç†Ÿè™‘**_**ï¼Œ [https://arxiv.org/abs/2501.18585](https://arxiv.org/abs/2501.18585)**

The researchers explore a phenomenon called "underthinking", where reasoning models frequently switch between reasoning paths instead of fully focusing on exploring promising ones, which lowers the problem solving accuracy.  

ç ”ç©¶äººå‘˜æ¢ç´¢äº†ä¸€ç§åä¸ºâ€œæ€è€ƒä¸è¶³â€çš„ç°è±¡ï¼Œå³æ¨ç†æ¨¡å‹é¢‘ç¹åœ¨æ¨ç†è·¯å¾„ä¹‹é—´åˆ‡æ¢ï¼Œè€Œä¸æ˜¯å®Œå…¨ä¸“æ³¨äºæ¢ç´¢æœ‰å¸Œæœ›çš„è·¯å¾„ï¼Œä»è€Œé™ä½äº†è§£å†³é—®é¢˜çš„å‡†ç¡®æ€§ã€‚

To address this "underthinking" issue, they introduce a method called the Thought Switching Penalty (TIP), which modifies the logits of thought-switching tokens to discourage premature reasoning path transitions.Â   

ä¸ºäº†è§£å†³è¿™ä¸ªâ€œæ€è€ƒä¸è¶³â€çš„é—®é¢˜ï¼Œä»–ä»¬å¼•å…¥äº†ä¸€ç§ç§°ä¸ºæ€ç»´è½¬æ¢æƒ©ç½šï¼ˆTIPï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¿®æ”¹äº†æ€ç»´è½¬æ¢æ ‡è®°çš„é€»è¾‘ï¼Œä»¥é˜»æ­¢è¿‡æ—©çš„æ¨ç†è·¯å¾„è½¬æ¢ã€‚

Their approach does not require model fine-tuning and empirically improves accuracy across multiple challenging test sets.  

ä»–ä»¬çš„æ–¹æ³•ä¸éœ€è¦æ¨¡å‹å¾®è°ƒï¼Œå¹¶ä¸”é€šè¿‡ç»éªŒæé«˜äº†å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•é›†çš„å‡†ç¡®æ€§ã€‚

![](17417772544291.jpg)

_Annotated figure from "Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs", https://arxiv.org/abs/2501.18585  

æ³¨é‡Šå›¾æ¥è‡ªã€Šæ€ç»ªä¸‡åƒï¼šè®º o1 ç±» LLMs çš„æ·±å±‚æ€è€ƒã€‹ï¼Œhttps://arxiv.org/abs/2501.18585_

**ğŸ“„ 31 Jan,** _**Trading Inference-Time Compute for Adversarial Robustness**_**, [https://arxiv.org/abs/2501.18841](https://arxiv.org/abs/2501.18841)**  

**ğŸ“„ 1 æœˆ 31 æ—¥ï¼Œ**_**ç”¨æ¨ç†æ—¶é—´è®¡ç®—æ¢å–å¯¹æŠ—é²æ£’æ€§**_**ï¼Œ [https://arxiv.org/abs/2501.18841](https://arxiv.org/abs/2501.18841)**

Increasing inference-time compute improves the adversarial robustness of reasoning LLMs in many cases in terms of reducing the rate of successful attacks. Unlike adversarial training, this method does not need any special training or require prior knowledge of specific attack types.Â   

åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œå¢åŠ æ¨ç†æ—¶é—´è®¡ç®—å¯ä»¥æé«˜æ¨ç†çš„å¯¹æŠ—é²æ£’æ€§ï¼Œä»è€Œé™ä½æˆåŠŸæ”»å‡»çš„æ¦‚ç‡ã€‚ä¸å¯¹æŠ—è®­ç»ƒä¸åŒï¼Œè¿™ç§æ–¹æ³•ä¸éœ€è¦ä»»ä½•ç‰¹æ®Šè®­ç»ƒï¼Œä¹Ÿä¸éœ€è¦äº‹å…ˆäº†è§£ç‰¹å®šçš„æ”»å‡»ç±»å‹ã€‚

However, there are some important exceptions. For example, the improvements in settings involving policy ambiguities or loophole exploitation are limited. Additionally, the reasoning-improved robustness increases can be reduced by new attack strategies such as "Think Less" and "Nerd Sniping".Â   

ç„¶è€Œï¼Œä¹Ÿå­˜åœ¨ä¸€äº›é‡è¦çš„ä¾‹å¤–ã€‚ä¾‹å¦‚ï¼Œåœ¨æ¶‰åŠç­–ç•¥æ¨¡ç³Šæ€§æˆ–æ¼æ´åˆ©ç”¨çš„è®¾ç½®ä¸­ï¼Œæ”¹è¿›çš„æ•ˆæœæœ‰é™ã€‚æ­¤å¤–ï¼Œæ¨ç†æ”¹è¿›å¸¦æ¥çš„é²æ£’æ€§æå‡å¯èƒ½ä¼šè¢«â€œThink Lessâ€å’Œâ€œNerd Snipingâ€ç­‰æ–°æ”»å‡»ç­–ç•¥æ‰€å‰Šå¼±ã€‚

So, while these findings suggest that scaling inference-time compute can improve LLM safety, this alone is not a complete solution to adversarial robustness.  

å› æ­¤ï¼Œè™½ç„¶è¿™äº›å‘ç°è¡¨æ˜æ‰©å±•æ¨ç†æ—¶é—´è®¡ç®—å¯ä»¥æé«˜å®‰å…¨æ€§ï¼Œä½†ä»…é è¿™ä¸€ç‚¹å¹¶ä¸èƒ½å®Œå…¨è§£å†³å¯¹æŠ—é²æ£’æ€§é—®é¢˜ã€‚

![](17417772544320.jpg)

_Annotated figure from "Trading Inference-Time Compute for Adversarial Robustness", https://arxiv.org/abs/2501.18841  

æ³¨é‡Šå›¾æ¥è‡ªâ€œTrading Inference-Time Compute for Adversarial Robustnessâ€ï¼Œhttps://arxiv.org/abs/2501.18841_

**ğŸ“„ 4 Feb, CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning, [https://arxiv.org/abs/2502.02390](https://arxiv.org/abs/2502.02390)  

ğŸ“„ 2 æœˆ 4 æ—¥ï¼ŒCoATï¼šç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†çš„å…³è”æ€ç»´é“¾æ¡†æ¶ï¼Œ [https://arxiv.org/abs/2502.02390](https://arxiv.org/abs/2502.02390)**

The researchers combine classic Monte Carlo Tree Search inference-time scaling with an "associative memory" that serves as the LLM's knowledge base during the exploration of reasoning pathways. Using this so-called associative memory, it's easier for the LLM to consider earlier reasoning pathways and use dynamically involving information during the response generation.  

ç ”ç©¶äººå‘˜å°†ç»å…¸çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢æ¨ç†æ—¶é—´æ‰©å±•ä¸â€œè”æƒ³è®°å¿†â€ç›¸ç»“åˆï¼Œåè€…åœ¨æ¢ç´¢æ¨ç†è·¯å¾„æ—¶å……å½“çŸ¥è¯†åº“ã€‚ä½¿ç”¨è¿™ç§æ‰€è°“çš„è”æƒ³è®°å¿†ï¼ŒLLM å¯ä»¥æ›´è½»æ¾åœ°è€ƒè™‘æ—©æœŸçš„æ¨ç†è·¯å¾„ï¼Œå¹¶åœ¨å“åº”ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€ä½¿ç”¨ç›¸å…³ä¿¡æ¯ã€‚

![](17417772544350.jpg)

_Annotated figure from "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning", https://arxiv.org/abs/2502.02390  

æ³¨é‡Šå›¾æ¥è‡ªâ€œCoATï¼šç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†çš„å…³è”æ€ç»´é“¾æ¡†æ¶â€ï¼Œhttps://arxiv.org/abs/2502.02390_

**ğŸ“„ 6 Feb, Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models, https://arxiv.org/abs/2502.0440  

ğŸ“„ 2 æœˆ 6 æ—¥ï¼Œé€€ä¸€æ­¥ï¼Œå‘å‰è¿ˆè¿›ï¼šè‡ªæˆ‘å›æº¯ä»¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œhttps://arxiv.org/abs/2502.0440**

This paper proposes a self-backtracking mechanism that allows LLMs to improve their reasoning by learning when and where to backtrack during training and inference. While training involves teaching the model to recognize and correct suboptimal reasoning paths using a <backtrack> token, the key contribution is an inference-time tree-based search that uses this learned backtracking ability to explore alternative solutions.Â   

æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªå›æº¯æœºåˆ¶ï¼Œé€šè¿‡å­¦ä¹ åœ¨è®­ç»ƒå’Œæ¨ç†æœŸé—´ä½•æ—¶ä½•åœ°å›æº¯ï¼Œè¯¥æœºåˆ¶å…è®¸æ¨¡å‹æ”¹è¿›æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶è®­ç»ƒæ¶‰åŠä½¿ç”¨ <backtrack> æ ‡è®°æ•™æ¨¡å‹è¯†åˆ«å’Œçº æ­£æ¬¡ä¼˜æ¨ç†è·¯å¾„ï¼Œä½†å…³é”®è´¡çŒ®åœ¨äºæ¨ç†æ—¶é—´åŸºäºæ ‘çš„æœç´¢ï¼Œè¯¥æœç´¢ä½¿ç”¨è¿™ç§å­¦ä¹ åˆ°çš„å›æº¯èƒ½åŠ›æ¥æ¢ç´¢æ›¿ä»£è§£å†³æ–¹æ¡ˆã€‚

What's unique is that this exploration does not require without relying on external reward models (unlike the search-based methods that use a process-reward-based model that I mentioned at the beginning of the "1. Inference-time compute scaling methods" section in this article).  

ç‹¬ç‰¹ä¹‹å¤„åœ¨äºï¼Œè¿™ç§æ¢ç´¢ä¸éœ€è¦ä¾èµ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼ˆä¸æˆ‘åœ¨æœ¬æ–‡â€œ1. æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•æ–¹æ³•â€éƒ¨åˆ†å¼€å¤´æåˆ°çš„ä½¿ç”¨åŸºäºè¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„åŸºäºæœç´¢çš„æ–¹æ³•ä¸åŒï¼‰ã€‚

![](17417772544382.jpg)

_Annotated figure from "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models", https://arxiv.org/abs/2502.04404  

æ³¨é‡Šå›¾æ¥è‡ªã€Šé€€ä¸€æ­¥ï¼Œè·ƒè¿›ä¸€æ­¥ï¼šè‡ªæˆ‘å›æº¯ä»¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„æ¨ç†ã€‹ï¼Œhttps://arxiv.org/abs/2502.04404_

I added this paper here as it's heavily focused on the proposed backtracking inference-time scaling method, which improves reasoning by dynamically adjusting search depth and breadth rather than fundamentally altering the training paradigm (although, the training with <backtrack> tokens is required).Â   

æˆ‘åœ¨è¿™é‡Œæ·»åŠ äº†è¿™ç¯‡è®ºæ–‡ï¼Œå› ä¸ºå®ƒä¸»è¦å…³æ³¨æå‡ºçš„å›æº¯æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´æœç´¢æ·±åº¦å’Œå¹¿åº¦è€Œä¸æ˜¯ä»æ ¹æœ¬ä¸Šæ”¹å˜è®­ç»ƒèŒƒå¼æ¥æ”¹è¿›æ¨ç†ï¼ˆå°½ç®¡éœ€è¦ä½¿ç”¨ <backtrack> ä»¤ç‰Œè¿›è¡Œè®­ç»ƒï¼‰ã€‚

**ğŸ“„ 7 Feb, Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach, [https://arxiv.org/abs/2502.05171](https://arxiv.org/abs/2502.05171)  

ğŸ“„ 2 æœˆ 7 æ—¥ï¼Œä½¿ç”¨æ½œåœ¨æ¨ç†æ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—ï¼šä¸€ç§å¾ªç¯æ·±åº¦æ–¹æ³•ï¼Œ [https://arxiv.org/abs/2502.05171](https://arxiv.org/abs/2502.05171)**

Instead of improving reasoning by generating more tokens, the researchers propose a model that scales inference-time compute by iterating over a recurrent depth block in latent space. This block functions like a hidden state in RNNs, which allows the model to refine its reasoning without requiring longer token outputs.Â   

ç ”ç©¶äººå‘˜å¹¶æ²¡æœ‰é€šè¿‡ç”Ÿæˆæ›´å¤š token æ¥æ”¹è¿›æ¨ç†ï¼Œè€Œæ˜¯æå‡ºäº†ä¸€ä¸ªæ¨¡å‹ï¼Œé€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿­ä»£å¾ªç¯æ·±åº¦å—æ¥æ‰©å±•æ¨ç†æ—¶é—´è®¡ç®—ã€‚è¯¥å—çš„åŠŸèƒ½ç±»ä¼¼äº RNN ä¸­çš„éšè—çŠ¶æ€ï¼Œå®ƒå…è®¸æ¨¡å‹æ”¹è¿›å…¶æ¨ç†ï¼Œè€Œæ— éœ€æ›´é•¿çš„ token è¾“å‡ºã€‚

However, a key drawback is the lack of explicit reasoning steps, which are (in my opinion) useful for human interpretability and a major advantage of chain-of-thought methods.  

ç„¶è€Œï¼Œä¸€ä¸ªå…³é”®çš„ç¼ºç‚¹æ˜¯ç¼ºä¹æ˜ç¡®çš„æ¨ç†æ­¥éª¤ï¼Œè¿™ï¼ˆåœ¨æˆ‘çœ‹æ¥ï¼‰å¯¹äºäººç±»çš„å¯è§£é‡Šæ€§å¾ˆæœ‰ç”¨ï¼Œå¹¶ä¸”æ˜¯æ€è·¯é“¾æ–¹æ³•çš„ä¸€å¤§ä¼˜åŠ¿ã€‚

![](17417772544412.jpg)

Annotated figure from "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach", https://arxiv.org/abs/2502.05171  

æ³¨é‡Šå›¾æ¥è‡ªâ€œä½¿ç”¨æ½œåœ¨æ¨ç†æ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—ï¼šä¸€ç§å¾ªç¯æ·±åº¦æ–¹æ³•â€ï¼Œhttps://arxiv.org/abs/2502.05171

**ğŸ“„ 10 Feb, Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling, [https://arxiv.org/abs/2502.06703](https://arxiv.org/abs/2502.06703)  

ğŸ“„ 2 æœˆ 10 æ—¥ï¼Œ1B LLM èƒ½å¦è¶…è¶Š 405B LLMï¼Ÿé‡æ–°æ€è€ƒè®¡ç®—æœ€ä¼˜æµ‹è¯•æ—¶é—´æ‰©å±•ï¼Œ [https://arxiv.org/abs/2502.06703](https://arxiv.org/abs/2502.06703)**

Many inference-time scaling techniques depend on sampling, which requires a Process Reward Model (PRM) to select the best solution. This paper systematically analyzes how inference-time compute scaling interacts with PRMs and problem difficulty.Â   

è®¸å¤šæ¨ç†æ—¶é—´æ‰©å±•æŠ€æœ¯ä¾èµ–äºé‡‡æ ·ï¼Œè¿™éœ€è¦è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ (PRM) æ¥é€‰æ‹©æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡ç³»ç»Ÿåœ°åˆ†æäº†æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•å¦‚ä½•ä¸ PRM å’Œé—®é¢˜éš¾åº¦ç›¸äº’ä½œç”¨ã€‚

The researchers develop a compute-optimal scaling strategy that adapts to the choice of PRM, policy model, and task complexity. Their results show that with the right inference-time scaling approach, a 1B parameter model can outperform a 405B Llama 3 model that lacks inference-time scaling.Â   

ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§è®¡ç®—ä¼˜åŒ–æ‰©å±•ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯é€‚åº” PRMã€ç­–ç•¥æ¨¡å‹å’Œä»»åŠ¡å¤æ‚æ€§çš„é€‰æ‹©ã€‚ä»–ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ­£ç¡®çš„æ¨ç†æ—¶é—´æ‰©å±•æ–¹æ³•ï¼Œ1B å‚æ•°æ¨¡å‹å¯ä»¥èƒœè¿‡ç¼ºä¹æ¨ç†æ—¶é—´æ‰©å±•çš„ 405B Llama 3 æ¨¡å‹ã€‚

Similarly, they show how a 7B model with inference-time scaling surpasses DeepSeek-R1 while maintaining higher inference efficiency.Â   

åŒæ ·ï¼Œä»–ä»¬å±•ç¤ºäº†å…·æœ‰æ¨ç†æ—¶é—´ç¼©æ”¾çš„ 7B æ¨¡å‹å¦‚ä½•è¶…è¶Š DeepSeek-R1ï¼ŒåŒæ—¶ä¿æŒæ›´é«˜çš„æ¨ç†æ•ˆç‡ã€‚

These findings highlight how inference-time scaling can significantly improve LLMs, where small LLMs, with the right inference compute budget, can outperform much larger models.  

è¿™äº›å‘ç°å¼ºè°ƒäº†æ¨ç†æ—¶é—´æ‰©å±•å¦‚ä½•æ˜¾è‘—æ”¹å–„LLMsï¼Œå…¶ä¸­ï¼Œå°LLMsï¼Œåœ¨é€‚å½“çš„æ¨ç†è®¡ç®—é¢„ç®—ä¸‹ï¼Œå¯ä»¥èƒœè¿‡æ›´å¤§çš„æ¨¡å‹ã€‚

![](17417772544440.jpg)

Annotated figure from "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling", https://arxiv.org/abs/2502.06703  

æ³¨é‡Šå›¾æ¥è‡ªâ€œ1B LLM èƒ½å¦è¶…è¶Š 405B LLMï¼Ÿé‡æ–°æ€è€ƒè®¡ç®—æœ€ä½³æµ‹è¯•æ—¶é—´æ‰©å±•â€ï¼Œhttps://arxiv.org/abs/2502.06703

**ğŸ“„ 16 Feb, Learning to Reason from Feedback at Test-Time, [https://www.arxiv.org/abs/2502.12521](https://arxiv.org/abs/2502.15771)  

ğŸ“„ 2 æœˆ 16 æ—¥ï¼Œå­¦ä¹ æ ¹æ®æµ‹è¯•æ—¶çš„åé¦ˆè¿›è¡Œæ¨ç†ï¼Œ [https://www.arxiv.org/abs/2502.12521](https://arxiv.org/abs/2502.15771)**

It's a bit hard to classify this as either an inference-time or training-time method, because it optimizes the LLM, changing its weight parameters, at inference-time.  

å°†å…¶å½’ç±»ä¸ºæ¨ç†æ—¶é—´æ–¹æ³•æˆ–è®­ç»ƒæ—¶é—´æ–¹æ³•æœ‰ç‚¹å›°éš¾ï¼Œå› ä¸ºå®ƒåœ¨æ¨ç†æ—¶ä¼˜åŒ–äº†LLMï¼Œæ”¹å˜äº†å…¶æƒé‡å‚æ•°ã€‚

So, this paper explores a way to make LLMs learn from their mistakes during inference time without having to store failed attempts in the prompt (which gets expensive). Instead of the usual method of refining answers by adding previous attempts to the context (sequential revision) or blindly generating new answers (parallel sampling), this approach updates the model's weights at inference time.  

å› æ­¤ï¼Œæœ¬æ–‡æ¢è®¨äº†ä¸€ç§æ–¹æ³•ï¼Œè®© LLMs åœ¨æ¨ç†æ—¶ä»é”™è¯¯ä¸­å¸å–æ•™è®­ï¼Œè€Œæ— éœ€åœ¨æç¤ºä¸­å­˜å‚¨å¤±è´¥çš„å°è¯•ï¼ˆè¿™ä¼šå¾ˆæ˜‚è´µï¼‰ã€‚è¿™ç§æ–¹æ³•ä¸æ˜¯é€šè¿‡å°†ä¹‹å‰çš„å°è¯•æ·»åŠ åˆ°ä¸Šä¸‹æ–‡ï¼ˆé¡ºåºä¿®è®¢ï¼‰æˆ–ç›²ç›®ç”Ÿæˆæ–°ç­”æ¡ˆï¼ˆå¹¶è¡Œé‡‡æ ·ï¼‰æ¥å®Œå–„ç­”æ¡ˆçš„å¸¸ç”¨æ–¹æ³•ï¼Œè€Œæ˜¯åœ¨æ¨ç†æ—¶æ›´æ–°æ¨¡å‹çš„æƒé‡ã€‚

To do this, the authors introduce OpTune, a small, trainable optimizer that updates the model's weights based on the mistakes it made in a previous attempt. This means the model remembers what it did wrong without needing to keep the incorrect answer in the prompt/context.  

ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº† OpTuneï¼Œè¿™æ˜¯ä¸€ä¸ªå°å‹çš„å¯è®­ç»ƒä¼˜åŒ–å™¨ï¼Œå®ƒæ ¹æ®æ¨¡å‹åœ¨å‰ä¸€æ¬¡å°è¯•ä¸­æ‰€çŠ¯çš„é”™è¯¯æ›´æ–°æ¨¡å‹çš„æƒé‡ã€‚è¿™æ„å‘³ç€æ¨¡å‹ä¼šè®°ä½å®ƒåšé”™äº†ä»€ä¹ˆï¼Œè€Œæ— éœ€åœ¨æç¤º/ä¸Šä¸‹æ–‡ä¸­ä¿ç•™é”™è¯¯ç­”æ¡ˆã€‚

**ğŸ“„ 18 Feb, Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights, [https://www.arxiv.org/abs/2502.12521](https://www.arxiv.org/abs/2502.12521)  

ğŸ“„ 2 æœˆ 18 æ—¥ï¼Œæ¨ç†å’Œè§„åˆ’çš„æ¨ç†æ—¶é—´è®¡ç®—ï¼šåŸºå‡†å’Œè§è§£ï¼Œ [https://www.arxiv.org/abs/2502.12521](https://www.arxiv.org/abs/2502.12521)**

This paper benchmarks various inference-time compute scaling techniques for reasoning and planning tasks with a focus on analyzing their trade-offs between computational cost and performance.  

æœ¬æ–‡å¯¹æ¨ç†å’Œè§„åˆ’ä»»åŠ¡çš„å„ç§æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•æŠ€æœ¯è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œé‡ç‚¹åˆ†æäº†å®ƒä»¬åœ¨è®¡ç®—æˆæœ¬å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚

The authors evaluate multiple techniquesâ€”such as Chain-of-Thought, Tree-of-Thought, and Reasoning as Planning across eleven tasks spanning arithmetic, logical, commonsense, algorithmic reasoning, and planning.Â   

ä½œè€…å¯¹å¤šç§æŠ€æœ¯è¿›è¡Œäº†è¯„ä¼°ï¼Œä¾‹å¦‚æ€è·¯é“¾ã€æ€è·¯æ ‘å’Œæ¨ç†è§„åˆ’ï¼Œæ¶µç›–äº†ç®—æœ¯ã€é€»è¾‘ã€å¸¸è¯†ã€ç®—æ³•æ¨ç†å’Œè§„åˆ’ç­‰åä¸€é¡¹ä»»åŠ¡ã€‚

The main finding is that while scaling inference-time computation can improve reasoning, no single technique consistently outperforms others across all tasks.  

ä¸»è¦å‘ç°æ˜¯ï¼Œè™½ç„¶æ‰©å±•æ¨ç†æ—¶é—´è®¡ç®—å¯ä»¥æé«˜æ¨ç†èƒ½åŠ›ï¼Œä½†æ²¡æœ‰ä¸€ç§æŠ€æœ¯èƒ½å¤Ÿåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å§‹ç»ˆä¼˜äºå…¶ä»–æŠ€æœ¯ã€‚

![](17417772544472.jpg)

Annotated figure from _Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights_, https://www.arxiv.org/abs/2502.12521  

æ³¨é‡Šå›¾æ¥è‡ª_ã€Šæ¨ç†å’Œè§„åˆ’çš„æ¨ç†æ—¶é—´è®¡ç®—ï¼šåŸºå‡†å’Œè§è§£ã€‹_ ï¼Œhttps://www.arxiv.org/abs/2502.12521

**ğŸ“„ 19 Feb, Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking, [https://arxiv.org/abs/2502.13842](https://arxiv.org/abs/2502.13842)  

ğŸ“„ 2 æœˆ 19 æ—¥ï¼Œå†…åœ¨æ€ç»´è½¬æ¢å™¨ï¼šåˆ©ç”¨åŠ¨æ€æ·±åº¦æ‰©å±•æ¥ä¿ƒè¿›è‡ªé€‚åº”å†…éƒ¨æ€ç»´ï¼Œ [https://arxiv.org/abs/2502.13842](https://arxiv.org/abs/2502.13842)**

The Inner Thinking Transformer (ITT) dynamically allocates more compute during inference. Instead of using a fixed depth (= using same number of layers) for all tokens as in standard transformer-based LLMs, ITT employs Adaptive Token Routing to allocate more compute to difficult tokens. These difficult tokens pass through the same layer multiple times to undergo additional processing, which increases the inference-compute budget for these difficult tokens.  

Inner Thinking Transformer (ITT) åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åˆ†é…æ›´å¤šè®¡ç®—ã€‚ä¸åŸºäºæ ‡å‡† Transformer çš„ LLMs ä¸­å¯¹æ‰€æœ‰ token ä½¿ç”¨å›ºå®šæ·±åº¦ï¼ˆ= ä½¿ç”¨ç›¸åŒæ•°é‡çš„å±‚ï¼‰ä¸åŒï¼ŒITT é‡‡ç”¨è‡ªé€‚åº” token è·¯ç”±ä¸ºå›°éš¾ token åˆ†é…æ›´å¤šè®¡ç®—ã€‚è¿™äº›å›°éš¾ token å¤šæ¬¡é€šè¿‡åŒä¸€å±‚è¿›è¡Œé¢å¤–å¤„ç†ï¼Œä»è€Œå¢åŠ äº†è¿™äº›å›°éš¾ token çš„æ¨ç†è®¡ç®—é¢„ç®—ã€‚

![](17417772544504.jpg)

_Annotated figure from "Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking", https://arxiv.org/abs/2502.13842  

æ³¨é‡Šå›¾æ¥è‡ªâ€œå†…åœ¨æ€è€ƒè½¬æ¢å™¨ï¼šåˆ©ç”¨åŠ¨æ€æ·±åº¦ç¼©æ”¾æ¥ä¿ƒè¿›è‡ªé€‚åº”å†…éƒ¨æ€è€ƒâ€ï¼Œhttps://arxiv.org/abs/2502.13842_

**ğŸ“„ 20 Feb, S\*: Test Time Scaling for Code Generation, [https://arxiv.org/abs/2502.14382](https://arxiv.org/abs/2502.14382)  

ğŸ“„ 2 æœˆ 20 æ—¥ï¼ŒS\*ï¼šä»£ç ç”Ÿæˆçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼Œ [https://arxiv.org/abs/2502.14382](https://arxiv.org/abs/2502.14382)**

Inference-time scaling can be achieved by parallel scaling (generating multiple answers), sequential scaling (iteratively refining answers), or both as described in the Google paper from Summer 2024 ([Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)).  

æ¨ç†æ—¶é—´æ‰©å±•å¯ä»¥é€šè¿‡å¹¶è¡Œæ‰©å±•ï¼ˆç”Ÿæˆå¤šä¸ªç­”æ¡ˆï¼‰ã€é¡ºåºæ‰©å±•ï¼ˆè¿­ä»£ä¼˜åŒ–ç­”æ¡ˆï¼‰æˆ–ä¸¤è€…å…¼è€Œæœ‰ä¹‹æ¥å®ç°ï¼Œå¦‚ 2024 å¹´å¤å­£çš„ Google è®ºæ–‡ä¸­æ‰€è¿°ï¼ˆ[æ‰©å±• LLM æµ‹è¯•æ—¶é—´è®¡ç®—çš„æœ€ä½³æ•ˆæœå¯èƒ½æ¯”æ‰©å±•æ¨¡å‹å‚æ•°æ›´æœ‰æ•ˆ](https://arxiv.org/abs/2408.03314)ï¼‰ã€‚

S\* is a test-time compute scaling method designed specifically for code generation that improves both parallel scaling (generating multiple solutions) and sequential scaling (iterative debugging).Â   

S\* æ˜¯ä¸€ç§ä¸“ä¸ºä»£ç ç”Ÿæˆè®¾è®¡çš„æµ‹è¯•æ—¶è®¡ç®—æ‰©å±•æ–¹æ³•ï¼Œå¯æ”¹å–„å¹¶è¡Œæ‰©å±•ï¼ˆç”Ÿæˆå¤šä¸ªè§£å†³æ–¹æ¡ˆï¼‰å’Œé¡ºåºæ‰©å±•ï¼ˆè¿­ä»£è°ƒè¯•ï¼‰ã€‚

![](17417772544538.jpg)

_Annotated figure from "S\*: Test Time Scaling for Code Generation", https://arxiv.org/abs/2502.14382  

æ³¨é‡Šå›¾æ¥è‡ªâ€œS\*ï¼šä»£ç ç”Ÿæˆçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾â€ï¼Œhttps://arxiv.org/abs/2502.14382_

The approach operates in two stages:  

è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š

**Stage 1: GenerationÂ Â ç¬¬ä¸€é˜¶æ®µï¼šç”Ÿæˆ**

The model generates multiple code solutions and iteratively refines them using execution results and test cases provided in the problem prompt.  

è¯¥æ¨¡å‹ç”Ÿæˆå¤šä¸ªä»£ç è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä½¿ç”¨é—®é¢˜æç¤ºä¸­æä¾›çš„æ‰§è¡Œç»“æœå’Œæµ‹è¯•ç”¨ä¾‹è¿­ä»£åœ°å®Œå–„å®ƒä»¬ã€‚

Think of this like a coding competition where a model submits solutions, runs tests, and fixes mistakes:  

å¯ä»¥å°†å…¶æƒ³è±¡æˆä¸€åœºç¼–ç ç«èµ›ï¼Œå…¶ä¸­æ¨¡å‹æäº¤è§£å†³æ–¹æ¡ˆã€è¿è¡Œæµ‹è¯•å¹¶ä¿®å¤é”™è¯¯ï¼š

1\. The model generates multiple candidate solutions.  

1.æ¨¡å‹ç”Ÿæˆå¤šä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆã€‚

2\. Each solution is executed on public test cases (predefined input-output pairs).  

2\. æ¯ä¸ªè§£å†³æ–¹æ¡ˆéƒ½åœ¨å…¬å…±æµ‹è¯•ç”¨ä¾‹ï¼ˆé¢„å®šä¹‰çš„è¾“å…¥è¾“å‡ºå¯¹ï¼‰ä¸Šæ‰§è¡Œã€‚

3\. If a solution fails (incorrect output or crashes), the model analyzes the execution results (errors, outputs) and modifies the code to improve it.  

3\. å¦‚æœè§£å†³æ–¹æ¡ˆå¤±è´¥ï¼ˆè¾“å‡ºä¸æ­£ç¡®æˆ–å´©æºƒï¼‰ï¼Œæ¨¡å‹ä¼šåˆ†ææ‰§è¡Œç»“æœï¼ˆé”™è¯¯ã€è¾“å‡ºï¼‰å¹¶ä¿®æ”¹ä»£ç ä»¥æ”¹è¿›å®ƒã€‚

4\. This refinement process continues iteratively until the model finds solutions that pass the test cases.  

4\. è¿™ä¸ªæ”¹è¿›è¿‡ç¨‹ä¸æ–­è¿­ä»£ï¼Œç›´åˆ°æ¨¡å‹æ‰¾åˆ°é€šè¿‡æµ‹è¯•ç”¨ä¾‹çš„è§£å†³æ–¹æ¡ˆã€‚

**For example,** suppose the model is asked to implement a function is\_even(n) that returns True for even numbers and False otherwise.  

**ä¾‹å¦‚ï¼Œ**å‡è®¾è¦æ±‚æ¨¡å‹å®ç°ä¸€ä¸ªå‡½æ•° is\_even(n)ï¼Œè¯¥å‡½æ•°å¯¹äºå¶æ•°è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚

The modelâ€™s first attempt might be:  

è¯¥æ¨¡å‹çš„é¦–æ¬¡å°è¯•å¯èƒ½æ˜¯ï¼š

```
def is_even(n):
Â Â Â Â return n % 2Â  # âŒ Incorrect: should be `== 0`
```

The model tests this implementation with public test cases:  

è¯¥æ¨¡å‹ä½¿ç”¨å…¬å…±æµ‹è¯•ç”¨ä¾‹æ¥æµ‹è¯•æ­¤å®ç°ï¼š

```
Input        ExpectedModel OutputStatus
is_even(4)True        False        âŒ Fail
is_even(3)False        True        âŒ Fail
```

After reviewing the results, the model realizes that 4 % 2 returns 0, not True, so it **modifies** the function:  

åœ¨æ£€æŸ¥ç»“æœåï¼Œæ¨¡å‹æ„è¯†åˆ° 4 % 2 è¿”å›çš„æ˜¯ 0ï¼Œè€Œä¸æ˜¯ Trueï¼Œå› æ­¤**ä¿®æ”¹äº†**å‡½æ•°ï¼š

```
def is_even(n):
    return n % 2 == 0Â  # âœ… Corrected
```

Now the function **passes all public tests**, completing the debugging phase.  

ç°åœ¨è¯¥å‡½æ•°**é€šè¿‡äº†æ‰€æœ‰å…¬å…±æµ‹è¯•**ï¼Œå®Œæˆäº†è°ƒè¯•é˜¶æ®µã€‚

**Stage 2: SelectionÂ Â ç¬¬äºŒé˜¶æ®µï¼šé€‰æ‹©**

Once multiple solutions have passed public tests, the model must choose the best one (if possible). Here, S\* introduces adaptive input synthesis to avoid random picking:  

ä¸€æ—¦å¤šä¸ªè§£å†³æ–¹æ¡ˆé€šè¿‡äº†å…¬å¼€æµ‹è¯•ï¼Œæ¨¡å‹å¿…é¡»é€‰æ‹©æœ€ä½³è§£å†³æ–¹æ¡ˆï¼ˆå¦‚æœå¯èƒ½ï¼‰ã€‚è¿™é‡Œï¼ŒS\* å¼•å…¥äº†è‡ªé€‚åº”è¾“å…¥åˆæˆï¼Œä»¥é¿å…éšæœºé€‰æ‹©ï¼š

1\. The model compares two solutions that both pass public tests.  

1\. è¯¥æ¨¡å‹æ¯”è¾ƒäº†ä¸¤ç§å‡é€šè¿‡å…¬å¼€æµ‹è¯•çš„è§£å†³æ–¹æ¡ˆã€‚

2\. It asks itself: "Can I generate an input that will reveal a difference between these solutions?"  

2\. å®ƒä¼šé—®è‡ªå·±ï¼šâ€œæˆ‘èƒ½å¦ç”Ÿæˆä¸€ä¸ªè¾“å…¥æ¥æ­ç¤ºè¿™äº›è§£å†³æ–¹æ¡ˆä¹‹é—´çš„å·®å¼‚ï¼Ÿâ€

3\. It creates a new test input and runs both solutions on it.  

3\. å®ƒåˆ›å»ºä¸€ä¸ªæ–°çš„æµ‹è¯•è¾“å…¥å¹¶åœ¨å…¶ä¸Šè¿è¡Œä¸¤ç§è§£å†³æ–¹æ¡ˆã€‚

4\. If one solution produces the correct output while the other fails, the model selects the better one.  

4\. å¦‚æœä¸€ä¸ªè§£å†³æ–¹æ¡ˆäº§ç”Ÿäº†æ­£ç¡®çš„è¾“å‡ºè€Œå¦ä¸€ä¸ªå¤±è´¥äº†ï¼Œé‚£ä¹ˆæ¨¡å‹ä¼šé€‰æ‹©æ›´å¥½çš„ä¸€ä¸ªã€‚

5\. If both solutions behave identically, the model randomly picks one.  

5\. å¦‚æœä¸¤ç§è§£å†³æ–¹æ¡ˆçš„è¡¨ç°ç›¸åŒï¼Œæ¨¡å‹å°†éšæœºé€‰æ‹©å…¶ä¸­ä¸€ä¸ªã€‚

**For example,** consider two different implementations of `is_perfect_square(n)`:  

**ä¾‹å¦‚ï¼Œ**è€ƒè™‘`is_perfect_square(n)`çš„ä¸¤ç§ä¸åŒå®ç°ï¼š

```
import math

def is_perfect_square_A(n):
    return math.isqrt(n) ** 2 == n
```

```
def is_perfect_square_B(n):
    return math.sqrt(n).is_integer()
```

Both pass the provided test cases for simple examples:  

ä¸¤è€…éƒ½é€šè¿‡äº†æä¾›çš„ç®€å•ç¤ºä¾‹æµ‹è¯•ç”¨ä¾‹ï¼š

```
n = 25
print(is_perfect_square_A(n))Â  # âœ… True (Correct)
print(is_perfect_square_B(n))Â  # âœ… True (Correct)
```

But when the LLM generates edge cases we can see one of them fail, so the model would select the solution A in this case:  

ä½†æ˜¯å½“ LLM ç”Ÿæˆè¾¹ç¼˜æƒ…å†µæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å…¶ä¸­ä¸€ä¸ªä¼šå¤±è´¥ï¼Œå› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹æ¨¡å‹ä¼šé€‰æ‹©è§£å†³æ–¹æ¡ˆ Aï¼š

```
n = 10**16 + 1
print(is_perfect_square_A(n))Â  # âœ… False (Correct)
print(is_perfect_square_B(n))Â  # âŒ True (Incorrect)
```

**ğŸ“„ 25 Feb, Chain of Draft: Thinking Faster by Writing Less, [https://arxiv.org/abs/2502.18600](https://arxiv.org/abs/2502.18600)  

ğŸ“„ 2 æœˆ 25 æ—¥ï¼Œè‰ç¨¿é“¾ï¼šå°‘å†™ï¼Œæ€è€ƒæ›´å¿«ï¼Œ [https://arxiv.org/abs/2502.18600](https://arxiv.org/abs/2502.18600)**

The researchers observe that while reasoning LLMs often generate verbose step-by-step explanations, humans typically rely on concise drafts that capture only essential information.Â   

ç ”ç©¶äººå‘˜è§‚å¯Ÿåˆ°ï¼Œè™½ç„¶æ¨ç†é€šå¸¸ä¼šäº§ç”Ÿè¯¦ç»†çš„é€æ­¥è§£é‡Šï¼Œä½†äººç±»é€šå¸¸ä¾èµ–äºä»…æ•æ‰åŸºæœ¬ä¿¡æ¯çš„ç®€æ´è‰ç¨¿ã€‚

Inspired by this, they propose Chain of Draft (CoD), a prompting strategy that reduces verbosity by generating minimal but informative intermediate steps. So, in a sense it's a method for inference-time scaling that improves the efficiency of inference-time scaling through generating fewer tokens.  

å—æ­¤å¯å‘ï¼Œä»–ä»¬æå‡ºäº† Chain of Draft (CoD)ï¼Œè¿™æ˜¯ä¸€ç§æç¤ºç­–ç•¥ï¼Œé€šè¿‡ç”Ÿæˆæœ€å°‘ä½†ä¿¡æ¯ä¸°å¯Œçš„ä¸­é—´æ­¥éª¤æ¥å‡å°‘å†—é•¿çš„å†…å®¹ã€‚å› æ­¤ï¼Œä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œè¿™æ˜¯ä¸€ç§æ¨ç†æ—¶é—´æ‰©å±•çš„æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆæ›´å°‘çš„ token æ¥æé«˜æ¨ç†æ—¶é—´æ‰©å±•çš„æ•ˆç‡ã€‚

Looking at the results, it seems that CoD is almost as brief as standard prompting, but as accurate as Chain of Thought (CoT) prompting. As I mentioned earlier, in my opinion, one of the advantages of reasoning models is that users can read the reasoning traces to learn and to better evaluate / trust the response. CoD somewhat diminishes the advantage of CoD. However, it might come in very handy where verbose intermediate steps are not needed as it speeds up the generation while maintaining the accuracy of CoT.  

ä»ç»“æœæ¥çœ‹ï¼ŒCoD ä¼¼ä¹ä¸æ ‡å‡†æç¤ºä¸€æ ·ç®€çŸ­ï¼Œä½†ä¸æ€ç»´é“¾ (CoT) æç¤ºä¸€æ ·å‡†ç¡®ã€‚æ­£å¦‚æˆ‘ä¹‹å‰æåˆ°çš„ï¼Œåœ¨æˆ‘çœ‹æ¥ï¼Œæ¨ç†æ¨¡å‹çš„ä¼˜åŠ¿ä¹‹ä¸€æ˜¯ç”¨æˆ·å¯ä»¥é˜…è¯»æ¨ç†ç—•è¿¹æ¥å­¦ä¹ å¹¶æ›´å¥½åœ°è¯„ä¼°/ä¿¡ä»»å“åº”ã€‚CoD åœ¨ä¸€å®šç¨‹åº¦ä¸Šå‰Šå¼±äº† CoD çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œå®ƒå¯èƒ½åœ¨ä¸éœ€è¦å†—é•¿çš„ä¸­é—´æ­¥éª¤çš„æƒ…å†µä¸‹éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥åŠ å¿«ç”Ÿæˆé€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒ CoT çš„å‡†ç¡®æ€§ã€‚

**ğŸ“„ 6 Mar, Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks, [https://arxiv.org/abs/2503.04378](https://arxiv.org/abs/2503.04378)  

ğŸ“„ 3 æœˆ 6 æ—¥ï¼Œä¸“ç”¨åé¦ˆå’Œç¼–è¾‘æ¨¡å‹ä¸ºå¼€æ”¾å¼é€šç”¨é¢†åŸŸä»»åŠ¡æä¾›æ¨ç†æ—¶é—´æ‰©å±•èƒ½åŠ›ï¼Œ [https://arxiv.org/abs/2503.04378](https://arxiv.org/abs/2503.04378)**

Many techniques for scaling inference-time reasoning rely on tasks with verifiable answers (like math and code that can be checked), which makes them difficult to apply to open-ended tasks like writing and general problem-solving.  

è®¸å¤šæ‰©å±•æ¨ç†æ—¶é—´æ¨ç†çš„æŠ€æœ¯ä¾èµ–äºå…·æœ‰å¯éªŒè¯ç­”æ¡ˆçš„ä»»åŠ¡ï¼ˆå¦‚å¯æ£€æŸ¥çš„æ•°å­¦å’Œä»£ç ï¼‰ï¼Œè¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥åº”ç”¨äºå†™ä½œå’Œä¸€èˆ¬é—®é¢˜è§£å†³ç­‰å¼€æ”¾å¼ä»»åŠ¡ã€‚

To address this limitation regarding verifiable answers, the researchers develop a system where one model generates an initial response, another provides feedback ("feedback model"), and a third refines the response based on that feedback ("edit model").  

ä¸ºäº†è§£å†³å¯éªŒè¯ç­”æ¡ˆçš„è¿™ç§é™åˆ¶ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ä¸ªç³»ç»Ÿï¼Œå…¶ä¸­ä¸€ä¸ªæ¨¡å‹ç”Ÿæˆåˆå§‹å“åº”ï¼Œå¦ä¸€ä¸ªæ¨¡å‹æä¾›åé¦ˆï¼ˆâ€œåé¦ˆæ¨¡å‹â€ï¼‰ï¼Œç¬¬ä¸‰ä¸ªæ¨¡å‹æ ¹æ®è¯¥åé¦ˆæ”¹è¿›å“åº”ï¼ˆâ€œç¼–è¾‘æ¨¡å‹â€ï¼‰ã€‚

They train these specialized "feedback" and "edit" models using a large dataset of human-annotated responses and feedback. These models then help improve responses by generating better feedback and making more effective edits during inference time.  

ä»–ä»¬ä½¿ç”¨å¤§é‡äººå·¥æ³¨é‡Šçš„å“åº”å’Œåé¦ˆæ•°æ®é›†æ¥è®­ç»ƒè¿™äº›ä¸“é—¨çš„â€œåé¦ˆâ€å’Œâ€œç¼–è¾‘â€æ¨¡å‹ã€‚ç„¶åï¼Œè¿™äº›æ¨¡å‹é€šè¿‡åœ¨æ¨ç†æ—¶é—´å†…ç”Ÿæˆæ›´å¥½çš„åé¦ˆå’Œè¿›è¡Œæ›´æœ‰æ•ˆçš„ç¼–è¾‘æ¥å¸®åŠ©æ”¹å–„å“åº”ã€‚

![](17417772544571.png)

Inference-time compute scaling has become one of the hottest research topics this year to improve the reasoning abilities of large language models without requiring modification to model weights.Â   

æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•å·²æˆä¸ºä»Šå¹´æœ€çƒ­é—¨çš„ç ”ç©¶è¯¾é¢˜ä¹‹ä¸€ï¼Œå®ƒå¯ä»¥åœ¨ä¸éœ€è¦ä¿®æ”¹æ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

The many techniques I summarized above range from simple token-based interventions like â€œWaitâ€ tokens to sophisticated search and optimization-based strategies such as Test-Time Preference Optimization and Chain-of-Associated-Thoughts.Â   

æˆ‘ä¸Šé¢æ€»ç»“çš„è®¸å¤šæŠ€æœ¯åŒ…æ‹¬ä»ç®€å•çš„åŸºäºæ ‡è®°çš„å¹²é¢„ï¼ˆå¦‚â€œç­‰å¾…â€æ ‡è®°ï¼‰åˆ°å¤æ‚çš„åŸºäºæœç´¢å’Œä¼˜åŒ–çš„ç­–ç•¥ï¼ˆå¦‚æµ‹è¯•æ—¶é—´åå¥½ä¼˜åŒ–å’Œå…³è”æ€ç»´é“¾ï¼‰ã€‚

On the big-picture level, one recurring theme is that increasing compute at inference allows even relatively small models to achieve substantial improvements (on reasoning benchmarks) compared to standard approaches.Â   

ä»æ€»ä½“å±‚é¢æ¥çœ‹ï¼Œä¸€ä¸ªåå¤å‡ºç°çš„ä¸»é¢˜æ˜¯ï¼Œä¸æ ‡å‡†æ–¹æ³•ç›¸æ¯”ï¼Œå¢åŠ æ¨ç†è®¡ç®—èƒ½åŠ›ç”šè‡³å¯ä»¥è®©ç›¸å¯¹è¾ƒå°çš„æ¨¡å‹å®ç°æ˜¾ç€çš„æ”¹è¿›ï¼ˆåœ¨æ¨ç†åŸºå‡†ä¸Šï¼‰ã€‚

This suggests that inference strategies can help narrow the performance gap between smaller, more cost-effective models and their larger counterparts.Â   

è¿™è¡¨æ˜æ¨ç†ç­–ç•¥å¯ä»¥å¸®åŠ©ç¼©å°è¾ƒå°ã€æ›´å…·æˆæœ¬æ•ˆç›Šçš„æ¨¡å‹ä¸è¾ƒå¤§æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚

**The cost caveatÂ Â æˆæœ¬è­¦å‘Š**

The caveat is that inference-time scaling increases the inference costs, so whether to use a small model with substantial inference scaling or training a larger model and using it with less or no inference scaling is a math that has to be worked out based on how much use the model gets.  

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ¨ç†æ—¶é—´æ‰©å±•ä¼šå¢åŠ æ¨ç†æˆæœ¬ï¼Œå› æ­¤ï¼Œæ˜¯å¦ä½¿ç”¨å…·æœ‰å¤§é‡æ¨ç†æ‰©å±•çš„å°æ¨¡å‹ï¼Œè¿˜æ˜¯è®­ç»ƒæ›´å¤§çš„æ¨¡å‹å¹¶ä½¿ç”¨è¾ƒå°‘æˆ–ä¸ä½¿ç”¨æ¨ç†æ‰©å±•ï¼Œæ˜¯ä¸€ä¸ªå¿…é¡»æ ¹æ®æ¨¡å‹çš„ä½¿ç”¨é‡æ¥è®¡ç®—çš„æ•°å­¦é—®é¢˜ã€‚

As an example, an o1 model, which uses heavy inference time scaling, is actually still slightly cheaper than a likely larger GPT-4.5 model that likely doesn't use inference time scaling.Â   

ä¸¾ä¾‹æ¥è¯´ï¼Œä½¿ç”¨é‡åº¦æ¨ç†æ—¶é—´ç¼©æ”¾çš„ o1 æ¨¡å‹å®é™…ä¸Šä»ç„¶æ¯”å¯èƒ½ä¸ä½¿ç”¨æ¨ç†æ—¶é—´ç¼©æ”¾çš„æ›´å¤§çš„ GPT-4.5 æ¨¡å‹ç¨å¾®ä¾¿å®œä¸€äº›ã€‚

![](17417772544577.png)

(It will be interesting to see how well GPT-4.5 will perform with o1- or o3-style inference-time scaling.)  

ï¼ˆçœ‹çœ‹ GPT-4.5 åœ¨ o1 æˆ– o3 é£æ ¼çš„æ¨ç†æ—¶é—´ç¼©æ”¾ä¸‹çš„è¡¨ç°å¦‚ä½•å°†ä¼šå¾ˆæœ‰è¶£ã€‚ï¼‰

**Which technique?Â Â å“ªç§æŠ€æœ¯ï¼Ÿ**

However, inference-time compute scaling is not a silver bullet. While methods like Monte Carlo Tree Search, self-backtracking, and dynamic-depth scaling can substantially improve reasoning performance, the effectiveness also still depends on the task and difficulty. As one of the earlier papers showed, there's no inference-time compute scaling technique that performs best across all tasks.  

ç„¶è€Œï¼Œæ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•å¹¶ä¸æ˜¯çµä¸¹å¦™è¯ã€‚è™½ç„¶è’™ç‰¹å¡æ´›æ ‘æœç´¢ã€è‡ªå›æº¯å’ŒåŠ¨æ€æ·±åº¦æ‰©å±•ç­‰æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜æ¨ç†æ€§èƒ½ï¼Œä½†å…¶æœ‰æ•ˆæ€§ä»ç„¶å–å†³äºä»»åŠ¡å’Œéš¾åº¦ã€‚æ­£å¦‚ä¸€ç¯‡æ—©æœŸè®ºæ–‡æ‰€è¡¨æ˜çš„é‚£æ ·ï¼Œæ²¡æœ‰ä¸€ç§æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•æŠ€æœ¯åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ã€‚

Additionally, many of these approaches trade off response latency for improved reasoning, and slow responses can be annoying to some users. For instance, I usually switch from o1 to GPT4o if I have simple tasks due to the faster response time.  

æ­¤å¤–ï¼Œè®¸å¤šæ­¤ç±»æ–¹æ³•éƒ½ä»¥ç‰ºç‰²å“åº”å»¶è¿Ÿä¸ºä»£ä»·æ¥æé«˜æ¨ç†èƒ½åŠ›ï¼Œè€Œå“åº”ç¼“æ…¢å¯èƒ½ä¼šè®©æŸäº›ç”¨æˆ·æ„Ÿåˆ°åŒçƒ¦ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘æœ‰ç®€å•çš„ä»»åŠ¡ï¼Œæˆ‘é€šå¸¸ä¼šä» o1 åˆ‡æ¢åˆ° GPT4oï¼Œå› ä¸ºå®ƒçš„å“åº”æ—¶é—´æ›´å¿«ã€‚

**What's nextÂ Â ä¸‹ä¸€æ­¥**

Looking ahead, I think we will see many more papers this year centered around the two main branches of "reasoning via inference-time compute scaling" research:Â   

å±•æœ›æœªæ¥ï¼Œæˆ‘è®¤ä¸ºä»Šå¹´æˆ‘ä»¬å°†çœ‹åˆ°æ›´å¤šå›´ç»•â€œé€šè¿‡æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•è¿›è¡Œæ¨ç†â€ç ”ç©¶çš„ä¸¤ä¸ªä¸»è¦åˆ†æ”¯çš„è®ºæ–‡ï¼š

1\. Research that is purely centered around developing the best possible model topping the benchmarks.  

1\. çº¯ç²¹ä»¥å¼€å‘è¾¾åˆ°åŸºå‡†çš„æœ€ä½³æ¨¡å‹ä¸ºä¸­å¿ƒçš„ç ”ç©¶ã€‚

2\. Research that is concerned with balancing cost and performance trade-offs across different reasoning tasks.  

2\. æ¶‰åŠå¹³è¡¡ä¸åŒæ¨ç†ä»»åŠ¡ä¹‹é—´çš„æˆæœ¬å’Œæ€§èƒ½æƒè¡¡çš„ç ”ç©¶ã€‚

Either way, what's nice about inference-time compute scaling is that it can be applied to any type of existing LLM to make it better for specific tasks.  

æ— è®ºå“ªç§æ–¹å¼ï¼Œæ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•çš„ä¼˜ç‚¹åœ¨äºå®ƒå¯ä»¥åº”ç”¨äºä»»ä½•ç±»å‹çš„ç°æœ‰LLMï¼Œä»¥ä½¿å…¶æ›´å¥½åœ°å®Œæˆç‰¹å®šä»»åŠ¡ã€‚

**Thinking on DemandÂ Â æŒ‰éœ€æ€è€ƒ**

An interesting trend on the industry side is what I refer to as "thinking on demand". Following the release of DeepSeek R1, it feels like companies have been rushing to add reasoning capabilities to their offerings.Â   

è¡Œä¸šæ–¹é¢ä¸€ä¸ªæœ‰è¶£çš„è¶‹åŠ¿å°±æ˜¯æˆ‘æ‰€è¯´çš„â€œæŒ‰éœ€æ€è€ƒâ€ã€‚åœ¨ DeepSeek R1 å‘å¸ƒä¹‹åï¼Œæ„Ÿè§‰å„å®¶å…¬å¸éƒ½åœ¨äº‰ç›¸ä¸ºå…¶äº§å“æ·»åŠ æ¨ç†åŠŸèƒ½ã€‚

An interesting development here is that most LLM providers started to add the option for users to enable or disable thinking. An interesting development is that most LLM providers now allow users to enable or disable these "thinking" features. The mechanism is not publicly shared, but it's likely the same model with dialed-back inference-time compute scaling.Â   

ä¸€ä¸ªæœ‰è¶£çš„å‘å±•æ˜¯ï¼Œå¤§å¤šæ•°æä¾›å•†å¼€å§‹ä¸ºç”¨æˆ·æ·»åŠ å¯ç”¨æˆ–ç¦ç”¨æ€è€ƒçš„é€‰é¡¹ã€‚ä¸€ä¸ªæœ‰è¶£çš„å‘å±•æ˜¯ï¼Œå¤§å¤šæ•°æä¾›å•†ç°åœ¨å…è®¸ç”¨æˆ·å¯ç”¨æˆ–ç¦ç”¨è¿™äº›â€œæ€è€ƒâ€åŠŸèƒ½ã€‚è¯¥æœºåˆ¶å¹¶æœªå…¬å¼€åˆ†äº«ï¼Œä½†å®ƒå¯èƒ½æ˜¯å…·æœ‰å›æ‹¨æ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•çš„ç›¸åŒæ¨¡å‹ã€‚

For instance, [Claude 3.7 Sonnet](https://www.anthropic.com/news/claude-3-7-sonnet) and [Grok 3](https://x.ai/blog/grok-3) now have a "thinking" that users can enable for their model, whereas OpenAI requires users to switch between models. For example, GPT4o/4.5 and o1/o3-mini if they want to use explicit reasoning models. However, the OpenAI CEO mentioned that GPT4.5 will likely be their last model, which doesn't explicitly have a reasoning or "thinking" mode. On the open-source side, even IBM added an explicit "thinking" toggle to their [Granite models](https://www.ibm.com/new/announcements/ibm-granite-3-2-open-source-reasoning-and-vision).  

ä¾‹å¦‚ï¼Œ [Claude 3.7 Sonnet](https://www.anthropic.com/news/claude-3-7-sonnet)å’Œ[Grok 3](https://x.ai/blog/grok-3)ç°åœ¨å…·æœ‰ç”¨æˆ·å¯ä»¥ä¸ºå…¶æ¨¡å‹å¯ç”¨çš„â€œæ€è€ƒâ€åŠŸèƒ½ï¼Œè€Œ OpenAI åˆ™è¦æ±‚ç”¨æˆ·åœ¨æ¨¡å‹ä¹‹é—´åˆ‡æ¢ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä»–ä»¬æƒ³ä½¿ç”¨æ˜¾å¼æ¨ç†æ¨¡å‹ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ GPT4o/4.5 å’Œ o1/o3-miniã€‚ç„¶è€Œï¼ŒOpenAI é¦–å¸­æ‰§è¡Œå®˜æåˆ°ï¼ŒGPT4.5 å¾ˆå¯èƒ½æ˜¯ä»–ä»¬çš„æœ€åä¸€ä¸ªæ¨¡å‹ï¼Œå®ƒæ²¡æœ‰æ˜ç¡®çš„æ¨ç†æˆ–â€œæ€è€ƒâ€æ¨¡å¼ã€‚åœ¨å¼€æºæ–¹é¢ï¼Œç”šè‡³ IBM ä¹Ÿä¸ºä»–ä»¬çš„[Granite æ¨¡å‹](https://www.ibm.com/new/announcements/ibm-granite-3-2-open-source-reasoning-and-vision)æ·»åŠ äº†ä¸€ä¸ªæ˜¾å¼çš„â€œæ€è€ƒâ€åˆ‡æ¢ã€‚

Overall, the trend of adding reasoning capabilities whether via inference-time or train-time compute scaling is a major step forward for LLMs in 2025.Â   

æ€»ä½“è€Œè¨€ï¼Œæ— è®ºæ˜¯é€šè¿‡æ¨ç†æ—¶é—´è¿˜æ˜¯è®­ç»ƒæ—¶é—´è®¡ç®—æ‰©å±•æ¥å¢åŠ æ¨ç†èƒ½åŠ›çš„è¶‹åŠ¿å¯¹äº 2025 å¹´æ¥è¯´éƒ½æ˜¯ä¸€ä¸ªé‡å¤§çš„è¿›æ­¥ã€‚

In time, I expect that reasoning will no longer be treated as an optional or special feature but will instead become the standard, much as instruction-finetuned or RLHF-tuned models are now the norm over raw pretrained models.  

éšç€æ—¶é—´çš„æ¨ç§»ï¼Œæˆ‘é¢„è®¡æ¨ç†å°†ä¸å†è¢«è§†ä¸ºå¯é€‰æˆ–ç‰¹æ®ŠåŠŸèƒ½ï¼Œè€Œæ˜¯æˆä¸ºæ ‡å‡†ï¼Œå°±åƒæŒ‡ä»¤å¾®è°ƒæˆ– RLHF è°ƒæ•´æ¨¡å‹ç°åœ¨æˆä¸ºåŸå§‹é¢„è®­ç»ƒæ¨¡å‹çš„æ ‡å‡†ä¸€æ ·ã€‚

As mentioned earlier, this article solely focused on inference-time compute length due to its already long lengths, thanks to the very active reasoning research activity. **In a future article, I plan to cover all the interesting train-time compute scaling methods for reasoning.**  

å¦‚å‰æ‰€è¿°ï¼Œç”±äºæ¨ç†ç ”ç©¶æ´»åŠ¨éå¸¸æ´»è·ƒï¼Œæœ¬æ–‡ä»…å…³æ³¨æ¨ç†æ—¶é—´è®¡ç®—é•¿åº¦ï¼Œå› ä¸ºå®ƒçš„é•¿åº¦å·²ç»å¾ˆé•¿ã€‚**åœ¨ä»¥åçš„æ–‡ç« ä¸­ï¼Œæˆ‘è®¡åˆ’ä»‹ç»æ‰€æœ‰æœ‰è¶£çš„æ¨ç†è®­ç»ƒæ—¶é—´è®¡ç®—æ‰©å±•æ–¹æ³•ã€‚**

_This magazine is a personal passion project. For those who wish to support me, please consider purchasing a copy of my or consider signing up for a .  

æœ¬æ‚å¿—æ˜¯æˆ‘ä¸ªäººçš„çƒ­æƒ…é¡¹ç›®ã€‚å¯¹äºé‚£äº›å¸Œæœ›æ”¯æŒæˆ‘çš„äººï¼Œè¯·è€ƒè™‘è´­ä¹°æˆ‘çš„æ‚å¿—æˆ–è€ƒè™‘æ³¨å†Œã€‚_

_This magazine is a personal passion project. To support me as an independent researcher, please consider purchasing a copy of my book, [Build a Large Language Model (From Scratch) book](https://amzn.to/4fqvn0D), or signing up for a [paid subscription](https://magazine.sebastianraschka.com/subscribe).  

æœ¬æ‚å¿—æ˜¯æˆ‘ä¸ªäººçš„çƒ­æƒ…é¡¹ç›®ã€‚ä¸ºäº†æ”¯æŒæˆ‘ä½œä¸ºä¸€åç‹¬ç«‹ç ”ç©¶å‘˜ï¼Œè¯·è€ƒè™‘è´­ä¹°æˆ‘çš„ä¹¦[ã€Šä»å¤´å¼€å§‹æ„å»ºå¤§å‹è¯­è¨€æ¨¡å‹ã€‹ä¸€ä¹¦](https://amzn.to/4fqvn0D)ï¼Œæˆ–æ³¨å†Œ[ä»˜è´¹è®¢é˜…](https://magazine.sebastianraschka.com/subscribe)ã€‚_

_If you read the book and have a few minutes to spare, I'd really appreciate a [brief review](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167). It helps us authors a lot!  

å¦‚æœæ‚¨è¯»è¿‡è¿™æœ¬ä¹¦ï¼Œå¹¶ä¸”æœ‰å‡ åˆ†é’Ÿçš„ç©ºé—²æ—¶é—´ï¼Œæˆ‘éå¸¸å¸Œæœ›æ‚¨èƒ½ç»™æˆ‘ä¸€ä¸ª[ç®€çŸ­çš„è¯„è®º](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167)ã€‚è¿™å¯¹æˆ‘ä»¬ä½œå®¶å¾ˆæœ‰å¸®åŠ©ï¼_

**Your support means a great deal! Thank you!  

æ‚¨çš„æ”¯æŒå¯¹æˆ‘ä»¬æ„ä¹‰é‡å¤§ï¼è°¢è°¢ï¼**
