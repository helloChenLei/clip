---
title: "如何在不牺牲安全和民主的情况下推进人工智能进步？"
date: 2024-01-05T20:29:07+08:00
updated: 2024-01-05T20:29:07+08:00
taxonomies:
  tags: []
extra:
  source: https://time.com/collection/time100-voices/6325786/ai-progress-safety-democracy/
  hostname: time.com
  author: By Yoshua Bengio and Daniel Privitera
  original_title: "How AI Progress Can Coexist With Safety and Democracy"
  original_lang: en
---

Dive into current discussions about how to regulate artificial intelligence, and you’d think we’re grappling with impossible choices: Should we choose AI progress or AI safety?  

深入探讨如何监管人工智能的当前讨论，你会觉得我们面临着无法选择的困境：我们应该选择人工智能的进步还是人工智能的安全？  

Address present-day impacts of AI or potential future risks? And many more perceived dilemmas.  

现在请翻译以下内容为Simplified Chinese ： 现今AI的现实影响或潜在的未来风险？以及许多其他被认为存在的困境。  

But are these trade-offs real?  

但这些权衡是真实存在的吗？  

As world leaders prepare to gather at the upcoming AI Safety Summit in [Bletchley Park](https://time.com/3609585/the-true-story-of-the-imitation-game/) in the U.K.  

随着世界各国领导人准备在即将到来的英国布莱切利公园AI安全峰会上聚集在一起，  

in early November, let’s dig a bit deeper and uncover three core values that underpin most policy proposals in the AI regulation discourse.  

让我们深入挖掘一下，并揭示AI监管讨论中大多数政策提案所依据的三个核心价值观。

The first value is progress.  

第一个价值观是进步。  

The promises of AI are vast: curing diseases, increasing productivity, helping to solve climate change.  

AI的承诺是巨大的：治愈疾病，提高生产力，帮助解决气候变化问题。  

This seems to call for a “full steam ahead” approach, in which we attempt to accelerate AI progress even beyond the current level.  

这似乎需要采取“全速前进”的方法，即我们试图加速AI的进展，甚至超越当前水平。  

But moving at breakneck speed comes with increased risks—epidemics of automated fake news, AI enhanced bio terrorism, automated cyber warfare, or out-of-control AI threatening the existence of humanity.  

但是，以破颈速度前进会带来增加的风险——自动化假新闻的流行，AI增强的生物恐怖主义，自动化网络战争，或者失控的AI威胁人类的存在。  

We are not currently prepared to handle these risks well.  

我们目前还没有准备好很好地处理这些风险。  

We don’t know how to reliably control advanced AI systems, and we don’t currently have mechanisms for preventing their misuse.  

我们不知道如何可靠地控制先进的AI系统，也没有防止其滥用的机制。

The second core value in AI regulation is therefore safety. [Leading experts](https://www.safe.ai/statement-on-ai-risk) as well as the [general public](https://theaipi.org/poll-shows-overwhelming-concern-about-risks-from-ai-as-new-institute-launches-to-understand-public-opinion-and-advocate-for-responsible-ai-policies/) are increasingly concerned about extreme risks from AI, and policy-makers are rightly beginning to look for ways to increase AI safety.  

AI监管的第二个核心价值是安全性。领先的专家以及普通公众对于AI带来的极端风险越来越关注，政策制定者也开始寻找增加AI安全性的方法，这是正确的。  

But prioritizing safety at all costs can also have undesirable consequences.  

但是，以安全为首要考虑可能也会带来不良后果。  

For instance, it can make sense from a safety perspective to limit the open-sourcing of AI models if they can be used for potentially dangerous purposes like engineering a highly contagious virus.  

例如，从安全角度来看，限制AI模型的开源可能是有意义的，如果它们可以被用于潜在危险的目的，比如工程化一个高度传染性的病毒。  

On the flipside, however, open-source code helps to reduce concentration of power.  

然而，开源代码有助于减少权力集中。  

In a world with increasingly capable AI, leaving this rapidly growing power in the hands of a few profit-driven companies could seriously endanger democratic sovereignty.  

在一个拥有越来越强大的AI的世界中，将这种快速增长的权力留在少数以盈利为驱动的公司手中可能严重危及民主主权。  

Who will decide what very powerful AI systems are used for? To whose benefit or to who’s detriment?  

谁来决定非常强大的AI系统用于什么目的？对谁有利或对谁有害？  

If superhuman capabilities end up in a few private hands without significant democratic governance, the very principle of sharing power that underlies democracy is threatened.  

如果超人类的能力最终掌握在少数私人手中而没有重要的民主治理，那么作为民主基础的权力分享原则将受到威胁。

That is why the third core value in AI regulation is democratic participation.  

这就是为什么AI监管的第三个核心价值是民主参与。  

There is a real concern that AI might entrench existing power imbalances at the expense of marginalized groups, low income countries, and potentially everyone but a handful of tech giants building the most powerful AI models.  

有一个真正的担忧是，人工智能可能会在弱势群体、低收入国家以及除了少数几家构建最强大人工智能模型的科技巨头之外，加剧现有的权力不平衡。  

This suggests we need to ensure continued participation from everyone in the future of AI.  

这表明我们需要确保每个人都能参与到人工智能的未来中去。  

But focusing exclusively on participation would also come at a cost.  

但是，仅仅关注参与也是有代价的。  

Democratizing access to potentially highly destructive technology can lead to catastrophic outcomes, which is why access to certain tech in sectors like nuclear energy or pathogen research is also not democratized, but highly restricted and regulated.  

将潜在具有高度破坏性的技术民主化可能导致灾难性的结果，这就是为什么像核能或病原体研究等领域的技术访问也没有民主化，而是高度限制和监管的原因。

How do progress, safety, and participation interact in practice, then? The [“Transformative Technology Trilemma,” a framework developed by the Collective Intelligence Project,](https://cip.org/whitepaper) suggests that discourse about these values is particularly susceptible to an either-or kind of thinking.  

那么，进步、安全和参与在实践中是如何相互作用的呢？由集体智能项目开发的“变革性技术三难题”框架表明，关于这些价值观的讨论特别容易陷入非此即彼的思维方式。  

And unfortunately, public discourse around AI can indeed convey a sense that we have to choose between three mutually incompatible AI futures: either AI Progress, or AI Safety, or AI with Democratic Participation.  

不幸的是，围绕人工智能的公共讨论确实传达了这样一种感觉，即我们必须在三种互不兼容的人工智能未来之间做出选择：要么是人工智能进步，要么是人工智能安全，要么是具有民主参与的人工智能。  

What a tragic trilemma! But this picture is not accurate.  

多么悲剧的三难题！但这种画面并不准确。  

And it can lead to a dangerous, well-studied phenomenon called [false polarization](https://www.sciencedirect.com/science/article/abs/pii/S2352250X21000749), in which people perceive disagreements as more profound than they actually are.  

这可能导致一种危险的、经过深入研究的现象，称为虚假极化，即人们认为分歧比实际更深刻。  

This can have grave consequences – ironically, False Polarization has been shown to reinforce actual polarization.  

这可能带来严重后果 - 具有讽刺意味的是，虚假极化已被证明会加剧实际的极化。  

If I (wrongly) think the other person strongly disagrees with me, I might actually toughen my rhetoric in response.  

如果我（错误地）认为对方与我强烈不同意，我可能会加强我的言辞回应。  

But in reality, most people care about all three core values underlying AI regulation.  

但实际上，大多数人都关心AI监管背后的三个核心价值观。

And in reality, we do not have to choose. We can have AI progress _and_ safety _and_ democratic participation.  

而事实上，我们不必做出选择。我们可以同时实现AI的进步、安全和民主参与。  

Here are four policy goals that will help us get all three—a “Beneficial AI Roadmap”:  

以下是四个政策目标，将帮助我们实现这三个目标 - 一个“有益的AI路线图”：

## **Invest in innovative and beneficial uses of existing AI  

投资于现有AI的创新和有益用途**

Many start-ups are building innovative applications on top of existing general-purpose AI models like GPT-4. They unlock productive use cases of existing AI that can benefit millions of people.  

许多初创公司正在基于现有的通用AI模型（如GPT-4）构建创新应用。它们开启了现有AI的生产性用例，可以惠及数百万人。  

Governments could ease the regulatory burden on these SMEs to foster progress by ensuring that the general-purpose models that they build upon, which are usually developed by tech giants like Microsoft and Google, are safe, unbiased and reliable.  

政府可以通过确保它们所依赖的通用模型（通常由微软和谷歌等科技巨头开发）的安全、公正和可靠，减轻这些中小企业的监管负担，以促进进步。  

Three-person startups should not have to deal with safety issues that stem from the technology they use as a foundation for their product.  

三人创业公司不应该因为他们产品的基础技术而面临安全问题。  

Governments could further invest in existing AI use cases that are not sufficiently valued by markets but can advance important social values like inclusion and participation.  

政府可以进一步投资于那些市场价值不足但能推动重要社会价值的现有AI应用案例。  

These include AI-driven scientific and medical advances, AI that advances the social good in low income countries, AI tutors that support high school students through high quality 1:1 tutoring, or tools that make it easier for people to participate in public discourse—for instance, through [large language models](https://time.com/6271657/a-to-z-of-artificial-intelligence/) that [help synthesize and map arguments](https://arxiv.org/abs/2306.11932) in a public deliberation process.  

这些案例包括AI驱动的科学和医学进步，AI在低收入国家推动社会福祉的应用，为高中学生提供高质量1对1辅导的AI导师，或者通过大型语言模型帮助合成和整理公共辩论过程中的论点的工具，使人们更容易参与公共话语。

## **Boost research on trustworthy AI  

加强对可信AI的研究**

The strongest AI models are increasingly capable, but [unreliable and potentially harmful](https://www.aiaaic.org/aiaaic-repository).  

最强大的AI模型越来越强大，但不可靠且有潜在危害。  

And while billions of dollars are spent each year to make AI more powerful, funding for research to make AI understandable, free from bias, and safe is tiny in comparison.  

而每年投入数十亿美元使AI更强大的资金相比，用于研究使AI可理解、无偏见和安全的资金微不足道。  

That is why we need a large-scale effort involving the world’s best AI scientists to ensure this technology will continue to benefit humanity instead of harming it.  

这就是为什么我们需要一个大规模的努力，让全球最优秀的AI科学家参与其中，以确保这项技术将继续造福人类而不是伤害人类。  

Such an effort could aim to map all potential risks from AI (similar to what the Intergovernmental Panel on Climate Change does regarding climate change) and propose a research agenda for mitigating each risk—and then invest in research into technical solutions for each risk.  

这样的努力可以致力于绘制出AI的所有潜在风险（类似于政府间气候变化专门委员会对气候变化的做法），并提出减轻每个风险的研究议程，然后投资于针对每个风险的技术解决方案的研究。  

This research would also provide the information required by regulators, for example to identify the more dangerous forms of AI or where to put the threshold for what should be open-source and what shouldn’t.  

这项研究还将为监管机构提供所需的信息，例如识别更危险的AI形式或确定何时应该开放源代码以及何时不应该开放源代码。  

And crucially, this research could enable us to develop [safe, defensive AI models that would](https://www.journalofdemocracy.org/ai-and-catastrophic-risk/) help humanity protect itself in an AI emergency, like the misuse of powerful AIs by terrorists or an out-of-control superhuman rogue AI.  

而且至关重要的是，这项研究可以使我们能够开发出安全的、防御性的AI模型，帮助人类在AI紧急情况下保护自己，例如恐怖分子滥用强大的AI或失控的超级人工智能。  

Importantly, advances in capabilities that could help in such AI defense could become a weapon in the wrong hands.  

重要的是，可以帮助AI防御的能力进步可能会成为错误人手中的武器。  

They should therefore be developed in a secure way that avoids a single point of failure.  

因此，它们应该以安全的方式进行开发，避免单点故障。  

This requires strong democratic governance that includes individual governments, civil society and the international community.  

这需要强大的民主治理，包括各个国家政府、民间社会和国际社会的参与。  

By doing this research, we would ensure that we can continue to reap the benefits of AI safely, embracing participation, while supporting progress.  

通过进行这项研究，我们将确保能够安全地享受AI的好处，鼓励参与，并支持进步。

## **Democratize AI oversight  

普及化AI监管**

Many of the world’s leading AI experts now think that human-level (or even more capable) AI could arrive before 2030. Regardless of the exact timelines, it’s clear that unelected tech leaders should not decide whether, when, how and for what purpose such transformative AI is built.  

如今，许多世界领先的AI专家认为，在2030年之前，人类水平（甚至更高水平）的AI可能会出现。无论确切的时间表如何，很明显，非选举产生的科技领导者不应该决定是否、何时、如何以及出于何种目的构建这种具有变革性的AI。  

This means that, while we might not want to democratize direct access to potentially destructive technology for safety reasons, we urgently need to democratize AI oversight (participation).  

这意味着，虽然出于安全原因，我们可能不希望普及直接接触潜在破坏性技术，但我们迫切需要普及化AI监管（参与）。  

This is beginning to happen. The E.U. and Canada are currently finalizing their first AI legislation.  

这一过程已经开始。欧盟和加拿大目前正在最后阶段制定他们的首部AI立法。

These efforts will not enable regulators to address all risks, but they are a good starting point.

  

这些努力并不能使监管机构解决所有风险，但它们是一个良好的起点。  

In the U.S., [various drafts](https://www.vox.com/future-perfect/23775650/ai-regulation-openai-gpt-anthropic-midjourney-stable) for binding regulation are circulating, including bi-partisan efforts.  

在美国，各种有约束力的监管草案正在流传，包括两党的共同努力。  

But with the current speed of AI development, we do not have time to lose; regulation will have to accelerate and rest on an agile principles-based framework.  

但随着AI发展的速度，我们没有时间可耽搁；监管必须加速，并建立在一个灵活的基于原则的框架之上。  

Voluntary commitments by companies can help in the short term but they are not enough.  

公司的自愿承诺可以在短期内有所帮助，但这还不够。  

And at a global level, we need a [minimal set of binding rules governing AI R&D worldwide](https://arxiv.org/abs/2307.04699). This will not be easy, but it is in every country’s interest to avoid global AI catastrophes.  

在全球范围内，我们需要一套最基本的约束规则来管理全球的AI研发。这并不容易，但每个国家都有兴趣避免全球性的AI灾难。  

A recent [statement](https://www.safe.ai/statement-on-ai-risk#signatories) warning about AI posing an extinction threat to humanity, for instance, was also signed by leading Chinese and Russian researchers.  

最近，一份警告称AI对人类构成灭绝威胁的声明也得到了中国和俄罗斯的领先研究人员的签署。

## **Set up procedures for monitoring and evaluating AI progress  

建立监测和评估AI进展的程序**

With increasingly capable AI models, we will want to know whether somebody in North Korea, for example, is currently using 20,000 AI chips for a huge training run to build their GPT-6. For this reason, governments should consider “[compute monitoring](https://arxiv.org/abs/2303.11341)”: tracking globally who is using the chips needed for building AI models.  

随着越来越强大的AI模型，我们想知道，例如，朝鲜是否正在使用2万个AI芯片进行大规模训练，以构建他们的GPT-6。因此，政府应考虑“计算监测”：全球追踪谁在使用构建AI模型所需的芯片。  

Governments might also want to mandate on-chip devices that signal to the regulator if they are being used for purposes that violate AI R&D rules, while protecting developers’ IP rights.  

政府还可能要求在芯片设备上设置信号，如果它们被用于违反AI研发规则的目的，可以向监管机构发出信号，同时保护开发者的知识产权。  

Compute monitoring could be paired with a licensing regime.  

计算监测可以与许可制度相结合。  

Only certified responsible labs would then be allowed to train the next generation of general-purpose AI models.  

只有经过认证的负责任实验室才能被允许训练下一代通用AI模型。  

This would create transparency and accountability, and it could help solve global coordination problems around AI development.  

这将带来透明度和问责制，并有助于解决围绕AI发展的全球协调问题。  

That’s important because at some point, it might become necessary to temporarily slow down or pause certain types of AI development globally for safety reasons.  

这很重要，因为在某些情况下，出于安全原因，可能需要暂时减缓或暂停全球某些类型的AI发展。  

Knowing who has the amount of AI chips needed for developing potentially dangerous models would make this easier.  

了解谁拥有开发潜在危险模型所需的AI芯片数量将使这一过程更容易。  

Finally, to identify undesirable or even dangerous capabilities in new models, governments should mandate external evaluations: expert “[red-teamers](https://www.washingtonpost.com/technology/2023/08/08/ai-red-team-defcon/)” should test models for risks such as dangerous capabilities, bias, dispositions for privacy violations and hallucination.  

最后，为了识别新模型中的不良或甚至危险能力，政府应该要求进行外部评估：专家“红队”应该测试模型的风险，如危险能力、偏见、侵犯隐私和幻觉倾向。

These four policy goals are not exhaustive, but they would make for a great start.  

这四个政策目标并不详尽，但它们将是一个很好的起点。  

And the best part is that they are mutually compatible, jointly supporting AI progress, safety, and participation.  

最好的部分是它们是相互兼容的，共同支持AI的进步、安全和参与。  

At the upcoming AI Safety Summit in the U.K.  

在即将举行的英国AI安全峰会上  

and in all efforts to regulate AI, governments should focus on these common-sense, pragmatic policies, and not get distracted by false claims that we have to choose between different core values.  

在所有监管AI的努力中，政府应该专注于这些常识、务实的政策，而不要被虚假的说法分散注意力，认为我们必须在不同的核心价值观之间做出选择。  

Yes, people have different opinions about AI regulation and yes, there will be serious disagreements.  

是的，人们对于AI监管有不同的观点，而且确实会存在严重的分歧。  

But we should not forget that mostly, we want similar things.  

但我们不应忘记，大多数情况下，我们都希望达到相似的目标。  

And we can have them all: progress, safety, and democratic participation.  

而且我们可以同时实现这些目标：进步、安全和民主参与。

_Yoshua Bengio_ _is a professor at the Department of Computer Science and Operations Research at the Université de Montréal, scientific director of the Montreal Institute for Learning Algorithms and 2018 Turing Award winner._ _Daniel Privitera_ _is the founder and executive director of the KIRA Center for AI Risks & Impacts and a PhD candidate at the University of Oxford._  

Yoshua Bengio是蒙特利尔大学计算机科学与运筹学系的教授，蒙特利尔机器学习算法研究所的科学主任，也是2018年图灵奖得主。Daniel Privitera是KIRA AI风险与影响中心的创始人和执行主任，同时也是牛津大学的博士候选人。

**Contact us** at [letters@time.com](mailto:letters@time.com?subject=(READER%20FEEDBACK)%20How%20We%20Can%20Have%20AI%20Progress%20Without%20Sacrificing%20Safety%20or%20Democracy&body=https%3A%2F%2Ftime.com%2Fcollection-post%2F6325786%2Fai-progress-safety-democracy%2F).  

联系我们：LETTERS@TIME.COM
