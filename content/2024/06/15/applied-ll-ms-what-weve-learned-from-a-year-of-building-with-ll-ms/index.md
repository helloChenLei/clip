---
title: "æ„å»ºæˆåŠŸLLMäº§å“çš„å®ç”¨æŒ‡å—"
date: 2024-06-15T07:50:25+08:00
updated: 2024-06-15T07:50:25+08:00
taxonomies:
  tags: []
extra:
  source: https://applied-llms.org/
  hostname: applied-llms.org
  author: Shreya Shankar
  original_title: "Applied LLMs - What Weâ€™ve Learned From A Year of Building with LLMs"
  original_lang: en
---

> Also published on Oâ€™Reilly Media in three parts: [Tactical](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/), [Operational](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-ii/), [Strategic](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-iii-strategy/). Also see [podcast](https://lu.ma/e8huz3s6).  
> 
> ä¹Ÿå¯åœ¨ Oâ€™Reilly Media ä¸Šåˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†å‘å¸ƒï¼šæˆ˜æœ¯ã€è¿è¥ã€æˆ˜ç•¥ã€‚å¦å¤–è¯·æŸ¥çœ‹æ’­å®¢ã€‚

Itâ€™s an exciting time to build with large language models (LLMs).  

ç°åœ¨æ­£æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºçš„æ¿€åŠ¨äººå¿ƒæ—¶åˆ»ï¼ˆLLMsï¼‰ã€‚  

Over the past year, LLMs have become â€œgood enoughâ€ for real-world applications.  

åœ¨è¿‡å»çš„ä¸€å¹´é‡Œï¼ŒLLMså·²ç»å˜å¾—â€œè¶³å¤Ÿå¥½â€ä»¥åº”ç”¨äºç°å®ä¸–ç•Œã€‚  

And theyâ€™re getting better and cheaper every year.  

ä»–ä»¬æ¯å¹´éƒ½å˜å¾—æ›´å¥½ï¼Œä»·æ ¼ä¹Ÿæ›´ä¾¿å®œã€‚  

Coupled with a parade of demos on social media, there will be an [estimated $200B investment in AI by 2025](https://www.goldmansachs.com/intelligence/pages/ai-investment-forecast-to-approach-200-billion-globally-by-2025.html).  

é¢„è®¡åˆ° 2025 å¹´ï¼Œå°†æœ‰çº¦ 2000 äº¿ç¾å…ƒçš„æŠ•èµ„è¿›å…¥äººå·¥æ™ºèƒ½é¢†åŸŸï¼ŒåŒæ—¶ä¼´éšç€ç¤¾äº¤åª’ä½“ä¸Šçš„ä¸€ç³»åˆ—æ¼”ç¤ºã€‚  

Furthermore, provider APIs have made LLMs more accessible, allowing everyone, not just ML engineers and scientists, to build intelligence into their products.  

æ­¤å¤–ï¼Œæä¾›è€… API ä½¿LLMsæ›´æ˜“è®¿é—®ï¼Œè®©æ¯ä¸ªäººï¼Œè€Œä¸ä»…ä»…æ˜¯æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆå’Œç§‘å­¦å®¶ï¼Œéƒ½èƒ½å°†æ™ºèƒ½èå…¥å…¶äº§å“ä¸­ã€‚  

Nonetheless, while the barrier to entry for building with AI has been lowered, creating products and systems that are effectiveâ€”beyond a demoâ€”remains deceptively difficult.  

å°½ç®¡ä½¿ç”¨äººå·¥æ™ºèƒ½æ„å»ºçš„å‡†å…¥é—¨æ§›å·²é™ä½ï¼Œä½†è¦åˆ›å»ºæœ‰æ•ˆçš„äº§å“å’Œç³»ç»Ÿâ€”â€”è¶…è¶Šæ¼”ç¤ºâ€”â€”ä»ç„¶å…·æœ‰æ¬ºéª—æ€§çš„éš¾åº¦ã€‚

Weâ€™ve spent the past year building, and have discovered many sharp edges along the way.  

æˆ‘ä»¬èŠ±äº†ä¸€å¹´çš„æ—¶é—´è¿›è¡Œå»ºè®¾ï¼Œæ²¿é€”å‘ç°äº†è®¸å¤šæ£±è§’åˆ†æ˜çš„é—®é¢˜ã€‚  

While we donâ€™t claim to speak for the entire industry, weâ€™d like to share what weâ€™ve learned to help you avoid our mistakes and iterate faster.  

è™½ç„¶æˆ‘ä»¬ä¸æ•¢è‡ªç§°ä»£è¡¨æ•´ä¸ªè¡Œä¸šï¼Œä½†æˆ‘ä»¬æ„¿æ„åˆ†äº«æˆ‘ä»¬æ‰€å­¦åˆ°çš„ç»éªŒï¼Œå¸®åŠ©æ‚¨é¿å…çŠ¯æˆ‘ä»¬çš„é”™è¯¯ï¼Œæ›´å¿«åœ°è¿­ä»£ã€‚  

These are organized into three sections:  

è¿™äº›å†…å®¹è¢«åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š

-   [Tactical](https://applied-llms.org/#tactical-nuts--bolts-of-working-with-llms): Some practices for prompting, RAG, flow engineering, evals, and monitoring.  
    
    æˆ˜æœ¯ï¼šå…³äºæç¤ºã€RAGã€æµç¨‹å·¥ç¨‹ã€è¯„ä¼°å’Œç›‘æ§çš„ä¸€äº›å®è·µæ–¹æ³•ã€‚  
    
    Whether youâ€™re a practitioner building with LLMs, or hacking on weekend projects, this section was written for you.  
    
    æ— è®ºæ‚¨æ˜¯ä¸“ä¸šäººå£«åœ¨LLMsä¸Šè¿›è¡Œæ„å»ºï¼Œè¿˜æ˜¯åœ¨å‘¨æœ«è¿›è¡Œé¡¹ç›®å¼€å‘ï¼Œæœ¬èŠ‚å†…å®¹éƒ½æ˜¯ä¸ºæ‚¨è€Œå†™çš„ã€‚
-   [Operational](https://applied-llms.org/#operation-day-to-day-and-org-concerns): The organizational, day-to-day concerns of shipping products, and how to build an effective team.  
    
    è¿è¥ï¼šæ¶‰åŠè¿é€äº§å“å’Œå»ºç«‹é«˜æ•ˆå›¢é˜Ÿçš„ç»„ç»‡æ—¥å¸¸å…³æ³¨äº‹é¡¹ã€‚  
    
    For product/technical leaders looking to deploy sustainably and reliably.  
    
    é’ˆå¯¹å¸Œæœ›å®ç°å¯æŒç»­å’Œå¯é éƒ¨ç½²çš„äº§å“/æŠ€æœ¯é¢†å¯¼è€…ã€‚
-   [Strategic](https://applied-llms.org/#strategy-building-with-llms-without-getting-out-maneuvered): The long-term, big-picture view, with opinionated takes such as â€œno GPU before PMFâ€ and â€œfocus on the system not the modelâ€, and how to iterate.  
    
    æˆ˜ç•¥ï¼šé•¿æœŸã€å®è§‚è§†è§’ï¼ŒåŒ…æ‹¬â€œåœ¨ PMF ä¹‹å‰ä¸ä½¿ç”¨ GPUâ€å’Œâ€œä¸“æ³¨äºç³»ç»Ÿè€Œéæ¨¡å‹â€ç­‰ä¸»è§‚çœ‹æ³•ï¼Œä»¥åŠå¦‚ä½•è¿­ä»£ã€‚  
    
    Written with founders and executives in mind.  
    
    é’ˆå¯¹åˆ›å§‹äººå’Œé«˜ç®¡æ’°å†™çš„å†…å®¹ã€‚

We intend to make this a practical guide to building successful products with LLMs, drawing from our own experiences and pointing to examples from around the industry.  

æˆ‘ä»¬æ‰“ç®—å°†è¿™æœ¬ä¹¦æ‰“é€ æˆä¸€ä¸ªå®ç”¨æŒ‡å—ï¼Œæ•™ä½ å¦‚ä½•ç”¨LLMsæ‰“é€ æˆåŠŸçš„äº§å“ï¼Œæˆ‘ä»¬å°†ä»è‡ªèº«ç»éªŒä¸­æ±²å–çµæ„Ÿï¼Œå¹¶æŒ‡å‡ºè¡Œä¸šä¸­çš„æ¡ˆä¾‹ã€‚

Ready to ~delve~ dive in? Letâ€™s go.  

å‡†å¤‡å¥½æ·±å…¥æ¢ç´¢äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹å§ã€‚

___

## Tactical: Nuts & Bolts of Working with LLMs  

1 æˆ˜æœ¯ï¼šä¸LLMsåˆä½œçš„åŸºæœ¬è¦é¢†

Here, we share best practices for core components of the emerging LLM stack: prompting tips to improve quality and reliability, evaluation strategies to assess output, retrieval-augmented generation ideas to improve grounding, how to design human-in-the-loop workflows, and more.  

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ†äº«æ–°å…´LLMå †æ ˆæ ¸å¿ƒç»„ä»¶çš„æœ€ä½³å®è·µï¼šæç¤ºæŠ€å·§ä»¥æé«˜è´¨é‡å’Œå¯é æ€§ï¼Œè¯„ä¼°ç­–ç•¥ä»¥è¯„ä¼°è¾“å‡ºï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æƒ³æ³•ä»¥æ”¹è¿›åŸºç¡€ï¼Œå¦‚ä½•è®¾è®¡äººåœ¨å›è·¯å·¥ä½œæµç¨‹ç­‰ã€‚  

While the technology is still nascent, we trust these lessons are broadly applicable and can help you ship robust LLM applications.  

å°½ç®¡æŠ€æœ¯ä»å¤„äºèŒèŠ½é˜¶æ®µï¼Œæˆ‘ä»¬ç›¸ä¿¡è¿™äº›ç»éªŒæ•™è®­å…·æœ‰å¹¿æ³›é€‚ç”¨æ€§ï¼Œå¯ä»¥å¸®åŠ©æ‚¨å¼€å‘å‡ºç¨³å¥çš„LLMåº”ç”¨ç¨‹åºã€‚

## Prompting  

1.1 æç¤º[](https://applied-llms.org/#prompting)

We recommend starting with prompting when prototyping new applications.  

æˆ‘ä»¬å»ºè®®åœ¨åŸå‹è®¾è®¡æ–°åº”ç”¨ç¨‹åºæ—¶ä»æç¤ºå¼€å§‹ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½åœ°ç†è§£å’Œå®ç°ã€‚  

Itâ€™s easy to both underestimate and overestimate its importance.  

äººä»¬å¾ˆå®¹æ˜“æ—¢ä½ä¼°åˆé«˜ä¼°å®ƒçš„é‡è¦æ€§ã€‚  

Itâ€™s underestimated because the right prompting techniques, when used correctly, can get us very far.  

å®ƒè¢«ä½ä¼°äº†ï¼Œå› ä¸ºæ­£ç¡®çš„æç¤ºæŠ€å·§ï¼Œå¦‚æœæ­£ç¡®ä½¿ç”¨ï¼Œå¯ä»¥è®©æˆ‘ä»¬å–å¾—å¾ˆå¤§è¿›å±•ã€‚  

Itâ€™s overestimated because even prompt-based applications require significant engineering around the prompt to work well.  

å®ƒè¢«é«˜ä¼°äº†ï¼Œå› ä¸ºå³ä½¿æ˜¯åŸºäºæç¤ºçš„åº”ç”¨ç¨‹åºï¼Œä¹Ÿéœ€è¦å›´ç»•æç¤ºè¿›è¡Œé‡è¦çš„å·¥ç¨‹å·¥ä½œæ‰èƒ½è¿è¡Œè‰¯å¥½ã€‚

### Focus on getting the most out of fundamental prompting techniques  

1.1.1 ä¸“æ³¨äºå……åˆ†åˆ©ç”¨åŸºç¡€æç¤ºæŠ€å·§[](https://applied-llms.org/#focus-on-getting-the-most-out-of-fundamental-prompting-techniques)

A few prompting techniques have consistently helped with improving performance across a variety of models and tasks: n-shot prompts + in-context learning, chain-of-thought, and providing relevant resources.  

ä¸€äº›æç¤ºæŠ€å·§ä¸€ç›´ä»¥æ¥éƒ½åœ¨æ”¹å–„å„ç§æ¨¡å‹å’Œä»»åŠ¡çš„æ€§èƒ½æ–¹é¢å‘æŒ¥ç€ä¸€è‡´çš„ä½œç”¨ï¼šn-shot æç¤º+ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæ€ç»´é“¾ï¼Œä»¥åŠæä¾›ç›¸å…³èµ„æºã€‚

The idea of in-context learning via n-shot prompts is to provide the LLM with examples that demonstrate the task and align outputs to our expectations.  

é€šè¿‡ n-shot æç¤ºè¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ çš„ç†å¿µæ˜¯ä¸º LLM æä¾›ç¤ºä¾‹ï¼Œå±•ç¤ºä»»åŠ¡å¹¶ä½¿è¾“å‡ºç¬¦åˆæˆ‘ä»¬çš„æœŸæœ›ã€‚  

A few tips:  

ä¸€äº›å»ºè®®ï¼š

-   If n is too low, the model may over-anchor on those specific examples, hurting its ability to generalize.  
    
    å¦‚æœ n å€¼è¿‡ä½ï¼Œæ¨¡å‹å¯èƒ½ä¼šè¿‡åº¦ä¾èµ–è¿™äº›ç‰¹å®šç¤ºä¾‹ï¼Œä»è€ŒæŸå®³å…¶æ³›åŒ–èƒ½åŠ›ã€‚  
    
    As a rule of thumb, aim for n â‰¥ 5. Donâ€™t be afraid to go as high as a few dozen.  
    
    ä¸€èˆ¬æ¥è¯´ï¼Œç›®æ ‡æ˜¯ n â‰¥ 5 æ˜¯ä¸€ä¸ªç»éªŒæ³•åˆ™ã€‚ä¸è¦å®³æ€•å°†æ•°å­—æé«˜åˆ°å‡ åã€‚  
    
-   Examples should be representative of the prod distribution.  
    
    ç¤ºä¾‹åº”å½“å…·æœ‰ä»£è¡¨æ€§ï¼Œåæ˜ äº§å“åˆ†å‘æƒ…å†µã€‚  
    
    If youâ€™re building a movie summarizer, include samples from different genres in roughly the same proportion youâ€™d expect to see in practice.  
    
    å¦‚æœæ‚¨æ­£åœ¨å¼€å‘ç”µå½±æ‘˜è¦ç”Ÿæˆå™¨ï¼Œè¯·ç¡®ä¿æ ·æœ¬æ¶µç›–ä¸åŒæµæ´¾ï¼Œæ¯”ä¾‹ä¸å®é™…æƒ…å†µå¤§è‡´ç›¸ç¬¦ã€‚  
    
-   You donâ€™t always need to provide the input-output pairs; examples of desired outputs may be sufficient.  
    
    æ‚¨å¹¶éæ€»éœ€è¦æä¾›è¾“å…¥è¾“å‡ºå¯¹ï¼›æä¾›æ‰€éœ€è¾“å‡ºçš„ç¤ºä¾‹å¯èƒ½å°±è¶³å¤Ÿäº†ã€‚  
    
-   If you plan for the LLM to use tools, include examples of using those tools.  
    
    å¦‚æœæ‚¨æ‰“ç®—è®©LLMä½¿ç”¨å·¥å…·ï¼Œè¯·æä¾›ä½¿ç”¨è¿™äº›å·¥å…·çš„ç¤ºä¾‹ã€‚

In Chain-of-Thought (CoT) prompting, we encourage the LLM to explain its thought process before returning the final answer.  

åœ¨æ€ç»´é“¾ (CoT) æç¤ºä¸­ï¼Œæˆ‘ä»¬é¼“åŠ± LLM åœ¨ç»™å‡ºæœ€ç»ˆç­”æ¡ˆä¹‹å‰è§£é‡Šä»–ä»¬çš„æ€è€ƒè¿‡ç¨‹ã€‚  

Think of it as providing the LLM with a sketchpad so it doesnâ€™t have to do it all in memory.  

å°†å…¶è§†ä¸ºä¸ºLLMæä¾›ç´ ææœ¬ï¼Œè¿™æ ·å®ƒå°±ä¸å¿…å…¨éƒ¨ä¾èµ–å†…å­˜ã€‚  

The original approach was to simply add the phrase â€œLetâ€™s think step by stepâ€ as part of the instructions, but, weâ€™ve found it helpful to make the CoT more specific, where adding specificity via an extra sentence or two often reduces hallucination rates significantly.  

æœ€åˆçš„æ–¹æ³•æ˜¯ç®€å•åœ°å°†çŸ­è¯­â€œè®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥åœ°æ€è€ƒâ€æ·»åŠ åˆ°è¯´æ˜ä¸­ï¼Œä½†æˆ‘ä»¬å‘ç°é€šè¿‡å¢åŠ ä¸€ä¸¤å¥é¢å¤–çš„å…·ä½“å†…å®¹ï¼Œé€šå¸¸å¯ä»¥æ˜¾è‘—é™ä½å¹»è§‰ç‡ã€‚

For example, when asking an LLM to summarize a meeting transcript, we can be explicit about the steps:  

ä¾‹å¦‚ï¼Œå½“è¦æ±‚ä¸€ä¸ªLLMæ€»ç»“ä¼šè®®è®°å½•æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æ˜ç¡®è¯´æ˜æ­¥éª¤ï¼š

-   First, list out the key decisions, follow-up items, and associated owners in a sketchpad.  
    
    é¦–å…ˆï¼Œåœ¨è‰å›¾ä¸­åˆ—å‡ºå…³é”®å†³ç­–ã€åç»­äº‹é¡¹å’Œç›¸å…³è´£ä»»äººã€‚  
    
-   Then, check that the details in the sketchpad are factually consistent with the transcript.  
    
    ç„¶åï¼Œè¯·æ£€æŸ¥ç´ ææœ¬ä¸­çš„ç»†èŠ‚æ˜¯å¦ä¸æˆç»©å•å†…å®¹ä¸€è‡´ã€‚  
    
-   Finally, synthesize the key points into a concise summary.  
    
    æœ€åï¼Œå°†å…³é”®è¦ç‚¹ç»¼åˆèµ·æ¥ï¼Œå½¢æˆç®€æ˜æ‰¼è¦çš„æ€»ç»“ã€‚

Note that in recent times, [some doubt](https://arxiv.org/abs/2405.04776) has been cast on if this technique is as powerful as believed.  

è¿‘å¹´æ¥ï¼Œä¸€äº›äººå¼€å§‹æ€€ç–‘è¿™ç§æŠ€æœ¯æ˜¯å¦çœŸçš„åƒäººä»¬æ‰€è®¤ä¸ºçš„é‚£æ ·å¼ºå¤§ã€‚  

Additionally, thereâ€™s significant debate as to exactly what is going on during inference when Chain-of-Thought is being used.  

æ­¤å¤–ï¼Œåœ¨ä½¿ç”¨â€œæ€ç»´é“¾â€è¿›è¡Œæ¨ç†æ—¶ï¼Œç©¶ç«Ÿå‘ç”Ÿäº†ä»€ä¹ˆå­˜åœ¨ç€é‡å¤§äº‰è®®ã€‚  

Regardless, this technique is one to experiment with when possible.  

æ— è®ºå¦‚ä½•ï¼Œè¿™é¡¹æŠ€æœ¯æ˜¯å€¼å¾—å°è¯•çš„ã€‚

Providing relevant resources is a powerful mechanism to expand the modelâ€™s knowledge base, reduce hallucinations, and increase the userâ€™s trust. Often accomplished via Retrieval Augmented Generation (RAG), providing the model with snippets of text that it can directly utilize in its response is an essential technique.  

æä¾›ç›¸å…³èµ„æºæ˜¯æ‰©å±•æ¨¡å‹çŸ¥è¯†åº“ã€å‡å°‘å¹»è§‰å¹¶å¢åŠ ç”¨æˆ·ä¿¡ä»»çš„å¼ºå¤§æœºåˆ¶ã€‚é€šå¸¸é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¥å®ç°ï¼Œå‘æ¨¡å‹æä¾›æ–‡æœ¬ç‰‡æ®µï¼Œä½¿å…¶èƒ½ç›´æ¥åœ¨å›åº”ä¸­ä½¿ç”¨ï¼Œè¿™æ˜¯ä¸€ç§é‡è¦çš„æŠ€æœ¯ã€‚  

When providing the relevant resources, itâ€™s not enough to merely include them; donâ€™t forget to tell the model to prioritize their use, refer to them directly, and to mention when none of the resources are sufficient.  

åœ¨æä¾›ç›¸å…³èµ„æºæ—¶ï¼Œä»…ä»…åŒ…å«å®ƒä»¬æ˜¯ä¸å¤Ÿçš„ï¼›ä¸è¦å¿˜è®°å‘Šè¯‰æ¨¡å‹ä¼˜å…ˆè€ƒè™‘å®ƒä»¬çš„ä½¿ç”¨ï¼Œç›´æ¥å¼•ç”¨å®ƒä»¬ï¼Œå¹¶åœ¨æ²¡æœ‰è¶³å¤Ÿèµ„æºæ—¶è¿›è¡Œè¯´æ˜ã€‚  

These help â€œgroundâ€ agent responses to a corpus of resources.  

è¿™äº›æœ‰åŠ©äºä»£ç†å“åº”èµ„æºè¯­æ–™åº“æ—¶â€œæ‰æ ¹â€ã€‚

### Structure your inputs and outputs  

1.1.2 è§„åˆ’å¥½æ‚¨çš„è¾“å…¥å’Œè¾“å‡º[](https://applied-llms.org/#structure-your-inputs-and-outputs)

Structured input and output help models better understand the input as well as return output that can reliably integrate with downstream systems. Adding serialization formatting to your inputs can help provide more clues to the model as to the relationships between tokens in the context, additional metadata to specific tokens (like types), or relate the request to similar examples in the modelâ€™s training data.  

ç»“æ„åŒ–çš„è¾“å…¥å’Œè¾“å‡ºæœ‰åŠ©äºæ¨¡å‹æ›´å¥½åœ°ç†è§£è¾“å…¥ï¼Œå¹¶ç”Ÿæˆå¯é åœ°ä¸ä¸‹æ¸¸ç³»ç»Ÿé›†æˆçš„è¾“å‡ºã€‚å°†åºåˆ—åŒ–æ ¼å¼åº”ç”¨äºæ‚¨çš„è¾“å…¥å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡ä¸­ä»¤ç‰Œä¹‹é—´çš„å…³ç³»ï¼Œä¸ºç‰¹å®šä»¤ç‰Œæä¾›é¢å¤–çš„å…ƒæ•°æ®ï¼ˆå¦‚ç±»å‹ï¼‰ï¼Œæˆ–å°†è¯·æ±‚ä¸æ¨¡å‹è®­ç»ƒæ•°æ®ä¸­çš„ç±»ä¼¼ç¤ºä¾‹ç›¸å…³è”ã€‚

As an example, many questions on the internet about writing SQL begin by specifying the SQL schema.  

ä¸¾ä¾‹æ¥è¯´ï¼Œè®¸å¤šå…³äºåœ¨äº’è”ç½‘ä¸Šç¼–å†™ SQL çš„é—®é¢˜éƒ½æ˜¯ä»æŒ‡å®š SQL æ¨¡å¼å¼€å§‹çš„ã€‚  

Thus, you can expect that effective prompting for Text-to-SQL should include [structured schema definitions](https://www.researchgate.net/publication/371223615_SQL-PaLM_Improved_Large_Language_ModelAdaptation_for_Text-to-SQL).  

å› æ­¤ï¼Œæ‚¨å¯ä»¥æœŸå¾…ï¼Œæœ‰æ•ˆçš„æ–‡æœ¬åˆ° SQL æç¤ºåº”è¯¥åŒ…æ‹¬ç»“æ„åŒ–æ¨¡å¼å®šä¹‰ã€‚

Structured input expresses tasks clearly and resembles how the training data is formatted, increasing the probability of better output.  

ç»“æ„åŒ–è¾“å…¥æ¸…æ™°åœ°è¡¨è¾¾ä»»åŠ¡ï¼Œç±»ä¼¼äºè®­ç»ƒæ•°æ®çš„æ ¼å¼ï¼Œæé«˜äº†è·å¾—æ›´å¥½è¾“å‡ºçš„æ¦‚ç‡ã€‚  

Structured output simplifies integration into downstream components of your system. [Instructor](https://github.com/jxnl/instructor) and [Outlines](https://github.com/outlines-dev/outlines) work well for structured output.  

ç»“æ„åŒ–è¾“å‡ºç®€åŒ–äº†é›†æˆåˆ°ç³»ç»Ÿä¸‹æ¸¸ç»„ä»¶ä¸­çš„è¿‡ç¨‹ã€‚è®²å¸ˆå’Œå¤§çº²å¯¹ç»“æ„åŒ–è¾“å‡ºéå¸¸æœ‰æ•ˆã€‚  

(If youâ€™re importing an LLM API SDK, use Instructor; if youâ€™re importing Huggingface for a self-hosted model, use Outlines.)  

å¦‚æœæ‚¨å¯¼å…¥çš„æ˜¯LLM API SDKï¼Œè¯·ä½¿ç”¨ Instructorï¼›å¦‚æœæ‚¨å¯¼å…¥çš„æ˜¯ Huggingface ç”¨äºè‡ªæ‰˜ç®¡æ¨¡å‹ï¼Œè¯·ä½¿ç”¨ Outlinesã€‚

When using structured input, be aware that each LLM family has their own preferences.  

åœ¨ä½¿ç”¨ç»“æ„åŒ–è¾“å…¥æ—¶ï¼Œè¯·æ³¨æ„æ¯ä¸ªLLMå®¶åº­éƒ½æœ‰ä»–ä»¬è‡ªå·±çš„åå¥½ã€‚  

Claude prefers `<xml>` while GPT favors Markdown and JSON.  

Claude åçˆ± `<xml>` ï¼Œè€Œ GPT æ›´å–œæ¬¢ä½¿ç”¨ Markdown å’Œ JSON æ ¼å¼ã€‚  

With XML, you can even pre-fill Claudeâ€™s responses by providing a `<response>` tag like so.  

ä½¿ç”¨ XMLï¼Œæ‚¨ç”šè‡³å¯ä»¥é€šè¿‡æä¾›ç±»ä¼¼äº `<response>` æ ‡è®°çš„æ–¹å¼æ¥é¢„å…ˆå¡«å†™ Claude çš„å“åº”ã€‚

```
messages=[
    {
        "role": "user",
        "content": """Extract the <name>, <size>, <price>, and <color> from this product description into your <response>.
            <description>The SmartHome Mini is a compact smart home assistant available in black or white for only $49.99. At just 5 inches wide, it lets you control lights, thermostats, and other connected devices via voice or appâ€”no matter where you place it in your home. This affordable little hub brings convenient hands-free control to your smart devices.
            </description>"""
    },
    {
        "role": "assistant",
        "content": "<response><name>"
    }
]
```

### Have small prompts that do one thing, and only one thing, well  

1.1.3 è®¾ç½®ç®€æ´æ˜äº†çš„å°æç¤ºï¼Œæ¯ä¸ªæç¤ºåªå®Œæˆä¸€é¡¹ä»»åŠ¡ï¼Œè€Œä¸”åšåˆ°æœ€å¥½[](https://applied-llms.org/#have-small-prompts-that-do-one-thing-and-only-one-thing-well)

A common anti-pattern / code smell in software is the â€œ[God Object](https://en.wikipedia.org/wiki/God_object)â€, where we have a single class or function that does everything. The same applies to prompts too.  

è½¯ä»¶å¼€å‘ä¸­å¸¸è§çš„åæ¨¡å¼/ä»£ç å¼‚å‘³æ˜¯â€œä¸Šå¸å¯¹è±¡â€ï¼Œå³ä¸€ä¸ªç±»æˆ–å‡½æ•°è´Ÿè´£è¿‡å¤šåŠŸèƒ½ã€‚æç¤ºä¹Ÿæ˜¯å¦‚æ­¤ã€‚

A prompt typically starts simple: A few sentences of instruction, a couple of examples, and weâ€™re good to go.  

ä¸€ä¸ªæç¤ºé€šå¸¸ä»ç®€å•å¼€å§‹ï¼šå‡ å¥æŒ‡å¯¼ï¼Œå‡ ä¸ªä¾‹å­ï¼Œå°±å¯ä»¥é¡ºåˆ©è¿›è¡Œã€‚  

But as we try to improve performance and handle more edge cases, complexity creeps in.  

ç„¶è€Œï¼Œéšç€æˆ‘ä»¬åŠªåŠ›æé«˜æ€§èƒ½å¹¶å¤„ç†æ›´å¤šè¾¹ç¼˜æƒ…å†µï¼Œå¤æ‚æ€§ä¹Ÿéšä¹‹å¢åŠ ã€‚  

More instructions. Multi-step reasoning. Dozens of examples.  

æ›´å¤šè¯´æ˜ã€‚å¤šæ­¥æ¨ç†ã€‚æ•°åä¸ªä¾‹å­ã€‚  

Before we know it, our initially simple prompt is now a 2,000 token Frankenstein.  

è½¬çœ¼é—´ï¼Œæˆ‘ä»¬æœ€åˆç®€å•çš„æç¤ºå·²ç»å˜æˆäº†ä¸€ä¸ªåŒ…å« 2,000 ä¸ªæ ‡è®°çš„â€œå¼—å…°è‚¯æ–¯å¦â€ã€‚  

And to add injury to insult, it has worse performance on the more common and straightforward inputs!  

æ›´è®©äººæ°”é¦çš„æ˜¯ï¼Œå®ƒåœ¨æ›´ä¸ºå¸¸è§å’Œç›´æ¥çš„è¾“å…¥ä¸Šè¡¨ç°æ›´ç³Ÿç³•ï¼  

GoDaddy shared this challenge as their [No.Â 1 lesson from building with LLMs](https://www.godaddy.com/resources/news/llm-from-the-trenches-10-lessons-learned-operationalizing-models-at-godaddy#h-1-sometimes-one-prompt-isn-t-enough).  

GoDaddy å°†è¿™ä¸€æŒ‘æˆ˜åˆ†äº«ä¸ºä»–ä»¬ä»LLMsæ„å»ºä¸­å¾—åˆ°çš„ç¬¬ä¸€è¯¾ã€‚

Just like how we strive (read: struggle) to keep our systems and code simple, so should we for our prompts.  

å°±åƒæˆ‘ä»¬åŠªåŠ›ï¼ˆå³ï¼šæŒ£æ‰ï¼‰ä¿æŒç³»ç»Ÿå’Œä»£ç ç®€å•ä¸€æ ·ï¼Œæˆ‘ä»¬çš„æç¤ºä¹Ÿåº”è¯¥å¦‚æ­¤ã€‚  

Instead of having a single, catch-all prompt for the meeting transcript summarizer, we can break it into steps:  

ä¸ºäº†é¿å…ä¼šè®®è®°å½•æ‘˜è¦ç”Ÿæˆå™¨åªæœ‰ä¸€ä¸ªå…¨èƒ½æç¤ºï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶åˆ†è§£ä¸ºå‡ ä¸ªæ­¥éª¤

-   Extract key decisions, action items, and owners into structured format  
    
    æå–å…³é”®å†³ç­–ã€è¡ŒåŠ¨é¡¹å’Œè´£ä»»äººï¼Œå¹¶ä»¥ç»“æ„åŒ–æ ¼å¼å‘ˆç°
-   Check extracted details against the original transcription for consistency  
    
    æ£€æŸ¥æå–çš„ç»†èŠ‚ä¸åŸå§‹è½¬å½•è¿›è¡Œå¯¹ç…§ï¼Œç¡®ä¿ä¸€è‡´
-   Generate a concise summary from the structured details  
    
    ä»ç»“æ„åŒ–ç»†èŠ‚ä¸­ç”Ÿæˆç®€æ´çš„æ‘˜è¦

As a result, weâ€™ve split our single prompt into multiple prompts that are each simple, focused, and easy to understand.  

å› æ­¤ï¼Œæˆ‘ä»¬å°†å•ä¸ªæç¤ºæ‹†åˆ†ä¸ºå¤šä¸ªç®€å•ã€ä¸“æ³¨ä¸”æ˜“äºç†è§£çš„æç¤ºã€‚  

And by breaking them up, we can now iterate and eval each prompt individually.  

é€šè¿‡æ‹†åˆ†å®ƒä»¬ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥é€ä¸ªè¿­ä»£å’Œè¯„ä¼°æ¯ä¸ªæç¤ºï¼Œä½¿å¾—å¤„ç†æ›´åŠ ç®€å•ã€‚

### Craft your context tokens  

åˆ¶ä½œæ‚¨çš„ä¸Šä¸‹æ–‡æ ‡è®°[](https://applied-llms.org/#craft-your-context-tokens)

Rethink, and challenge your assumptions about how much context you actually need to send to the agent.  

é‡æ–°æ€è€ƒï¼Œå¹¶æŒ‘æˆ˜æ‚¨å¯¹å®é™…éœ€è¦å‘é€ç»™ä»£ç†çš„ä¸Šä¸‹æ–‡é‡çš„å‡è®¾ã€‚  

Be like Michaelangelo, do not build up your context sculptureâ€”chisel away the superfluous material until the sculpture is revealed.  

åƒç±³å¼€æœ—åŸºç½—ä¸€æ ·ï¼Œä¸è¦ä¸€å‘³å¢åŠ ä¸Šä¸‹æ–‡é›•å¡‘çš„ææ–™ - è€Œæ˜¯å»é™¤å¤šä½™çš„éƒ¨åˆ†ï¼Œç›´åˆ°é›•å¡‘è¢«å±•ç°å‡ºæ¥ã€‚  

RAG is a popular way to collate all of the potentially relevant blocks of marble, but what are you doing to extract whatâ€™s necessary?  

RAG æ˜¯ä¸€ç§æµè¡Œçš„æ–¹æ³•ï¼Œç”¨äºæ•´ç†æ‰€æœ‰æ½œåœ¨ç›¸å…³çš„å¤§ç†çŸ³å—ï¼Œä½†æ‚¨åˆåœ¨åšä»€ä¹ˆæ¥æå–å¿…è¦çš„å†…å®¹å‘¢ï¼Ÿ

Weâ€™ve found that taking the final prompt sent to the modelâ€”with all of the context construction, and meta-prompting, and RAG resultsâ€”putting it on a blank page and just reading it, really helps you rethink your context.  

æˆ‘ä»¬å‘ç°ï¼Œå°†æ¨¡å‹æ”¶åˆ°çš„æœ€ç»ˆæç¤ºâ€”â€”åŒ…æ‹¬æ‰€æœ‰çš„ä¸Šä¸‹æ–‡æ„å»ºã€å…ƒæç¤ºå’Œ RAG ç»“æœâ€”â€”æ”¾åœ¨ç©ºç™½é¡µé¢ä¸Šï¼Œç„¶åé˜…è¯»ï¼Œç¡®å®æœ‰åŠ©äºé‡æ–°æ€è€ƒæ‚¨çš„ä¸Šä¸‹æ–‡ã€‚  

We have found redundancy, self-contradictory language, and poor formatting using this method.  

æˆ‘ä»¬å‘ç°ä½¿ç”¨è¿™ç§æ–¹æ³•å­˜åœ¨å†—ä½™ã€è‡ªç›¸çŸ›ç›¾çš„è¯­è¨€ä»¥åŠæ ¼å¼ä¸ä½³ã€‚

The other key optimization is the structure of your context.  

å¦ä¸€ä¸ªå…³é”®çš„ä¼˜åŒ–æ˜¯è°ƒæ•´æ‚¨ä¸Šä¸‹æ–‡çš„ç»“æ„ã€‚  

If your bag-of-docs representation isnâ€™t helpful for humans, donâ€™t assume itâ€™s any good for agents.  

å¦‚æœæ‚¨çš„æ–‡æ¡£åŒ…è¡¨ç¤ºå¯¹äººç±»æ²¡æœ‰å¸®åŠ©ï¼Œä¸è¦è®¤ä¸ºå®ƒå¯¹ä»£ç†äººæœ‰ä»»ä½•å¥½å¤„ã€‚  

Think carefully about how you structure your context to underscore the relationships between parts of it and make extraction as simple as possible.  

ä»”ç»†æ€è€ƒå¦‚ä½•æ„å»ºä¸Šä¸‹æ–‡ï¼Œçªå‡ºå„éƒ¨åˆ†ä¹‹é—´çš„å…³ç³»ï¼Œä½¿æå–å˜å¾—å°½å¯èƒ½ç®€å•ã€‚

More [prompting fundamentals](https://eugeneyan.com/writing/prompting/) such as prompting mental model, prefilling, context placement, etc.  

æ›´å¤šæç¤ºåŸºç¡€çŸ¥è¯†ï¼Œä¾‹å¦‚æç¤ºå¿ƒç†æ¨¡å‹ã€é¢„å¡«å……ã€ä¸Šä¸‹æ–‡æ”¾ç½®ç­‰ã€‚

## Information Retrieval / RAG  

1.2 ä¿¡æ¯æ£€ç´¢ / RAG[](https://applied-llms.org/#information-retrieval-rag)

Beyond prompting, another effective way to steer an LLM is by providing knowledge as part of the prompt.  

é™¤äº†æç¤ºä¹‹å¤–ï¼Œå¦ä¸€ç§æœ‰æ•ˆçš„å¼•å¯¼LLMçš„æ–¹å¼æ˜¯åœ¨æç¤ºä¸­æä¾›çŸ¥è¯†ã€‚  

This grounds the LLM on the provided context which is then used for in-context learning.  

è¿™å°†åœ¨æä¾›çš„ä¸Šä¸‹æ–‡ä¸­å¯¹LLMè¿›è¡ŒåŸºç¡€ï¼Œç„¶åç”¨äºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚  

This is known as retrieval-augmented generation (RAG).  

è¿™è¢«ç§°ä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚  

Practitioners have found RAG effective at providing knowledge and improving output, while requiring far less effort and cost compared to finetuning.  

ä»ä¸šè€…å‘ç°ï¼ŒRAG åœ¨æä¾›çŸ¥è¯†å’Œæ”¹å–„äº§å‡ºæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œè€Œä¸”ç›¸è¾ƒäºå¾®è°ƒï¼Œæ‰€éœ€çš„å·¥ä½œé‡å’Œæˆæœ¬è¦å°‘å¾—å¤šã€‚

### RAG is only as good as the retrieved documentsâ€™ relevance, density, and detail  

1.2.1 RAG çš„å¥½åå–å†³äºæ£€ç´¢æ–‡æ¡£çš„ç›¸å…³æ€§ã€å¯†åº¦å’Œç»†èŠ‚[](https://applied-llms.org/#rag-is-only-as-good-as-the-retrieved-documents-relevance-density-and-detail)

The quality of your RAGâ€™s output is dependent on the quality of retrieved documents, which in turn can be considered along a few factors  

æ‚¨çš„ RAG è¾“å‡ºè´¨é‡å–å†³äºæ£€ç´¢æ–‡æ¡£çš„è´¨é‡ï¼Œè€Œè¿™åˆå¯ä»¥è€ƒè™‘å‡ ä¸ªå› ç´ 

The first and most obvious metric is relevance.  

é¦–è¦ä¸”æœ€æ˜æ˜¾çš„æŒ‡æ ‡æ˜¯ç›¸å…³æ€§ã€‚  

This is typically quantified via ranking metrics such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank) or [Normalized Discounted Cumulative Gain (NDCG)](https://en.wikipedia.org/wiki/Discounted_cumulative_gain).  

é€šå¸¸æ˜¯é€šè¿‡æ’åæŒ‡æ ‡ï¼ˆå¦‚å¹³å‡å€’æ•°æ’åï¼ˆMRRï¼‰æˆ–å½’ä¸€åŒ–æŠ˜ç°ç´¯ç§¯å¢ç›Šï¼ˆNDCGï¼‰ï¼‰æ¥è¡¡é‡çš„ã€‚  

MRR evaluates how well a system places the first relevant result in a ranked list while NDCG considers the relevance of all the results and their positions.  

MRR è¯„ä¼°ç³»ç»Ÿåœ¨æ’ååˆ—è¡¨ä¸­æ”¾ç½®ç¬¬ä¸€ä¸ªç›¸å…³ç»“æœçš„æ•ˆæœï¼Œè€Œ NDCG åˆ™è€ƒè™‘æ‰€æœ‰ç»“æœçš„ç›¸å…³æ€§åŠå…¶ä½ç½®ã€‚  

They measure how good the system is at ranking relevant documents higher and irrelevant documents lower.  

ä»–ä»¬è¯„ä¼°ç³»ç»Ÿåœ¨å°†ç›¸å…³æ–‡æ¡£æ’åé å‰ã€ä¸ç›¸å…³æ–‡æ¡£æ’åé åæ–¹é¢çš„è¡¨ç°ã€‚  

For example, if weâ€™re retrieving user summaries to generate movie review summaries, weâ€™ll want to rank reviews for the specific movie higher while excluding reviews for other movies.  

ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬è¦è·å–ç”¨æˆ·æ‘˜è¦ä»¥ç”Ÿæˆç”µå½±è¯„è®ºæ‘˜è¦ï¼Œæˆ‘ä»¬ä¼šå¸Œæœ›å¯¹ç‰¹å®šç”µå½±çš„è¯„è®ºè¿›è¡Œæ›´é«˜æ’åï¼ŒåŒæ—¶æ’é™¤å…¶ä»–ç”µå½±çš„è¯„è®ºã€‚

Like traditional recommendation systems, the rank of retrieved items will have a significant impact on how the LLM performs on downstream tasks.  

å°±åƒä¼ ç»Ÿçš„æ¨èç³»ç»Ÿä¸€æ ·ï¼Œæ£€ç´¢åˆ°çš„ç‰©å“æ’åå¯¹LLMåœ¨åç»­ä»»åŠ¡ä¸­çš„è¡¨ç°æœ‰ç€é‡è¦å½±å“ã€‚  

To measure the impact, run a RAG-based task but with the retrieved items shuffledâ€”how does the RAG output perform?  

ä¸ºäº†è¯„ä¼°å½±å“ï¼Œè¿è¡Œä¸€ä¸ªåŸºäº RAG çš„ä»»åŠ¡ï¼Œä½†æ˜¯å°†æ£€ç´¢åˆ°çš„é¡¹ç›®æ‰“ä¹±é¡ºåºâ€”â€”RAG è¾“å‡ºçš„è¡¨ç°å¦‚ä½•ï¼Ÿ

Second, we also want to consider information density.  

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è€ƒè™‘ä¿¡æ¯çš„å¯†åº¦ã€‚  

If two documents are equally relevant, we should prefer one thatâ€™s more concise and has fewer extraneous details.  

å¦‚æœä¸¤ä¸ªæ–‡æ¡£åŒç­‰ç›¸å…³ï¼Œæˆ‘ä»¬åº”è¯¥é€‰æ‹©æ›´ç®€æ´ã€æ›´å°‘æ— å…³ç»†èŠ‚çš„é‚£ä¸€ä¸ªã€‚  

Returning to our movie example, we might consider the movie transcript and all user reviews to be relevant in a broad sense.  

å›åˆ°æˆ‘ä»¬çš„ç”µå½±ç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯èƒ½è®¤ä¸ºç”µå½±å‰§æœ¬å’Œæ‰€æœ‰ç”¨æˆ·è¯„è®ºåœ¨å¹¿ä¹‰ä¸Šæ˜¯ç›¸å…³çš„ã€‚  

Nonetheless, the top-rated reviews and editorial reviews will likely be more dense in information.  

å°½ç®¡å¦‚æ­¤ï¼Œé¡¶çº§è¯„ä»·å’Œç¼–è¾‘è¯„è®ºå¾ˆå¯èƒ½ä¼šæ›´åŠ è¯¦ç»†ä¸°å¯Œã€‚

Finally, consider the level of detail provided in the document.  

æœ€åï¼Œè¯·è€ƒè™‘æ–‡æ¡£ä¸­æä¾›çš„è¯¦ç»†ç¨‹åº¦ã€‚  

Imagine weâ€™re building a RAG system to generate SQL queries from natural language.  

æƒ³è±¡æˆ‘ä»¬æ­£åœ¨å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿä»è‡ªç„¶è¯­è¨€ç”Ÿæˆ SQL æŸ¥è¯¢çš„ RAG ç³»ç»Ÿã€‚  

We could simply provide table schemas with column names as context.  

æˆ‘ä»¬å¯ä»¥ç®€å•åœ°æä¾›å¸¦æœ‰åˆ—åä½œä¸ºä¸Šä¸‹æ–‡çš„è¡¨ç»“æ„ã€‚  

But, what if we include column descriptions and some representative values?  

ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬åŠ å…¥åˆ—æè¿°å’Œä¸€äº›ä»£è¡¨æ€§æ•°å€¼å‘¢ï¼Ÿ  

The additional detail could help the LLM better understand the semantics of the table and thus generate more correct SQL.  

é™„åŠ ç»†èŠ‚å¯ä»¥å¸®åŠ©LLMæ›´å¥½åœ°ç†è§£è¡¨çš„è¯­ä¹‰ï¼Œè¿›è€Œç”Ÿæˆæ›´å‡†ç¡®çš„ SQL è¯­å¥ã€‚

### Donâ€™t forget keyword search; use it as a baseline and in hybrid search  

1.2.2 ä¸è¦å¿˜è®°å…³é”®å­—æœç´¢ï¼›å°†å…¶ä½œä¸ºåŸºå‡†å¹¶åœ¨æ··åˆæœç´¢ä¸­ä½¿ç”¨[](https://applied-llms.org/#dont-forget-keyword-search-use-it-as-a-baseline-and-in-hybrid-search)

Given how prevalent the embedding-based RAG demo is, itâ€™s easy to forget or overlook the decades of research and solutions in information retrieval.  

è€ƒè™‘åˆ°åŸºäºåµŒå…¥å¼çš„ RAG æ¼”ç¤ºå¦‚æ­¤æ™®éï¼Œå¾ˆå®¹æ˜“å¿½ç•¥ä¿¡æ¯æ£€ç´¢é¢†åŸŸæ•°åå¹´çš„ç ”ç©¶å’Œè§£å†³æ–¹æ¡ˆã€‚

Nonetheless, while embeddings are undoubtedly a powerful tool, they are not the be-all and end-all.  

å°½ç®¡åµŒå…¥å¼æ— ç–‘æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œä½†å¹¶éä¸‡äº‹ä¸‡ç‰©ã€‚  

First, while they excel at capturing high-level semantic similarity, they may struggle with more specific, keyword-based queries, like when users search for names (e.g., Ilya), acronyms (e.g., RAG), or IDs (e.g., claude-3-sonnet).  

é¦–å…ˆï¼Œè™½ç„¶å®ƒä»¬æ“…é•¿æ•æ‰é«˜çº§è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œä½†åœ¨å¤„ç†æ›´å…·ä½“çš„åŸºäºå…³é”®å­—çš„æŸ¥è¯¢æ—¶å¯èƒ½ä¼šé‡åˆ°å›°éš¾ï¼Œæ¯”å¦‚å½“ç”¨æˆ·æœç´¢åç§°ï¼ˆä¾‹å¦‚ï¼ŒIlyaï¼‰ã€é¦–å­—æ¯ç¼©å†™ï¼ˆä¾‹å¦‚ï¼ŒRAGï¼‰æˆ– IDï¼ˆä¾‹å¦‚ï¼Œclaude-3-sonnetï¼‰æ—¶ï¼Œå¯èƒ½ä¼šé‡åˆ°å›°éš¾ã€‚  

Keyword-based search, such as BM25, is explicitly designed for this.  

åŸºäºå…³é”®è¯çš„æœç´¢ï¼Œä¾‹å¦‚ BM25ï¼Œæ˜¯ä¸“é—¨ä¸ºæ­¤ç›®çš„è®¾è®¡çš„ã€‚  

Finally, after years of keyword-based search, users have likely taken it for granted and may get frustrated if the document they expect to retrieve isnâ€™t being returned.  

ç»è¿‡å¤šå¹´çš„åŸºäºå…³é”®å­—çš„æœç´¢ï¼Œç”¨æˆ·å¾ˆå¯èƒ½å·²ç»ä¹ ä»¥ä¸ºå¸¸ï¼Œå¦‚æœä»–ä»¬æœŸæœ›æ£€ç´¢çš„æ–‡æ¡£æ²¡æœ‰è¢«è¿”å›ï¼Œä»–ä»¬å¯èƒ½ä¼šæ„Ÿåˆ°æ²®ä¸§ã€‚

> Vector embeddings _do not_ magically solve search.  
> 
> å‘é‡åµŒå…¥å¹¶ä¸èƒ½ç¥å¥‡åœ°è§£å†³æœç´¢é—®é¢˜ã€‚  
> 
> In fact, the heavy lifting is in the step before you re-rank with semantic similarity search.  
> 
> å®é™…ä¸Šï¼Œé‡è¦çš„å·¥ä½œåœ¨äºåœ¨é‡æ–°æ’åºæ—¶ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼æ€§æœç´¢ä¹‹å‰çš„æ­¥éª¤ã€‚  
> 
> Making a genuine improvement over BM25 or full-text search is hard. â€” [Aravind Srinivas, CEO Perplexity.ai](https://x.com/AravSrinivas/status/1737886080555446552)  
> 
> è¦æ¯” BM25 æˆ–å…¨æ–‡æœç´¢å–å¾—çœŸæ­£çš„æ”¹è¿›æ˜¯å›°éš¾çš„ã€‚â€” Aravind Srinivasï¼ŒPerplexity.ai é¦–å¸­æ‰§è¡Œå®˜

> Weâ€™ve been communicating this to our customers and partners for months now.  
> 
> æˆ‘ä»¬å·²ç»å‡ ä¸ªæœˆå‘æˆ‘ä»¬çš„å®¢æˆ·å’Œåˆä½œä¼™ä¼´ä¼ è¾¾äº†è¿™ä¸€ä¿¡æ¯ã€‚  
> 
> Nearest Neighbor Search with naive embeddings yields very noisy results and youâ€™re likely better off starting with a keyword-based approach.  
> 
> æœ€è¿‘é‚»æœç´¢ä½¿ç”¨å¤©çœŸçš„åµŒå…¥ä¼šäº§ç”Ÿéå¸¸å˜ˆæ‚çš„ç»“æœï¼Œæœ€å¥½ä»åŸºäºå…³é”®å­—çš„æ–¹æ³•å¼€å§‹ã€‚  
> 
> â€” [Beyang Liu, CTO Sourcegraph](https://twitter.com/beyang/status/1767330006999720318)  
> 
> â€” Beyang Liuï¼ŒSourcegraph é¦–å¸­æŠ€æœ¯å®˜

Second, itâ€™s more straightforward to understand why a document was retrieved with keyword searchâ€”we can look at the keywords that match the query.  

å…¶æ¬¡ï¼Œé€šè¿‡å…³é”®å­—æœç´¢æ›´å®¹æ˜“ç†è§£æ–‡æ¡£æ˜¯å¦‚ä½•è¢«æ£€ç´¢å‡ºæ¥çš„â€”â€”æˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ä¸æŸ¥è¯¢åŒ¹é…çš„å…³é”®è¯ã€‚  

In contrast, embedding-based retrieval is less interpretable.  

ç›¸åï¼ŒåŸºäºåµŒå…¥å¼æ£€ç´¢çš„æ–¹æ³•è¾ƒéš¾è§£é‡Šã€‚  

Finally, thanks to systems like Lucene and OpenSearch that have been optimized and battle-tested over decades, keyword search is usually more computationally efficient.  

æœ€ç»ˆï¼Œå¤šäºäº†åƒ Lucene å’Œ OpenSearch è¿™æ ·ç»è¿‡æ•°åå¹´ä¼˜åŒ–å’Œå®æˆ˜æ£€éªŒçš„ç³»ç»Ÿï¼Œå…³é”®è¯æœç´¢é€šå¸¸æ›´å…·è®¡ç®—æ•ˆç‡ã€‚

In most cases, a hybrid will work best: keyword matching for the obvious matches, and embeddings for synonyms, hypernyms, and spelling errors, as well as multimodality (e.g., images and text).  

åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ··åˆä½¿ç”¨æ•ˆæœæœ€ä½³ï¼šå¯¹äºæ˜æ˜¾åŒ¹é…ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…ï¼›å¯¹äºåŒä¹‰è¯ã€ä¸Šä½è¯ã€æ‹¼å†™é”™è¯¯ä»¥åŠå¤šæ¨¡æ€ï¼ˆä¾‹å¦‚å›¾åƒå’Œæ–‡æœ¬ï¼‰ï¼Œåˆ™ä½¿ç”¨åµŒå…¥ã€‚  

[Shortwave shared how they built their RAG pipeline](https://www.shortwave.com/blog/deep-dive-into-worlds-smartest-email-ai/), including query rewriting, keyword + embedding retrieval, and ranking.  

Shortwave åˆ†äº«äº†ä»–ä»¬æ„å»º RAG ç®¡é“çš„è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æŸ¥è¯¢é‡å†™ã€å…³é”®è¯+åµŒå…¥å¼æ£€ç´¢å’Œæ’åã€‚

### Prefer RAG over finetuning for new knowledge  

1.2.3 ä¼˜å…ˆé€‰æ‹© RAG è€Œä¸æ˜¯å¾®è°ƒæ¥è·å–æ–°çŸ¥è¯†[](https://applied-llms.org/#prefer-rag-over-finetuning-for-new-knowledge)

Both RAG and finetuning can be used to incorporate new information into LLMs and increase performance on specific tasks.  

RAG å’Œ finetuning éƒ½å¯ç”¨äºå°†æ–°ä¿¡æ¯æ•´åˆåˆ°LLMsä¸­ï¼Œå¹¶æé«˜ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ã€‚  

However, which should we prioritize?  

ç„¶è€Œï¼Œæˆ‘ä»¬åº”è¯¥ä¼˜å…ˆè€ƒè™‘ä»€ä¹ˆï¼Ÿ

Recent research suggests RAG may have an edge. [One study](https://arxiv.org/abs/2312.05934) compared RAG against unsupervised finetuning (aka continued pretraining), evaluating both on a subset of MMLU and current events.  

æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ RAG å¯èƒ½å…·æœ‰ä¼˜åŠ¿ã€‚ä¸€é¡¹ç ”ç©¶å°† RAG ä¸æ— ç›‘ç£å¾®è°ƒï¼ˆä¹Ÿç§°ä¸ºæŒç»­é¢„è®­ç»ƒï¼‰è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶åœ¨ MMLU å’Œå½“å‰äº‹ä»¶çš„å­é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚  

They found that RAG consistently outperformed finetuning for knowledge encountered during training as well as entirely new knowledge.  

ä»–ä»¬å‘ç°ï¼Œç›¸å¯¹äºå¾®è°ƒï¼ŒRAG åœ¨è®­ç»ƒæœŸé—´é‡åˆ°çš„çŸ¥è¯†ä»¥åŠå…¨æ–°çŸ¥è¯†æ–¹é¢è¡¨ç°æ›´å‡ºè‰²ã€‚  

In [another paper](https://arxiv.org/abs/2401.08406), they compared RAG against supervised finetuning on an agricultural dataset.  

åœ¨å¦ä¸€ç¯‡è®ºæ–‡ä¸­ï¼Œä»–ä»¬å¯¹ RAG ä¸åœ¨å†œä¸šæ•°æ®é›†ä¸Šè¿›è¡Œçš„ç›‘ç£å¾®è°ƒè¿›è¡Œäº†æ¯”è¾ƒã€‚  

Similarly, the performance boost from RAG was greater than finetuning, especially for GPT-4 (see Table 20).  

åŒæ ·ï¼Œæ¥è‡ª RAG çš„æ€§èƒ½æå‡å¤§äºå¾®è°ƒï¼Œå°¤å…¶æ˜¯å¯¹äº GPT-4ï¼ˆè¯¦è§è¡¨ 20ï¼‰ã€‚

Beyond improved performance, RAG has other practical advantages.  

é™¤äº†æå‡æ€§èƒ½å¤–ï¼ŒRAG è¿˜å…·æœ‰å…¶ä»–å®é™…ä¼˜åŠ¿ã€‚  

First, compared to continuous pretraining or finetuning, itâ€™s easierâ€”and cheaper!â€”to keep retrieval indices up-to-date.  

é¦–å…ˆï¼Œä¸è¿ç»­çš„é¢„è®­ç»ƒæˆ–å¾®è°ƒç›¸æ¯”ï¼Œä¿æŒæ£€ç´¢ç´¢å¼•çš„æœ€æ–°æ›´å®¹æ˜“ä¸”æ›´ç»æµå®æƒ ï¼  

Second, if our retrieval indices have problematic documents that contain toxic or biased content, we can easily drop or modify the offending documents.  

å¦å¤–ï¼Œå¦‚æœæˆ‘ä»¬çš„æ£€ç´¢æŒ‡æ ‡ä¸­åŒ…å«æœ‰æ¯’æˆ–åè§å†…å®¹çš„é—®é¢˜æ–‡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°åˆ é™¤æˆ–ä¿®æ”¹è¿™äº›æœ‰é—®é¢˜çš„æ–‡ä»¶ã€‚  

Consider it an andon cord for [documents that ask us to add glue to pizza](https://x.com/petergyang/status/1793480607198323196).  

å°†å…¶è§†ä¸ºæ–‡ä»¶ä¸Šçš„å®‰å…¨ç»³ï¼Œè¦æ±‚æˆ‘ä»¬åœ¨æŠ«è¨ä¸ŠåŠ èƒ¶æ°´ã€‚

In addition, the R in RAG provides finer-grained control over how we retrieve documents.  

æ­¤å¤–ï¼ŒRAG ä¸­çš„ R æä¾›äº†æ›´ç²¾ç»†çš„æ§åˆ¶ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæ›´ç²¾å‡†åœ°æ£€ç´¢æ–‡æ¡£ã€‚  

For example, if weâ€™re hosting a RAG system for multiple organizations, by partitioning the retrieval indices, we can ensure that each organization can only retrieve documents from their own index.  

ä¸¾ä¾‹æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬ä¸ºå¤šä¸ªç»„ç»‡æ‰˜ç®¡ RAG ç³»ç»Ÿï¼Œé€šè¿‡å¯¹æ£€ç´¢ç´¢å¼•è¿›è¡Œåˆ†åŒºï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿æ¯ä¸ªç»„ç»‡åªèƒ½ä»è‡ªå·±çš„ç´¢å¼•ä¸­æ£€ç´¢æ–‡æ¡£ã€‚  

This ensures that we donâ€™t inadvertently expose information from one organization to another.  

è¿™æ ·å¯ä»¥ç¡®ä¿æˆ‘ä»¬ä¸ä¼šæ— æ„ä¸­å°†ä¸€ä¸ªç»„ç»‡çš„ä¿¡æ¯æ³„éœ²ç»™å¦ä¸€ä¸ªç»„ç»‡ã€‚

### Long-context models wonâ€™t make RAG obsolete  

1.2.4 é•¿æ–‡æœ¬æ¨¡å‹ä¸ä¼šä½¿ RAG å˜å¾—è¿‡æ—¶[](https://applied-llms.org/#long-context-models-wont-make-rag-obsolete)

With Gemini 1.5 providing context windows of up to 10M tokens in size, some have begun to question the future of RAG.  

éšç€ Gemini 1.5 æä¾›çš„ä¸Šä¸‹æ–‡çª—å£æœ€å¤šå¯è¾¾ 10M ä¸ªæ ‡è®°ï¼Œä¸€äº›äººå·²å¼€å§‹è´¨ç–‘ RAG çš„æœªæ¥ã€‚

> I tend to believe that Gemini 1.5 is significantly overhyped by Sora.  
> 
> æˆ‘å€¾å‘äºè®¤ä¸º Gemini 1.5 å—åˆ° Sora çš„è¿‡åº¦ç‚’ä½œã€‚  
> 
> A context window of 10M tokens effectively makes most of existing RAG frameworks unnecessary â€” you simply put whatever your data into the context and talk to the model like usual.  
> 
> ä¸€ä¸ªåŒ…å« 10M ä»¤ç‰Œçš„ä¸Šä¸‹æ–‡çª—å£å®é™…ä¸Šä½¿å¤§å¤šæ•°ç°æœ‰çš„ RAG æ¡†æ¶å˜å¾—ä¸å†å¿…è¦ â€” æ‚¨åªéœ€å°†æ‚¨çš„æ•°æ®æ”¾å…¥ä¸Šä¸‹æ–‡ä¸­ï¼Œç„¶ååƒå¹³å¸¸ä¸€æ ·ä¸æ¨¡å‹äº¤äº’ã€‚  
> 
> Imagine how it does to all the startups / agents / langchain projects where most of the engineering efforts goes to RAG ğŸ˜… Or in one sentence: the 10m context kills RAG.  
> 
> æƒ³è±¡ä¸€ä¸‹ï¼Œå¤§éƒ¨åˆ†çš„å·¥ç¨‹å·¥ä½œéƒ½æŠ•å…¥åˆ° RAG ä¸­çš„æ‰€æœ‰åˆåˆ›å…¬å¸/ä»£ç†å•†/langchain é¡¹ç›®ä¸­ä¼šå‘ç”Ÿä»€ä¹ˆğŸ˜…æˆ–è€…ç”¨ä¸€å¥è¯è¯´ï¼š10m çš„èƒŒæ™¯æ‰¼æ€äº† RAGã€‚  
> 
> Nice work Gemini â€” [Yao Fu](https://x.com/Francis_YAO_/status/1758935954189115714)  
> 
> åŒå­åº§å¹²å¾—å¥½ â€” å§šå¤«

While itâ€™s true that long contexts will be a game-changer for use cases such as analyzing multiple documents or chatting with PDFs, the rumors of RAGâ€™s demise are greatly exaggerated.  

å°½ç®¡é•¿ç¯‡ä¸Šä¸‹æ–‡å¯¹äºåˆ†æå¤šä¸ªæ–‡æ¡£æˆ–ä¸ PDF è¿›è¡ŒèŠå¤©ç­‰ç”¨ä¾‹å°†æ˜¯ä¸€ä¸ªé‡å¤§å˜é©ï¼Œä½†æ˜¯ RAG çš„æ¶ˆäº¡ä¼ é—»è¢«å¤¸å¤§äº†ã€‚

First, even with a context size of 10M tokens, weâ€™d still need a way to select relevant context.  

å³ä½¿ä¸Šä¸‹æ–‡å¤§å°ä¸º 10M ä»¤ç‰Œï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦ä¸€ç§æ–¹æ³•æ¥é€‰æ‹©ç›¸å…³çš„ä¸Šä¸‹æ–‡ã€‚  

Second, beyond the narrow needle-in-a-haystack eval, weâ€™ve yet to see convincing data that models can effectively reason over large context sizes.  

å…¶æ¬¡ï¼Œåœ¨ç‹­éš˜çš„å¤§æµ·æé’ˆè¯„ä¼°ä¹‹å¤–ï¼Œæˆ‘ä»¬å°šæœªçœ‹åˆ°æœ‰è¯´æœåŠ›çš„æ•°æ®è¡¨æ˜æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨ç†å¤§èŒƒå›´çš„ä¸Šä¸‹æ–‡å¤§å°ã€‚  

Thus, without good retrieval (and ranking), we risk overwhelming the model with distractors, or may even fill the context window with completely irrelevant information.  

å› æ­¤ï¼Œå¦‚æœæ²¡æœ‰è‰¯å¥½çš„æ£€ç´¢ï¼ˆå’Œæ’åï¼‰åŠŸèƒ½ï¼Œæˆ‘ä»¬å°±æœ‰å¯èƒ½ç”¨å¹²æ‰°å› ç´ æ·¹æ²¡æ¨¡å‹ï¼Œç”šè‡³å¯èƒ½ç”¨å®Œå…¨æ— å…³çš„ä¿¡æ¯å¡«å……ä¸Šä¸‹æ–‡çª—å£ã€‚

Finally, thereâ€™s cost. During inference, the Transformerâ€™s time complexity scales linearly with context length.  

æœ€åï¼Œæˆæœ¬ä¹Ÿæ˜¯ä¸€ä¸ªè€ƒé‡å› ç´ ã€‚åœ¨æ¨æ–­æ—¶ï¼ŒTransformer çš„æ—¶é—´å¤æ‚åº¦éšç€ä¸Šä¸‹æ–‡é•¿åº¦å‘ˆçº¿æ€§å¢é•¿ã€‚  

Just because there exists a model that can read your orgâ€™s entire Google Drive contents before answering each question doesnâ€™t mean thatâ€™s a good idea.  

ä»…ä»…å› ä¸ºå­˜åœ¨ä¸€ä¸ªæ¨¡å‹å¯ä»¥åœ¨å›ç­”æ¯ä¸ªé—®é¢˜ä¹‹å‰è¯»å–æ‚¨ç»„ç»‡çš„æ•´ä¸ªè°·æ­Œé©±åŠ¨å™¨å†…å®¹ï¼Œå¹¶ä¸æ„å‘³ç€è¿™æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚  

Consider an analogy to how we use RAM: we still read and write from disk, even though there exist compute instances with [RAM running into the tens of terabytes](https://aws.amazon.com/ec2/instance-types/high-memory/).  

è€ƒè™‘ç±»æ¯”æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ RAMï¼šå°½ç®¡å­˜åœ¨è¿è¡Œå†…å­˜è¾¾æ•°å TB çš„è®¡ç®—å®ä¾‹ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦ä»ç£ç›˜è¯»å–å’Œå†™å…¥æ•°æ®ã€‚

So donâ€™t throw your RAGs in the trash just yet.  

æ‰€ä»¥ä¸è¦æ€¥ç€æŠŠä½ çš„æŠ¹å¸ƒæ‰”è¿›åƒåœ¾æ¡¶ã€‚  

This pattern will remain useful even as context sizes grow.  

è¿™ç§æ¨¡å¼å°†åœ¨ä¸Šä¸‹æ–‡å°ºå¯¸å¢å¤§æ—¶ä»ç„¶ä¿æŒå®ç”¨ã€‚

## Tuning and optimizing workflows  

1.3 è°ƒæ•´å’Œä¼˜åŒ–å·¥ä½œæµç¨‹[](https://applied-llms.org/#tuning-and-optimizing-workflows)

Prompting an LLM is just the beginning.  

å¼•å¯¼ä¸€ä¸ªLLMåªæ˜¯ä¸€ä¸ªèµ·ç‚¹ã€‚  

To get the most juice out of them, we need to think beyond a single prompt and embrace workflows.  

ä¸ºäº†å……åˆ†åˆ©ç”¨å®ƒä»¬ï¼Œæˆ‘ä»¬éœ€è¦è¶…è¶Šå•ä¸€æç¤ºï¼Œæ‹¥æŠ±å·¥ä½œæµç¨‹ã€‚  

For example, how could we split a single complex task into multiple simpler tasks?  

ä¸¾ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¦‚ä½•å°†ä¸€ä¸ªå¤æ‚çš„ä»»åŠ¡åˆ†è§£æˆå¤šä¸ªç®€å•çš„ä»»åŠ¡ï¼Ÿ  

When is finetuning or caching helpful with increasing performance and reducing latency/cost?  

ä½•æ—¶ä½¿ç”¨ finetuning æˆ–ç¼“å­˜å¯ä»¥æé«˜æ€§èƒ½å¹¶é™ä½å»¶è¿Ÿ/æˆæœ¬ï¼Ÿ  

Here, we share proven strategies and real-world examples to help you optimize and build reliable LLM workflows.  

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ†äº«ç»è¿‡éªŒè¯çš„ç­–ç•¥å’Œç°å®ä¸–ç•Œçš„ç¤ºä¾‹ï¼Œä»¥å¸®åŠ©æ‚¨ä¼˜åŒ–å’Œæ„å»ºå¯é çš„LLMå·¥ä½œæµç¨‹ã€‚

### Step-by-step, multi-turn â€œflowsâ€ can give large boosts  

1.3.1 é€æ­¥è¿›è¡Œï¼Œå¤šè½®â€œæµç¨‹â€å¯ä»¥å¤§å¤§æå‡æ•ˆç‡[](https://applied-llms.org/#step-by-step-multi-turn-flows-can-give-large-boosts)

Itâ€™s common knowledge that decomposing a single big prompt into multiple smaller prompts can achieve better results.  

ä¼—æ‰€å‘¨çŸ¥ï¼Œå°†ä¸€ä¸ªå¤§æç¤ºåˆ†è§£ä¸ºå¤šä¸ªè¾ƒå°çš„æç¤ºå¯ä»¥å–å¾—æ›´å¥½çš„æ•ˆæœã€‚  

For example, [AlphaCodium](https://arxiv.org/abs/2401.08500): By switching from a single prompt to a multi-step workflow, they increased GPT-4 accuracy (pass@5) on CodeContests from 19% to 44%.  

ä¾‹å¦‚ï¼ŒAlphaCodiumï¼šé€šè¿‡ä»å•ä¸€æç¤ºè½¬å˜ä¸ºå¤šæ­¥å·¥ä½œæµç¨‹ï¼Œä»–ä»¬æˆåŠŸå°†åœ¨ CodeContests ä¸Šçš„ GPT-4 å‡†ç¡®ç‡ï¼ˆpass@5ï¼‰ä» 19%æå‡è‡³ 44%ã€‚  

The workflow includes:  

å·¥ä½œæµç¨‹åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š

-   Reflecting on the problem  
    
    åæ€é—®é¢˜
-   Reasoning on the public tests  
    
    åœ¨å…¬å…±æµ‹è¯•ä¸Šè¿›è¡Œæ¨ç†
-   Generating possible solutions  
    
    åˆ¶å®šå¯èƒ½è§£å†³æ–¹æ¡ˆ
-   Ranking possible solutions  
    
    å¯¹å¯èƒ½è§£å†³æ–¹æ¡ˆè¿›è¡Œæ’å
-   Generating synthetic tests  
    
    ç”Ÿæˆåˆæˆæµ‹è¯•çš„è¿‡ç¨‹
-   Iterating on the solutions on public and synthetic tests.  
    
    ä¸æ–­æ”¹è¿›å…¬å…±å’Œåˆæˆæµ‹è¯•ä¸­çš„è§£å†³æ–¹æ¡ˆã€‚

Small tasks with clear objectives make for the best agent or flow prompts.  

å°ä»»åŠ¡ç›®æ ‡æ˜ç¡®ï¼Œæ˜¯æœ€ä½³çš„ä»£ç†äººæˆ–æµç¨‹æç¤ºã€‚  

Itâ€™s not required that every agent prompt requests structured output, but structured outputs help a lot to interface with whatever system is orchestrating the agentâ€™s interactions with the environment.  

å¹¶éæ¯ä¸ªä»£ç†éƒ½éœ€è¦æç¤ºè¯·æ±‚ç»“æ„åŒ–è¾“å‡ºï¼Œä½†ç»“æ„åŒ–è¾“å‡ºå¯¹äºä¸ç¼–æ’ä»£ç†ä¸ç¯å¢ƒäº¤äº’çš„ä»»ä½•ç³»ç»Ÿæ¥å£éå¸¸æœ‰å¸®åŠ©ã€‚  

Some things to try:  

ä¸€äº›å°è¯•çš„å»ºè®®ï¼š

-   A tightly-specified, explicit planning step. Also, consider having predefined plans to choose from.  
    
    ä¸€ä¸ªä¸¥æ ¼è§„å®šçš„ã€æ˜ç¡®çš„è§„åˆ’æ­¥éª¤ã€‚åŒæ—¶ï¼Œè€ƒè™‘æä¾›é¢„å…ˆåˆ¶å®šçš„è®¡åˆ’ä¾›é€‰æ‹©ã€‚
-   Rewriting the original user prompts into agent prompts, though this process may be lossy!  
    
    å°†åŸå§‹ç”¨æˆ·æç¤ºæ”¹å†™ä¸ºä»£ç†æç¤ºï¼Œå°½ç®¡è¿™ä¸ªè¿‡ç¨‹å¯èƒ½ä¼šæœ‰ä¿¡æ¯ä¸¢å¤±ï¼
-   Agent behaviors as linear chains, DAGs, and state machines; different dependency and logic relationships can be more and less appropriate for different scales.  
    
    ä»£ç†è¡Œä¸ºå¯ä»¥è¢«è§†ä¸ºçº¿æ€§é“¾ã€æœ‰å‘æ— ç¯å›¾å’ŒçŠ¶æ€æœºï¼›ä¸åŒè§„æ¨¡å¯èƒ½éœ€è¦ä¸åŒçš„ä¾èµ–å…³ç³»å’Œé€»è¾‘å…³ç³»ï¼Œæ›´æˆ–æ›´å°‘é€‚åˆã€‚  
    
    Can you squeeze performance optimization out of different task architectures?  
    
    ä½ èƒ½ä»ä¸åŒçš„ä»»åŠ¡æ¶æ„ä¸­æå‡æ€§èƒ½å—ï¼Ÿ
-   Planning validations; your planning can include instructions on how to evaluate the responses from other agents to make sure the final assembly works well together.  
    
    è®¡åˆ’éªŒè¯ï¼›æ‚¨çš„è®¡åˆ’å¯ä»¥åŒ…æ‹¬å¦‚ä½•è¯„ä¼°å…¶ä»–ä»£ç†å•†çš„å“åº”ï¼Œä»¥ç¡®ä¿æœ€ç»ˆè£…é…åè°ƒè‰¯å¥½ã€‚
-   Prompt engineering with fixed upstream stateâ€”make sure your agent prompts are evaluated against a collection of variants of what may have happen before.  
    
    é€šè¿‡å›ºå®šçš„ä¸Šæ¸¸çŠ¶æ€è¿›è¡Œå·¥ç¨‹æç¤ºâ€”â€”ç¡®ä¿æ‚¨çš„ä»£ç†æç¤ºæ ¹æ®å¯èƒ½åœ¨ä¹‹å‰å‘ç”Ÿçš„å„ç§æƒ…å†µçš„é›†åˆè¿›è¡Œè¯„ä¼°ã€‚

### Prioritize deterministic workflows for now  

ç›®å‰ä¼˜å…ˆè€ƒè™‘ç¡®å®šæ€§å·¥ä½œæµç¨‹[](https://applied-llms.org/#prioritize-deterministic-workflows-for-now)

While AI agents can dynamically react to user requests and the environment, their non-deterministic nature makes them a challenge to deploy.  

äººå·¥æ™ºèƒ½ä»£ç†è™½ç„¶èƒ½å¤ŸåŠ¨æ€åœ°å¯¹ç”¨æˆ·è¯·æ±‚å’Œç¯å¢ƒåšå‡ºååº”ï¼Œä½†å…¶éç¡®å®šæ€§ç‰¹æ€§ä½¿å¾—å®ƒä»¬åœ¨éƒ¨ç½²æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚  

Each step an agent takes has a chance of failing, and the chances of recovering from the error are poor.  

ä»£ç†äººæ¯èµ°ä¸€æ­¥éƒ½æœ‰å¤±è´¥çš„å¯èƒ½æ€§ï¼Œè€Œä¸”ä»é”™è¯¯ä¸­æ¢å¤çš„æœºä¼šå¾ˆå°‘ã€‚  

Thus, the likelihood that an agent completes a multi-step task successfully decreases exponentially as the number of steps increases.  

å› æ­¤ï¼Œéšç€æ­¥éª¤æ•°é‡çš„å¢åŠ ï¼Œä»£ç†æˆåŠŸå®Œæˆå¤šæ­¥ä»»åŠ¡çš„å¯èƒ½æ€§å‘ˆæŒ‡æ•°çº§ä¸‹é™ã€‚  

As a result, teams building agents find it difficult to deploy reliable agents.  

å› æ­¤ï¼Œæ„å»ºä»£ç†çš„å›¢é˜Ÿå‘ç°éš¾ä»¥éƒ¨ç½²å¯é çš„ä»£ç†ã€‚

A potential approach is to have agent systems produce deterministic plans which are then executed in a structured, reproducible way.  

ä¸€ç§æ½œåœ¨çš„æ–¹æ³•æ˜¯è®©ä»£ç†ç³»ç»Ÿç”Ÿæˆç¡®å®šæ€§è®¡åˆ’ï¼Œç„¶åä»¥ç»“æ„åŒ–ã€å¯å¤ç°çš„æ–¹å¼æ‰§è¡Œã€‚  

First, given a high-level goal or prompt, the agent generates a plan.  

é¦–å…ˆï¼Œæ ¹æ®é«˜å±‚æ¬¡çš„ç›®æ ‡æˆ–æç¤ºï¼Œä»£ç†åˆ¶å®šä¸€ä¸ªè®¡åˆ’ã€‚  

Then, the plan is executed deterministically.  

ç„¶åï¼Œè®¡åˆ’ä¼šè¢«ä»¥ç¡®å®šæ€§æ–¹å¼æ‰§è¡Œã€‚  

This allows each step to be more predictable and reliable. Benefits include:  

è¿™æ ·å¯ä»¥ä½¿æ¯ä¸ªæ­¥éª¤æ›´åŠ å¯é¢„æµ‹å’Œå¯é ã€‚å…¶ä¸­çš„å¥½å¤„åŒ…æ‹¬ï¼š

-   Generated plans can serve as few-shot samples to prompt or finetune an agent.  
    
    ç”Ÿæˆçš„è®¡åˆ’å¯ä»¥ä½œä¸ºå°‘é‡æ ·æœ¬ï¼Œç”¨äºæ¿€åŠ±æˆ–å¾®è°ƒä»£ç†ã€‚
-   Deterministic execution makes the system more reliable, and thus easier to test and debug.  
    
    ç¡®å®šæ€§æ‰§è¡Œä½¿ç³»ç»Ÿæ›´åŠ å¯é ï¼Œä»è€Œæ›´æ˜“äºæµ‹è¯•å’Œè°ƒè¯•ã€‚  
    
    In addition, failures can be traced to the specific steps in the plan.  
    
    æ­¤å¤–ï¼Œå¤±è´¥å¯ä»¥è¿½æº¯åˆ°è®¡åˆ’ä¸­çš„å…·ä½“æ­¥éª¤ã€‚
-   Generated plans can be represented as directed acyclic graphs (DAGs) which are easier, relative to a static prompt, to understand and adapt to new situations.  
    
    ç”Ÿæˆçš„è®¡åˆ’å¯ä»¥è¡¨ç¤ºä¸ºæœ‰å‘æ— ç¯å›¾ï¼ˆDAGsï¼‰ï¼Œç›¸å¯¹äºé™æ€æç¤ºï¼Œæ›´æ˜“äºç†è§£å’Œé€‚åº”æ–°æƒ…å†µã€‚

The most successful agent builders may be those with strong experience managing junior engineers because the process of generating plans is similar to how we instruct and manage juniors.  

é‚£äº›æˆåŠŸçš„ä»£ç†å•†å»ºé€ è€…å¯èƒ½æ˜¯é‚£äº›æœ‰ç€ä¸°å¯Œç®¡ç†åˆçº§å·¥ç¨‹å¸ˆç»éªŒçš„äººï¼Œå› ä¸ºåˆ¶å®šè®¡åˆ’çš„è¿‡ç¨‹ç±»ä¼¼äºæˆ‘ä»¬æŒ‡å¯¼å’Œç®¡ç†åˆçº§å·¥ç¨‹å¸ˆçš„æ–¹å¼ã€‚  

We give juniors clear goals and concrete plans, instead of vague open-ended directions, and we should do the same for our agents too.  

æˆ‘ä»¬ä¸ºåˆå­¦è€…è®¾å®šæ˜ç¡®çš„ç›®æ ‡å’Œå…·ä½“è®¡åˆ’ï¼Œè€Œä¸æ˜¯æ¨¡ç³Šçš„å¼€æ”¾å¼æŒ‡å¯¼ï¼Œæˆ‘ä»¬ä¹Ÿåº”è¯¥ä¸ºæˆ‘ä»¬çš„ä»£ç†äººåšåŒæ ·çš„äº‹æƒ…ã€‚

In the end, the key to reliable, working agents will likely be found in adopting more structured, deterministic approaches, as well as collecting data to refine prompts and finetune models.  

æœ€ç»ˆï¼Œå»ºç«‹å¯é ã€é«˜æ•ˆçš„ä»£ç†çš„å…³é”®å¯èƒ½åœ¨äºé‡‡ç”¨æ›´åŠ ç»“æ„åŒ–ã€ç¡®å®šæ€§çš„æ–¹æ³•ï¼Œä»¥åŠæ”¶é›†æ•°æ®æ¥ä¼˜åŒ–æç¤ºå¹¶è°ƒä¼˜æ¨¡å‹ã€‚  

Without this, weâ€™ll build agents that may work exceptionally well some of the time, but on average, disappoint users.  

æ²¡æœ‰è¿™ä¸ªï¼Œæˆ‘ä»¬å°†ä¼šæ„å»ºå‡ºä¸€äº›æ—¶å€™è¡¨ç°å¼‚å¸¸å‡ºè‰²ï¼Œä½†å¹³å‡è€Œè¨€å´è®©ç”¨æˆ·å¤±æœ›çš„ä»£ç†ã€‚

### Getting more diverse outputs beyond temperature  

1.3.3 è·å–æ›´å¤šä¸ä»…é™äºæ¸©åº¦çš„å¤šæ ·åŒ–è¾“å‡º[](https://applied-llms.org/#getting-more-diverse-outputs-beyond-temperature)

Suppose your task requires diversity in an LLMâ€™s output.  

å‡è®¾æ‚¨çš„ä»»åŠ¡éœ€è¦åœ¨LLMçš„è¾“å‡ºä¸­å…·æœ‰å¤šæ ·æ€§ã€‚  

Maybe youâ€™re writing an LLM pipeline to suggest products to buy from your catalog given a list of products the user bought previously.  

ä¹Ÿè®¸æ‚¨æ­£åœ¨ç¼–å†™ä¸€ä¸ªLLMç®¡é“ï¼Œæ ¹æ®ç”¨æˆ·å…ˆå‰è´­ä¹°çš„äº§å“åˆ—è¡¨æ¨èä»æ‚¨çš„ç›®å½•ä¸­è´­ä¹°çš„äº§å“ã€‚  

When running your prompt multiple times, you might notice that the resulting recommendations are too similarâ€”so you might increase the temperature parameter in your LLM requests.  

å½“æ‚¨å¤šæ¬¡è¿è¡Œæ‚¨çš„æç¤ºæ—¶ï¼Œæ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°ç”Ÿæˆçš„å»ºè®®å¤ªè¿‡ç›¸ä¼¼â€”å› æ­¤æ‚¨å¯èƒ½éœ€è¦å¢åŠ æ‚¨çš„LLMè¯·æ±‚ä¸­çš„æ¸©åº¦å‚æ•°ã€‚

Briefly, increasing the temperature parameter makes LLM responses more varied.  

ç®€å•æ¥è¯´ï¼Œå¢åŠ æ¸©åº¦å‚æ•°ä¼šä½¿LLMçš„å“åº”æ›´åŠ ä¸°å¯Œå¤šæ ·ã€‚  

At sampling time, the probability distributions of the next token become flatter, meaning that tokens that are usually less likely get chosen more often.  

åœ¨é‡‡æ ·æ—¶ï¼Œä¸‹ä¸€ä¸ªæ ‡è®°çš„æ¦‚ç‡åˆ†å¸ƒå˜å¾—æ›´åŠ å¹³å¦ï¼Œè¿™æ„å‘³ç€é€šå¸¸ä¸å¤ªå¯èƒ½è¢«é€‰æ‹©çš„æ ‡è®°ä¼šæ›´é¢‘ç¹åœ°è¢«é€‰ä¸­ã€‚  

Still, when increasing temperature, you may notice some failure modes related to output diversity.  

åœ¨å¢åŠ æ¸©åº¦æ—¶ï¼Œæ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°ä¸€äº›ä¸è¾“å‡ºå¤šæ ·æ€§ç›¸å…³çš„æ•…éšœæ¨¡å¼ã€‚  

For example, some products from the catalog that could be a good fit may never be output by the LLM.  

ä¾‹å¦‚ï¼Œç›®å½•ä¸­ä¸€äº›äº§å“å¯èƒ½éå¸¸åˆé€‚ï¼Œä½†å¯èƒ½æ°¸è¿œä¸ä¼šè¢«LLMè¾“å‡ºã€‚  

The same handful of products might be overrepresented in outputs, if they are highly likely to follow the prompt based on what the LLM has learned at training time.  

å¦‚æœè¿™äº›äº§å“åœ¨è®­ç»ƒæ—¶é«˜åº¦å¯èƒ½æ ¹æ®LLMå­¦åˆ°çš„å†…å®¹æ¥éµå¾ªæç¤ºï¼Œé‚£ä¹ˆå®ƒä»¬åœ¨è¾“å‡ºä¸­å¯èƒ½ä¼šè¢«è¿‡åº¦ä»£è¡¨ã€‚  

If the temperature is too high, you may get outputs that reference nonexistent products (or gibberish!)  

å¦‚æœæ¸©åº¦è¿‡é«˜ï¼Œæ‚¨å¯èƒ½ä¼šå¾—åˆ°å¼•ç”¨ä¸å­˜åœ¨äº§å“ï¼ˆæˆ–æ— æ„ä¹‰å†…å®¹ï¼‰çš„è¾“å‡º

In other words, increasing temperature does not guarantee that the LLM will sample outputs from the probability distribution you expect (e.g., uniform random).  

æ¢å¥è¯è¯´ï¼Œå¢åŠ æ¸©åº¦å¹¶ä¸ä¿è¯LLMä¼šä»æ‚¨æœŸæœ›çš„æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·è¾“å‡ºï¼Œä¾‹å¦‚å‡åŒ€éšæœºã€‚  

Nonetheless, we have other tricks to increase output diversity.  

ç„¶è€Œï¼Œæˆ‘ä»¬è¿˜æœ‰å…¶ä»–æ–¹æ³•æ¥æé«˜äº§å‡ºçš„å¤šæ ·æ€§ã€‚  

The simplest way is to adjust elements within the prompt.  

è°ƒæ•´æç¤ºä¸­çš„å…ƒç´ æ˜¯æœ€ç®€å•çš„æ–¹å¼ã€‚  

For example, if the prompt template includes a list of items, such as historical purchases, shuffling the order of these items each time theyâ€™re inserted into the prompt can make a significant difference.  

ä¾‹å¦‚ï¼Œå¦‚æœæç¤ºæ¨¡æ¿åŒ…å«ä¸€ç³»åˆ—é¡¹ç›®ï¼Œæ¯”å¦‚å†å²è´­ä¹°è®°å½•ï¼Œæ¯æ¬¡å°†è¿™äº›é¡¹ç›®æ’å…¥æç¤ºæ—¶æ”¹å˜å®ƒä»¬çš„é¡ºåºå¯èƒ½ä¼šäº§ç”Ÿé‡å¤§å½±å“ã€‚

Additionally, keeping a short list of recent outputs can help prevent redundancy.  

æ­¤å¤–ï¼Œä¿æŒä¸€ä»½æœ€è¿‘è¾“å‡ºæ¸…å•å¯ä»¥æœ‰æ•ˆé¿å…é‡å¤ã€‚  

In our recommended products example, by instructing the LLM to avoid suggesting items from this recent list, or by rejecting and resampling outputs that are similar to recent suggestions, we can further diversify the responses.  

åœ¨æˆ‘ä»¬æ¨èäº§å“çš„ç¤ºä¾‹ä¸­ï¼Œé€šè¿‡æŒ‡ç¤ºLLMé¿å…å»ºè®®æ¥è‡ªæœ€è¿‘åˆ—è¡¨çš„é¡¹ç›®ï¼Œæˆ–è€…æ‹’ç»å¹¶é‡æ–°é‡‡æ ·ç±»ä¼¼äºæœ€è¿‘å»ºè®®çš„è¾“å‡ºï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥ä½¿å“åº”å¤šæ ·åŒ–ã€‚  

Another effective strategy is to vary the phrasing used in the prompts.  

å¦ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥æ˜¯æ”¹å˜æç¤ºä¸­ä½¿ç”¨çš„æªè¾ã€‚  

For instance, incorporating phrases like â€œpick an item that the user would love using regularlyâ€ or â€œselect a product that the user would likely recommend to friendsâ€ can shift the focus and thereby influence the variety of recommended products.  

ä¾‹å¦‚ï¼ŒåŠ å…¥è¯¸å¦‚â€œé€‰æ‹©ç”¨æˆ·ä¼šå–œæ¬¢ç»å¸¸ä½¿ç”¨çš„ç‰©å“â€æˆ–â€œæŒ‘é€‰ç”¨æˆ·å¯èƒ½ä¼šå‘æœ‹å‹æ¨èçš„äº§å“â€ç­‰çŸ­è¯­ï¼Œå¯ä»¥æ”¹å˜ç„¦ç‚¹ï¼Œä»è€Œå½±å“æ¨èäº§å“çš„å¤šæ ·æ€§ã€‚

### Caching is underrated  

ç¼“å­˜çš„é‡è¦æ€§è¢«ä½ä¼°äº†[](https://applied-llms.org/#caching-is-underrated)

Caching saves cost and eliminates generation latency by removing the need to recompute responses for the same input.  

ç¼“å­˜é€šè¿‡é¿å…éœ€è¦é‡æ–°è®¡ç®—ç›¸åŒè¾“å…¥çš„å“åº”ï¼Œæ—¢èŠ‚çº¦æˆæœ¬åˆæ¶ˆé™¤ç”Ÿæˆå»¶è¿Ÿã€‚  

Furthermore, if a response has previously been guardrailed, we can serve these vetted responses and reduce the risk of serving harmful or inappropriate content.  

æ­¤å¤–ï¼Œå¦‚æœå…ˆå‰å·²å¯¹å“åº”è¿›è¡Œäº†ä¿æŠ¤ï¼Œæˆ‘ä»¬å¯ä»¥æä¾›ç»è¿‡å®¡æŸ¥çš„å“åº”ï¼Œä»è€Œé™ä½æä¾›æœ‰å®³æˆ–ä¸é€‚å½“å†…å®¹çš„é£é™©ã€‚

One straightforward approach to caching is to use unique IDs for the items being processed, such as if weâ€™re summarizing new articles or [product reviews](https://www.cnbc.com/2023/06/12/amazon-is-using-generative-ai-to-summarize-product-reviews.html). When a request comes in, we can check to see if a summary already exists in the cache.  

ä¸€ä¸ªç®€å•çš„ç¼“å­˜æ–¹æ³•æ˜¯ä¸ºæ­£åœ¨å¤„ç†çš„é¡¹ç›®ä½¿ç”¨å”¯ä¸€çš„ IDï¼Œæ¯”å¦‚æˆ‘ä»¬æ­£åœ¨æ€»ç»“æ–°æ–‡ç« æˆ–äº§å“è¯„è®ºã€‚å½“æ”¶åˆ°è¯·æ±‚æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²ç»å­˜åœ¨æ‘˜è¦ã€‚  

If so, we can return it immediately; if not, we generate, guardrail, and serve it, and then store it in the cache for future requests.  

å¦‚æœæ˜¯è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ç«‹å³è¿”å›ï¼›å¦‚æœä¸æ˜¯ï¼Œæˆ‘ä»¬ä¼šç”Ÿæˆã€è®¾ç½®ä¿æŠ¤æ ï¼Œå¹¶æä¾›ï¼Œç„¶åå°†å…¶å­˜å‚¨åœ¨ç¼“å­˜ä¸­ä»¥å¤‡å°†æ¥è¯·æ±‚ã€‚

For more open-ended queries, we can borrow techniques from the field of search, which also leverages caching for open-ended inputs.  

å¯¹äºæ›´åŠ å¼€æ”¾å¼çš„æŸ¥è¯¢ï¼Œæˆ‘ä»¬å¯ä»¥å€Ÿé‰´æœç´¢é¢†åŸŸçš„æŠ€æœ¯ï¼Œè¯¥é¢†åŸŸä¹Ÿåˆ©ç”¨ç¼“å­˜æ¥å¤„ç†å¼€æ”¾å¼è¾“å…¥ã€‚  

Features like autocomplete, spelling correction, and suggested queries also help normalize user input and thus increase the cache hit rate.  

è‡ªåŠ¨å®Œæˆã€æ‹¼å†™çº æ­£å’Œå»ºè®®æŸ¥è¯¢ç­‰åŠŸèƒ½æœ‰åŠ©äºè§„èŒƒç”¨æˆ·è¾“å…¥ï¼Œè¿›è€Œæé«˜ç¼“å­˜å‘½ä¸­ç‡ã€‚

### When to finetune  

1.3.5 ä½•æ—¶è¿›è¡Œå¾®è°ƒ[](https://applied-llms.org/#when-to-finetune)

We may have some tasks where even the most cleverly designed prompts fall short.  

æˆ‘ä»¬å¯èƒ½ä¼šé‡åˆ°ä¸€äº›ä»»åŠ¡ï¼Œå³ä½¿æ˜¯è®¾è®¡å¾—å†å·§å¦™çš„æç¤ºä¹Ÿä¼šä¸å¤Ÿç”¨ã€‚  

For example, even after significant prompt engineering, our system may still be a ways from returning reliable, high-quality output.  

ä¾‹å¦‚ï¼Œå³ä½¿ç»è¿‡é‡å¤§çš„ä¼˜åŒ–å·¥ç¨‹ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿå¯èƒ½ä»ç„¶éœ€è¦ä¸€æ®µæ—¶é—´æ‰èƒ½è¿”å›å¯é ä¸”é«˜è´¨é‡çš„è¾“å‡ºã€‚  

If so, then it may be necessary to finetune a model for your specific task.  

å¦‚æœæ˜¯è¿™æ ·çš„è¯ï¼Œå¯èƒ½éœ€è¦å¯¹æ‚¨çš„ç‰¹å®šä»»åŠ¡è¿›è¡Œæ¨¡å‹å¾®è°ƒã€‚

Successful examples include:  

æˆåŠŸçš„æ¡ˆä¾‹åŒ…æ‹¬ï¼š

-   [Honeycombâ€™s Natural Language Query Assistant](https://www.honeycomb.io/blog/introducing-query-assistant): Initially, the â€œprogramming manualâ€ was provided in the prompt together with n-shot examples for in-context learning.  
    
    Honeycomb çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢åŠ©æ‰‹ï¼šæœ€åˆï¼Œ"ç¼–ç¨‹æ‰‹å†Œ"ä¸ n-shot ç¤ºä¾‹ä¸€èµ·æä¾›ï¼Œä»¥ä¾¿è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚  
    
    While this worked decently, finetuning the model led to better output on the syntax and rules of the domain-specific language.  
    
    è™½ç„¶è¿™ä¸ªæ–¹æ³•æ•ˆæœè¿˜ä¸é”™ï¼Œä½†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä¼šåœ¨é¢†åŸŸç‰¹å®šè¯­è¨€çš„è¯­æ³•å’Œè§„åˆ™æ–¹é¢äº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚
-   [Rechatâ€™s Lucy](https://www.youtube.com/watch?v=B_DMMlDuJB0): The LLM needed to generate responses in a very specific format that combined structured and unstructured data for the frontend to render correctly.  
    
    Rechat çš„ Lucyï¼šéœ€è¦LLMæ¥ä»¥éå¸¸ç‰¹å®šçš„æ ¼å¼ç»“åˆç»“æ„åŒ–å’Œéç»“æ„åŒ–æ•°æ®ï¼Œä»¥ä¾¿å‰ç«¯æ­£ç¡®æ¸²æŸ“ã€‚  
    
    Finetuning was essential to get it to work consistently.  
    
    å¾®è°ƒæ˜¯å¿…ä¸å¯å°‘çš„ï¼Œä»¥ç¡®ä¿å…¶èƒ½å¤ŸæŒç»­ç¨³å®šè¿è¡Œã€‚

Nonetheless, while finetuning can be effective, it comes with significant costs.  

å°½ç®¡å¾®è°ƒå¯èƒ½æœ‰æ•ˆï¼Œä½†ä¼šå¸¦æ¥ç›¸å½“å¤§çš„æˆæœ¬ã€‚  

We have to annotate finetuning data, finetune and evaluate models, and eventually self-host them.  

æˆ‘ä»¬éœ€è¦å¯¹å¾®è°ƒæ•°æ®è¿›è¡Œæ ‡æ³¨ï¼Œå¾®è°ƒå’Œè¯„ä¼°æ¨¡å‹ï¼Œæœ€ç»ˆè‡ªè¡Œæ‰˜ç®¡å®ƒä»¬ã€‚  

Thus, consider if the higher upfront cost is worth it.  

å› æ­¤ï¼Œè¯·è€ƒè™‘é«˜æ˜‚çš„å‰æœŸæˆæœ¬æ˜¯å¦å€¼å¾—ã€‚  

If prompting gets you 90% of the way there, then finetuning may not be worth the investment.  

å¦‚æœæç¤ºè®©æ‚¨å®Œæˆäº† 90%çš„å·¥ä½œï¼Œé‚£ä¹ˆå¾®è°ƒå¯èƒ½å¹¶ä¸å€¼å¾—æŠ•èµ„ã€‚  

However, if we do decide to finetune, to reduce the cost of collecting human-annotated data, we can [generate and finetune on synthetic data](https://eugeneyan.com/writing/synthetic/), or [bootstrap on open-source data](https://eugeneyan.com/writing/finetuning/).  

ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å†³å®šè¿›è¡Œå¾®è°ƒä»¥é™ä½æ”¶é›†äººå·¥æ ‡æ³¨æ•°æ®çš„æˆæœ¬ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆåˆæˆæ•°æ®è¿›è¡Œå¾®è°ƒï¼Œæˆ–è€…åœ¨å¼€æºæ•°æ®ä¸Šå¼•å¯¼ã€‚

## Evaluation & Monitoring  

1.4 è¯„ä¼°ä¸ç›‘æµ‹[](https://applied-llms.org/#evaluation-monitoring)

Evaluating LLMs is a [minefield](https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield/) and even the biggest labs find it [challenging](https://www.anthropic.com/news/evaluating-ai-systems). LLMs return open-ended outputs, and the tasks we set them to are varied.  

è¯„ä¼°LLMsæ˜¯ä¸€é¡¹å……æ»¡æŒ‘æˆ˜çš„é›·åŒºï¼Œå³ä½¿æ˜¯æœ€å¤§çš„å®éªŒå®¤ä¹Ÿä¼šè§‰å¾—å›°éš¾ã€‚LLMsä¼šäº§ç”Ÿå¼€æ”¾å¼çš„ç»“æœï¼Œè€Œæˆ‘ä»¬ç»™å®ƒä»¬çš„ä»»åŠ¡ä¹Ÿæ˜¯å¤šæ ·åŒ–çš„ã€‚  

Nonetheless, rigorous and thoughtful evals are criticalâ€”itâ€™s no coincidence that technical leaders at OpenAI [work on evaluation and give feedback on individual evals](https://twitter.com/eugeneyan/status/1701692908074873036).  

ç„¶è€Œï¼Œä¸¥æ ¼è€Œæ·±æ€ç†Ÿè™‘çš„è¯„ä¼°è‡³å…³é‡è¦â€”â€”OpenAI çš„æŠ€æœ¯é¢†å¯¼è€…ä»¬è‡´åŠ›äºè¯„ä¼°å·¥ä½œå¹¶å°±ä¸ªåˆ«è¯„ä¼°æä¾›åé¦ˆï¼Œè¿™å¹¶éå·§åˆã€‚

Evaluating LLM applications invites a diversity of definitions and reductions: itâ€™s simply unit testing, or itâ€™s more like observability, or maybe itâ€™s just data science.  

è¯„ä¼°LLMä¸ªåº”ç”¨ç¨‹åºå¼•å‘äº†å„ç§å®šä¹‰å’Œæ¦‚å¿µï¼šå®ƒå¯ä»¥ç®€å•çœ‹ä½œæ˜¯å•å…ƒæµ‹è¯•ï¼Œä¹Ÿå¯ä»¥è§†ä¸ºæ›´ç±»ä¼¼äºå¯è§‚å¯Ÿæ€§ï¼Œåˆæˆ–è€…ä»…ä»…æ˜¯æ•°æ®ç§‘å­¦ã€‚  

We have found all of these perspectives useful.  

æˆ‘ä»¬å‘ç°æ‰€æœ‰è¿™äº›è§‚ç‚¹éƒ½å¾ˆæœ‰ç”¨ã€‚  

In this section, we provide some lessons on what is important in building evals and monitoring pipelines.  

åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€äº›å…³äºæ„å»ºè¯„ä¼°å’Œç›‘æ§ç®¡é“ä¸­é‡è¦çš„æ•™è®­ã€‚

### Create a few assertion-based unit tests from real input/output samples  

1.4.1 ä»çœŸå®çš„è¾“å…¥/è¾“å‡ºæ ·æœ¬ä¸­åˆ›å»ºå‡ ä¸ªåŸºäºæ–­è¨€çš„å•å…ƒæµ‹è¯•[](https://applied-llms.org/#create-a-few-assertion-based-unit-tests-from-real-inputoutput-samples)

Create [unit tests (i.e., assertions)](https://hamel.dev/blog/posts/evals/#level-1-unit-tests) consisting of samples of inputs and outputs from production, with expectations for outputs based on at least three criteria.  

åˆ›å»ºå•å…ƒæµ‹è¯•ï¼Œå³æ–­è¨€ï¼Œä½¿ç”¨ç”Ÿäº§ä¸­çš„è¾“å…¥å’Œè¾“å‡ºæ ·æœ¬ï¼Œæ ¹æ®è‡³å°‘ä¸‰ä¸ªæ ‡å‡†å¯¹è¾“å‡ºçš„é¢„æœŸè¿›è¡Œæ–­è¨€ã€‚  

While three criteria might seem arbitrary, itâ€™s a practical number to start with; fewer might indicate that your task isnâ€™t sufficiently defined or is too open-ended, like a general-purpose chatbot.  

å½“ä¸‰ä¸ªæ ‡å‡†å¯èƒ½çœ‹èµ·æ¥æ˜¯æ­¦æ–­çš„æ—¶å€™ï¼Œè¿™æ˜¯ä¸€ä¸ªå®ç”¨çš„èµ·ç‚¹æ•°å­—ï¼›æ›´å°‘å¯èƒ½è¡¨æ˜æ‚¨çš„ä»»åŠ¡å®šä¹‰ä¸å¤Ÿå……åˆ†ï¼Œæˆ–è€…å¤ªå¼€æ”¾ï¼Œå°±åƒä¸€ä¸ªé€šç”¨èŠå¤©æœºå™¨äººã€‚  

These unit tests, or assertions, should be triggered by any changes to the pipeline, whether itâ€™s editing a prompt, adding new context via RAG, or other modifications.  

è¿™äº›å•å…ƒæµ‹è¯•æˆ–æ–­è¨€åº”è¯¥åœ¨ç®¡é“å‘ç”Ÿä»»ä½•æ›´æ”¹æ—¶è§¦å‘ï¼Œæ— è®ºæ˜¯ç¼–è¾‘æç¤ºï¼Œé€šè¿‡ RAG æ·»åŠ æ–°çš„ä¸Šä¸‹æ–‡ï¼Œè¿˜æ˜¯å…¶ä»–ä¿®æ”¹ã€‚  

This [write-up has an example](https://hamel.dev/blog/posts/evals/#step-1-write-scoped-tests) of an assertion-based test for an actual use case.  

è¿™ç¯‡å†™ä½œä¸­æä¾›äº†ä¸€ä¸ªå…³äºå®é™…ç”¨ä¾‹çš„åŸºäºæ–­è¨€çš„æµ‹è¯•ç¤ºä¾‹ã€‚

Consider beginning with assertions that specify phrases that let us include or exclude responses.  

è€ƒè™‘ä»æŒ‡å®šçš„æ–­è¨€å¼€å§‹ï¼Œè¿™äº›æ–­è¨€æŒ‡å®šäº†æˆ‘ä»¬å¯ä»¥åŒ…å«æˆ–æ’é™¤å“åº”çš„çŸ­è¯­ã€‚  

Also try checks to ensure that word, item, or sentence counts lie within a range.  

ä¹Ÿå°è¯•æ£€æŸ¥ä»¥ç¡®ä¿å•è¯ã€é¡¹ç›®æˆ–å¥å­çš„è®¡æ•°åœ¨åˆç†èŒƒå›´å†…ã€‚  

For other kinds of generations, assertions can look different. [Execution-based evaluation](https://www.semanticscholar.org/paper/Execution-Based-Evaluation-for-Open-Domain-Code-Wang-Zhou/1bed34f2c23b97fd18de359cf62cd92b3ba612c3) is one way to evaluate code generation, wherein you run the generated code and check if the state of runtime is sufficient for the user request.  

å¯¹äºå…¶ä»–ç±»å‹çš„ç”Ÿæˆï¼Œæ–­è¨€å¯èƒ½æœ‰æ‰€ä¸åŒã€‚åŸºäºæ‰§è¡Œçš„è¯„ä¼°æ˜¯è¯„ä¼°ä»£ç ç”Ÿæˆçš„ä¸€ç§æ–¹å¼ï¼Œæ‚¨è¿è¡Œç”Ÿæˆçš„ä»£ç å¹¶æ£€æŸ¥è¿è¡Œæ—¶çŠ¶æ€æ˜¯å¦è¶³å¤Ÿæ»¡è¶³ç”¨æˆ·è¯·æ±‚ã€‚

As an example, if the user asks for a new function named foo; then after executing the agentâ€™s generated code, foo should be callable!  

ä¸¾ä¾‹æ¥è¯´ï¼Œå¦‚æœç”¨æˆ·è¦æ±‚ä¸€ä¸ªåä¸º foo çš„æ–°åŠŸèƒ½ï¼›é‚£ä¹ˆåœ¨æ‰§è¡Œä»£ç†ç”Ÿæˆçš„ä»£ç åï¼Œåº”è¯¥å¯ä»¥è°ƒç”¨ fooï¼  

One challenge in execution-based evaluation is that the agent code frequently leaves the runtime in a slightly different form than the target code.  

åœ¨æ‰§è¡Œå‹è¯„ä¼°ä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ï¼Œä»£ç†ä»£ç ç»å¸¸ä»¥ä¸ç›®æ ‡ä»£ç ç•¥æœ‰ä¸åŒçš„å½¢å¼ç¦»å¼€è¿è¡Œæ—¶ã€‚  

It can be effective to â€œrelaxâ€ assertions to the absolute most weak assumptions that any viable answer would satisfy.  

æ”¾å®½æ–­è¨€çš„æ¡ä»¶ï¼Œä½¿å…¶ç¬¦åˆä»»ä½•å¯è¡Œç­”æ¡ˆçš„æœ€åŸºæœ¬è¦æ±‚å¯èƒ½æ›´ä¸ºæœ‰æ•ˆã€‚

Finally, using your product as intended for customers (i.e., â€œdogfoodingâ€) can provide insight into failure modes on real-world data.  

æœ€åï¼ŒæŒ‰ç…§å®¢æˆ·é¢„æœŸä½¿ç”¨æ‚¨çš„äº§å“ï¼ˆå³â€œdogfoodingâ€ï¼‰å¯ä»¥å¸®åŠ©æ‚¨æ´å¯ŸçœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„æ•…éšœæ¨¡å¼ã€‚  

This approach not only helps identify potential weaknesses, but also provides a useful source of production samples that can be converted into evals.  

è¿™ç§æ–¹æ³•ä¸ä»…æœ‰åŠ©äºè¯†åˆ«æ½œåœ¨çš„å¼±ç‚¹ï¼Œè¿˜æä¾›äº†ä¸€ä¸ªå¯è½¬åŒ–ä¸ºè¯„ä¼°çš„ç”Ÿäº§æ ·æœ¬çš„æœ‰ç›Šæ¥æºã€‚

### LLM-as-Judge can work (somewhat), but itâ€™s not a silver bullet  

1.4.2 LLM-ä½œä¸ºæ³•å®˜å¯ä»¥èµ·ä½œç”¨ï¼ˆåœ¨æŸç§ç¨‹åº¦ä¸Šï¼‰ï¼Œä½†è¿™å¹¶ä¸æ˜¯ä¸‡èƒ½ä¹‹ç­–[](https://applied-llms.org/#llm-as-judge-can-work-somewhat-but-its-not-a-silver-bullet)

LLM-as-Judge, where we use a strong LLM to evaluate the output of other LLMs, has been met with skepticism.  

ä½œä¸ºæ³•å®˜ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ºå¤§çš„LLMæ¥è¯„ä¼°å…¶ä»–LLMsçš„è¾“å‡ºï¼Œä¸€ç›´å¤‡å—æ€€ç–‘ã€‚  

(Some of us were initially huge skeptics.) Nonetheless, when implemented well, LLM-as-Judge achieves decent correlation with human judgments, and can at least help build priors about how a new prompt or technique may perform.  

æœ‰äº›äººæœ€åˆå¯¹æ­¤æŒæ€€ç–‘æ€åº¦ã€‚ç„¶è€Œï¼Œå½“å®æ–½å¾—å½“æ—¶ï¼ŒLLMä½œä¸ºè¯„åˆ¤è€…ä¸äººç±»åˆ¤æ–­æœ‰ç€ç›¸å½“çš„ç›¸å…³æ€§ï¼Œè‡³å°‘å¯ä»¥å¸®åŠ©å»ºç«‹å…³äºæ–°æç¤ºæˆ–æŠ€æœ¯å¯èƒ½è¡¨ç°çš„å…ˆéªŒã€‚  

Specifically, when doing pairwise comparisons (control vs.  

å…·ä½“æ¥è¯´ï¼Œåœ¨è¿›è¡Œæˆå¯¹æ¯”è¾ƒæ—¶ï¼ˆå¯¹ç…§ç»„ vs.  

Â treatment), LLM-as-Judge typically gets the direction right though the magnitude of the win/loss may be noisy.  

é€šå¸¸æƒ…å†µä¸‹ï¼ŒLLM-ä½œä¸ºæ³•å®˜é€šå¸¸èƒ½å¤Ÿå‡†ç¡®æŠŠæ¡æ–¹å‘ï¼Œå°½ç®¡èƒœè´Ÿçš„å¹…åº¦å¯èƒ½ä¼šæœ‰äº›å˜ˆæ‚ã€‚

Here are some suggestions to get the most out of LLM-as-Judge:  

è¿™é‡Œæœ‰ä¸€äº›å»ºè®®ï¼Œå¸®åŠ©æ‚¨å……åˆ†å‘æŒ¥LLMä½œä¸ºæ³•å®˜çš„ä½œç”¨ï¼š

-   Use pairwise comparisons: Instead of asking the LLM to score a single output on a [Likert](https://en.wikipedia.org/wiki/Likert_scale) scale, present it with two options and ask it to select the better one.  
    
    ä½¿ç”¨æˆå¯¹æ¯”è¾ƒï¼šä¸è¦æ±‚LLMåœ¨æå…‹ç‰¹é‡è¡¨ä¸Šå¯¹å•ä¸ªè¾“å‡ºè¿›è¡Œè¯„åˆ†ï¼Œè€Œæ˜¯å‘ˆç°ä¸¤ä¸ªé€‰é¡¹ï¼Œå¹¶è¦æ±‚å…¶é€‰æ‹©æ›´å¥½çš„ä¸€ä¸ªã€‚  
    
    This tends to lead to more stable results.  
    
    è¿™é€šå¸¸ä¼šå¯¼è‡´æ›´åŠ ç¨³å®šçš„ç»“æœã€‚
-   Control for position bias: The order of options presented can bias the LLMâ€™s decision.  
    
    æ§åˆ¶ä½ç½®åè§ï¼šå‘ˆç°é€‰é¡¹çš„é¡ºåºå¯èƒ½ä¼šå½±å“LLMçš„å†³å®šã€‚  
    
    To mitigate this, do each pairwise comparison twice, swapping the order of pairs each time.  
    
    ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæ¯æ¬¡éƒ½è¦è¿›è¡Œä¸¤ä¸¤æ¯”è¾ƒä¸¤æ¬¡ï¼Œæ¯æ¬¡äº¤æ¢ä¸€ä¸‹å¯¹çš„é¡ºåºã€‚  
    
    Just be sure to attribute wins to the right option after swapping!  
    
    è¯·ç¡®ä¿åœ¨äº¤æ¢åå°†èƒœåˆ©å½’å› äºæ­£ç¡®çš„é€‰é¡¹ï¼
-   Allow for ties: In some cases, both options may be equally good.  
    
    å…è®¸å¹³å±€ï¼šåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¸¤ä¸ªé€‰é¡¹å¯èƒ½åŒæ ·å‡ºè‰²ã€‚  
    
    Thus, allow the LLM to declare a tie so it doesnâ€™t have to arbitrarily pick a winner.  
    
    å› æ­¤ï¼Œå…è®¸LLMå®£å¸ƒå¹³å±€ï¼Œé¿å…éšæ„é€‰æ‹©è·èƒœè€…ã€‚
-   Use Chain-of-Thought: Asking the LLM to explain its decision before giving a final answer can increase eval reliability.  
    
    ä½¿ç”¨æ€ç»´é“¾ï¼šåœ¨æœ€ç»ˆç»™å‡ºç­”å¤ä¹‹å‰ï¼Œè¦æ±‚LLMè§£é‡Šå…¶å†³ç­–ï¼Œå¯ä»¥æé«˜è¯„ä¼°çš„å¯é æ€§ã€‚  
    
    As a bonus, this lets you to use a weaker but faster LLM and still achieve similar results.  
    
    ä½œä¸ºå¥–åŠ±ï¼Œè¿™æ ·æ‚¨å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæ›´å¼±ä½†æ›´å¿«çš„LLMï¼Œä»ç„¶èƒ½å¤Ÿè¾¾åˆ°ç±»ä¼¼çš„æ•ˆæœã€‚  
    
    Because this part of the pipeline is typically run in batch, the extra latency from CoT isnâ€™t a problem.  
    
    å› ä¸ºè¿™éƒ¨åˆ†æµæ°´çº¿é€šå¸¸ä»¥æ‰¹å¤„ç†æ–¹å¼è¿è¡Œï¼Œæ¥è‡ª CoT çš„é¢å¤–å»¶è¿Ÿå¹¶ä¸æ˜¯é—®é¢˜ã€‚
-   Control for response length: LLMs tend to bias toward longer responses.  
    
    æ§åˆ¶å“åº”é•¿åº¦ï¼šLLMs ä¼šå¯¼è‡´æ›´é•¿çš„å“åº”ã€‚  
    
    To mitigate this, ensure response pairs are similar in length.  
    
    ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œè¯·ç¡®ä¿å“åº”å¯¹çš„é•¿åº¦ç›¸ä¼¼ã€‚

A useful application of LLM-as-Judge is checking a new prompting strategy against regression.  

LLM-ä½œä¸ºæ³•å®˜çš„ä¸€ä¸ªæœ‰ç”¨çš„åº”ç”¨æ˜¯å°†æ–°çš„æç¤ºç­–ç•¥ä¸å›å½’è¿›è¡Œå¯¹æ¯”ï¼Œä»¥æ£€éªŒå…¶æœ‰æ•ˆæ€§ã€‚  

If you have tracked a collection of production results, sometimes you can rerun those production examples with a new prompting strategy, and use LLM-as-Judge to quickly assess where the new strategy may suffer.  

å¦‚æœæ‚¨å·²ç»è®°å½•äº†ä¸€ç³»åˆ—ç”Ÿäº§ç»“æœï¼Œæœ‰æ—¶æ‚¨å¯ä»¥ä½¿ç”¨æ–°çš„æç¤ºç­–ç•¥é‡æ–°è¿è¡Œè¿™äº›ç”Ÿäº§ç¤ºä¾‹ï¼Œå¹¶åˆ©ç”¨LLM-ä½œä¸ºè¯„åˆ¤è€…ï¼Œå¿«é€Ÿè¯„ä¼°æ–°ç­–ç•¥å¯èƒ½å­˜åœ¨çš„é—®é¢˜ã€‚

Hereâ€™s an example of a [simple but effective approach](https://hamel.dev/blog/posts/evals/#automated-evaluation-w-llms) to iterate on LLM-as-Judge, where we log the LLM response, judgeâ€™s critique (i.e., CoT), and final outcome.  

è¿™æ˜¯ä¸€ä¸ªç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•ç¤ºä¾‹ï¼Œç”¨äºå¯¹LLM-ä½œä¸ºè¯„å§”è¿›è¡Œè¿­ä»£ã€‚æˆ‘ä»¬è®°å½•LLMçš„å“åº”ï¼Œè¯„å§”çš„è¯„è®ºï¼ˆå³ CoTï¼‰å’Œæœ€ç»ˆç»“æœã€‚  

They are then reviewed with stakeholders to identify areas for improvement.  

ç„¶åä¸åˆ©ç›Šç›¸å…³è€…ä¸€èµ·å®¡æŸ¥ï¼Œä»¥ç¡®å®šæ”¹è¿›çš„æ–¹å‘ã€‚  

Over three iterations, agreement with humans and LLM improved from 68% to 94%!  

ç»è¿‡ä¸‰è½®è¿­ä»£ï¼Œä¸äººç±»å’ŒLLMçš„ä¸€è‡´æ€§ä» 68%æé«˜åˆ° 94%ï¼

![](spreadsheet.png)

LLM-as-Judge is not a silver bullet though.  

LLM-æ‹…ä»»æ³•å®˜å¹¶ä¸æ˜¯è§£å†³é—®é¢˜çš„çµä¸¹å¦™è¯ã€‚  

There are subtle aspects of language where even the strongest models fail to evaluate reliably.  

è¯­è¨€ä¸­å­˜åœ¨å¾®å¦™ä¹‹å¤„ï¼Œå³ä½¿æ˜¯æœ€å¼ºå¤§çš„æ¨¡å‹ä¹Ÿéš¾ä»¥å¯é è¯„ä¼°ã€‚  

In addition, weâ€™ve found that [conventional classifiers](https://eugeneyan.com/writing/finetuning/) and reward models can achieve higher accuracy than LLM-as-Judge, and with lower cost and latency.  

æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ä¼ ç»Ÿåˆ†ç±»å™¨å’Œå¥–åŠ±æ¨¡å‹å¯ä»¥æ¯”LLM-as-Judge å®ç°æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œä¸”æˆæœ¬å’Œå»¶è¿Ÿæ›´ä½ã€‚  

For code generation, LLM-as-Judge can be weaker than more direct evaluation strategies like execution-evaluation.  

å¯¹äºä»£ç ç”Ÿæˆï¼ŒLLM-ä½œä¸ºè¯„åˆ¤è€…å¯èƒ½æ¯”ç›´æ¥æ‰§è¡Œè¯„ä¼°ç­‰ç­–ç•¥æ›´å¼±ã€‚

### The â€œintern testâ€ for evaluating generations  

1.4.3 ç”¨äºè¯„ä¼°ä¸–ä»£çš„â€œå®ä¹ ç”Ÿæµ‹è¯•â€[](https://applied-llms.org/#the-intern-test-for-evaluating-generations)

We like to use the following â€œintern testâ€ when evaluating generations: If you took the exact input to the language model, including the context, and gave it to an average college student in the relevant major as a task, could they succeed?  

åœ¨è¯„ä¼°ä¸–ä»£æ—¶ï¼Œæˆ‘ä»¬å–œæ¬¢ä½¿ç”¨ä»¥ä¸‹â€œå®ä¹ ç”Ÿæµ‹è¯•â€ï¼šå¦‚æœæ‚¨å°†å‡†ç¡®çš„è¾“å…¥æä¾›ç»™è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡ï¼Œå¹¶å°†å…¶ä½œä¸ºä»»åŠ¡äº¤ç»™ç›¸å…³ä¸“ä¸šçš„æ™®é€šå¤§å­¦ç”Ÿï¼Œä»–ä»¬èƒ½æˆåŠŸå—ï¼Ÿ  

How long would it take?  

éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ

-   If the answer is no because the LLM lacks the required knowledge, consider ways to enrich the context.  
    
    å¦‚æœç­”æ¡ˆæ˜¯å¦å®šçš„ï¼Œå› ä¸ºLLMç¼ºä¹å¿…è¦çš„çŸ¥è¯†ï¼Œè¯·è€ƒè™‘å¦‚ä½•ä¸°å¯ŒèƒŒæ™¯ä¿¡æ¯ã€‚
-   If the answer is no and we simply canâ€™t improve the context to fix it, then we may have hit a task thatâ€™s too hard for contemporary LLMs.  
    
    å¦‚æœç­”æ¡ˆæ˜¯å¦å®šçš„ï¼Œè€Œæˆ‘ä»¬æ— æ³•æ”¹å–„ä¸Šä¸‹æ–‡ä»¥è§£å†³é—®é¢˜ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯èƒ½é‡åˆ°äº†å¯¹å½“ä»£LLMsæ¥è¯´å¤ªå›°éš¾çš„ä»»åŠ¡ã€‚
-   If the answer is yes, but it would take a while, we can try to reduce the complexity of the task.  
    
    å¦‚æœç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼Œä½†éœ€è¦ä¸€äº›æ—¶é—´ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ç®€åŒ–ä»»åŠ¡çš„å¤æ‚æ€§ã€‚  
    
    Is it decomposable? Are there aspects of the task that can be made more templatized?  
    
    è¿™ä¸ªä»»åŠ¡å¯ä»¥åˆ†è§£å—ï¼Ÿæœ‰å“ªäº›æ–¹é¢å¯ä»¥æ›´åŠ æ¨¡æ¿åŒ–ï¼Ÿ
-   If the answer is yes, they would get it quickly, then itâ€™s time to dig into the data.  
    
    å¦‚æœç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼Œä»–ä»¬ä¼šè¿…é€Ÿè·å¾—ç»“æœï¼Œé‚£ä¹ˆç°åœ¨æ˜¯æ—¶å€™æ·±å…¥æŒ–æ˜æ•°æ®äº†ã€‚  
    
    Whatâ€™s the model doing wrong? Can we find a pattern of failures?  
    
    æ¨¡å‹å‡ºäº†ä»€ä¹ˆé—®é¢˜ï¼Ÿæˆ‘ä»¬èƒ½æ‰¾åˆ°å¤±è´¥çš„æ¨¡å¼å—ï¼Ÿ  
    
    Try asking the model to explain itself before or after it responds, to help you build a theory of mind.  
    
    åœ¨æ¨¡å‹å›ç­”å‰æˆ–åï¼Œè¯·å°è¯•è¦æ±‚æ¨¡å‹è§£é‡Šè‡ªå·±ï¼Œä»¥å¸®åŠ©æ‚¨å»ºç«‹å¿ƒæ™ºç†è®ºã€‚

### Overemphasizing certain evals can hurt overall performance  

1.4.4 è¿‡åˆ†å¼ºè°ƒæŸäº›è¯„ä¼°å¯èƒ½ä¼šå½±å“æ•´ä½“æ€§èƒ½[](https://applied-llms.org/#overemphasizing-certain-evals-can-hurt-overall-performance)

> â€œWhen a measure becomes a target, it ceases to be a good measure.â€ â€” Goodhartâ€™s Law.  
> 
> å½“ä¸€ä¸ªæŒ‡æ ‡å˜æˆç›®æ ‡æ—¶ï¼Œå®ƒå°±ä¸å†æ˜¯ä¸€ä¸ªå¥½çš„æŒ‡æ ‡ã€‚ â€” å¤å¾·å“ˆç‰¹å®šå¾‹ã€‚

An example of this is the Needle-in-a-Haystack (NIAH) eval.  

è¿™æ˜¯â€œå¤§æµ·æé’ˆâ€ï¼ˆNIAHï¼‰è¯„ä¼°çš„ä¸€ä¸ªä¾‹å­çš„ç¤ºä¾‹ã€‚  

The original eval helped quantify model recall as context sizes grew, as well as how recall is affected by needle position.  

éšç€ä¸Šä¸‹æ–‡è§„æ¨¡çš„å¢é•¿ï¼ŒåŸå§‹è¯„ä¼°æœ‰åŠ©äºé‡åŒ–æ¨¡å‹å¬å›ç‡çš„å˜åŒ–ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†å¬å›ç‡å—é’ˆä½ç½®å½±å“çš„æƒ…å†µã€‚  

However, itâ€™s been so overemphasized that itâ€™s featured as [Figure 1 for Gemini 1.5â€™s report](https://arxiv.org/abs/2403.05530).  

ç„¶è€Œï¼Œå®ƒè¢«è¿‡åˆ†å¼ºè°ƒï¼Œä»¥è‡³äºå®ƒè¢«åˆ—ä¸º Gemini 1.5 æŠ¥å‘Šä¸­çš„ç¬¬ä¸€å›¾ã€‚  

The eval involves inserting a specific phrase (â€œThe special magic {city} number is: {number}â€) into a long document that repeats the essays of Paul Graham, and then prompting the model to recall the magic number.  

è¯„ä¼°æ¶‰åŠå°†ç‰¹å®šçŸ­è¯­ï¼ˆâ€œç‰¹æ®Šé­”æ³•{åŸå¸‚}æ•°å­—ä¸ºï¼š{æ•°å­—}â€ï¼‰æ’å…¥é‡å¤ä¿ç½—Â·æ ¼é›·å„å§†çš„æ–‡ç« çš„é•¿æ–‡æ¡£ä¸­ï¼Œç„¶åæç¤ºæ¨¡å‹å›å¿†é­”æ³•æ•°å­—ã€‚

While some models achieve near-perfect recall, itâ€™s questionable whether NIAH truly measures the reasoning and recall abilities needed in real-world applications.  

å°½ç®¡ä¸€äº›æ¨¡å‹å®ç°äº†æ¥è¿‘å®Œç¾çš„å¬å›ç‡ï¼Œä½†æ˜¯å¦ NIAH çœŸæ­£è¡¡é‡äº†ç°å®ä¸–ç•Œåº”ç”¨ä¸­æ‰€éœ€çš„æ¨ç†å’Œå¬å›èƒ½åŠ›ä»ç„¶å€¼å¾—æ€€ç–‘ã€‚  

Consider a more practical scenario: Given the transcript of an hour-long meeting, can the LLM summarize the key decisions and next steps, as well as correctly attribute each item to the relevant person?  

è€ƒè™‘ä¸€ä¸ªæ›´å®é™…çš„æƒ…å¢ƒï¼šå‡è®¾æœ‰ä¸€åœºæŒç»­ä¸€ä¸ªå°æ—¶çš„ä¼šè®®è®°å½•ï¼ŒLLMæ˜¯å¦èƒ½å¤Ÿæ€»ç»“å‡ºå…³é”®å†³ç­–å’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œå¹¶æ­£ç¡®åœ°å°†æ¯ä¸ªé¡¹ç›®å½’å±åˆ°ç›¸å…³äººå‘˜ï¼Ÿ  

This task is more realistic, going beyond rote memorization, and considers the ability to parse complex discussions, identify relevant information, and synthesize summaries.  

è¿™é¡¹ä»»åŠ¡æ›´è´´è¿‘ç°å®ï¼Œä¸ä»…ä»…æ˜¯æ­»è®°ç¡¬èƒŒï¼Œè¿˜è€ƒè™‘åˆ°è§£æå¤æ‚è®¨è®ºçš„èƒ½åŠ›ï¼Œè¯†åˆ«ç›¸å…³ä¿¡æ¯ï¼Œå¹¶ç»¼åˆæ€»ç»“ã€‚

Hereâ€™s an example of a [practical NIAH eval](https://observablehq.com/@shreyashankar/needle-in-the-real-world-experiments). Using [doctor-patient transcripts](https://github.com/wyim/aci-bench/tree/main/data/challenge_data), the LLM is queried about the patientâ€™s medication.  

è¿™æ˜¯ä¸€ä¸ªå®é™…çš„ NIAH è¯„ä¼°ç¤ºä¾‹ã€‚é€šè¿‡åŒ»ç”Ÿå’Œæ‚£è€…çš„å¯¹è¯è®°å½•ï¼Œè¯¢é—®LLMå…³äºæ‚£è€…çš„è¯ç‰©æƒ…å†µã€‚  

It also includes a more challenging NIAH, inserting a phrase for random ingredients for pizza toppings, such as â€œ_The secret ingredients needed to build the perfect pizza are: Espresso-soaked dates, Lemon, and Goat cheese._â€. Recall was around 80% on the medication task and 30% on the pizza task.  

å®ƒè¿˜åŒ…æ‹¬ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„ NIAHï¼Œæ’å…¥äº†ä¸€ä¸ªçŸ­è¯­ï¼Œç”¨äºæè¿°æ¯”è¨é…æ–™çš„éšæœºæˆåˆ†ï¼Œä¾‹å¦‚â€œæ„å»ºå®Œç¾æ¯”è¨æ‰€éœ€çš„ç§˜å¯†é…æ–™æ˜¯ï¼šæµ¸æ³¡æµ“ç¼©å’–å•¡çš„æ£ã€æŸ æª¬å’Œç¾Šå¥¶é…ªã€‚â€åœ¨è¯ç‰©ä»»åŠ¡ä¸­çš„å¬å›ç‡çº¦ä¸º 80ï¼…ï¼Œåœ¨æ¯”è¨ä»»åŠ¡ä¸­ä¸º 30ï¼…ã€‚

![](niah.png)

Tangentially, an overemphasis on NIAH evals can reduce performance on extraction and summarization tasks.  

åœ¨æå–å’Œæ€»ç»“ä»»åŠ¡ä¸­ï¼Œè¿‡åˆ†å¼ºè°ƒ NIAH è¯„ä¼°å¯èƒ½ä¼šé™ä½è¡¨ç°ã€‚  

Because these LLMs are so finetuned to attend to every sentence, they may start to treat irrelevant details and distractors as important, thus including them in the final output (when they shouldnâ€™t!)  

ç”±äºè¿™äº›LLMså¯¹æ¯ä¸ªå¥å­éƒ½éå¸¸æ•æ„Ÿï¼Œå®ƒä»¬å¯èƒ½å¼€å§‹å°†ä¸ç›¸å…³çš„ç»†èŠ‚å’Œå¹²æ‰°å› ç´ è§†ä¸ºé‡è¦ï¼Œå› æ­¤åœ¨æœ€ç»ˆè¾“å‡ºä¸­åŒ…å«å®ƒä»¬ï¼ˆä½†å®é™…ä¸Šä¸åº”è¯¥ï¼ï¼‰

This could also apply to other evals and use cases. For example, summarization.  

è¿™ä¹Ÿé€‚ç”¨äºå…¶ä»–è¯„ä¼°å’Œç”¨ä¾‹ï¼Œæ¯”å¦‚æ€»ç»“ã€‚  

An emphasis on factual consistency could lead to summaries that are less specific (and thus less likely to be factually inconsistent) and possibly less relevant.  

å¼ºè°ƒäº‹å®çš„ä¸€è‡´æ€§å¯èƒ½å¯¼è‡´æ‘˜è¦å˜å¾—ä¸å¤Ÿå…·ä½“ï¼ˆå› æ­¤ä¸å¤ªå¯èƒ½å­˜åœ¨äº‹å®ä¸Šçš„ä¸ä¸€è‡´ï¼‰ï¼Œä¹Ÿå¯èƒ½å˜å¾—ä¸å¤ªç›¸å…³ã€‚  

Conversely, an emphasis on writing style and eloquence could lead to more flowery, marketing-type language that could introduce factual inconsistencies.  

ç›¸åï¼Œè¿‡åˆ†å¼ºè°ƒå†™ä½œé£æ ¼å’Œä¿®è¾å¯èƒ½å¯¼è‡´ä½¿ç”¨æ›´èŠ±å“¨ã€è¥é”€åŒ–çš„è¯­è¨€ï¼Œä»è€Œå¯èƒ½å¼•å…¥äº‹å®ä¸Šçš„ä¸ä¸€è‡´æ€§ã€‚

### Simplify annotation to binary tasks or pairwise comparisons  

1.4.5 å°†æ³¨é‡Šç®€åŒ–ä¸ºäºŒå…ƒä»»åŠ¡æˆ–æˆå¯¹æ¯”è¾ƒ[](https://applied-llms.org/#simplify-annotation-to-binary-tasks-or-pairwise-comparisons)

Providing open-ended feedback or ratings for model output on a [Likert scale](https://en.wikipedia.org/wiki/Likert_scale) is cognitively demanding.  

åœ¨æå…‹ç‰¹é‡è¡¨ä¸Šä¸ºæ¨¡å‹è¾“å‡ºæä¾›å¼€æ”¾å¼åé¦ˆæˆ–è¯„åˆ†åœ¨è®¤çŸ¥ä¸Šæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚  

As a result, the data collected is more noisyâ€”due to variability among human ratersâ€”and thus less useful.  

å› æ­¤ï¼Œç”±äºäººç±»è¯„åˆ†è€…ä¹‹é—´çš„å·®å¼‚æ€§ï¼Œæ”¶é›†çš„æ•°æ®å˜å¾—æ›´åŠ å˜ˆæ‚ï¼Œå› æ­¤ä¸å¤ªå®ç”¨ã€‚  

A more effective approach is to simplify the task and reduce the cognitive burden on annotators.  

æ›´æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ç®€åŒ–ä»»åŠ¡ï¼Œå‡è½»æ³¨é‡Šè€…çš„è®¤çŸ¥è´Ÿæ‹…ã€‚  

Two tasks that work well are binary classifications and pairwise comparisons.  

äºŒå…ƒåˆ†ç±»å’Œæˆå¯¹æ¯”è¾ƒæ˜¯ä¸¤ç§æ•ˆæœå¾ˆå¥½çš„ä»»åŠ¡ã€‚

In binary classifications, annotators are asked to make a simple yes-or-no judgment on the modelâ€™s output.  

åœ¨äºŒå…ƒåˆ†ç±»ä¸­ï¼Œæ³¨é‡Šè€…è¢«è¦æ±‚å¯¹æ¨¡å‹çš„è¾“å‡ºåšå‡ºç®€å•çš„æ˜¯æˆ–å¦åˆ¤æ–­ã€‚  

They might be asked whether the generated summary is factually consistent with the source document, or whether the proposed response is relevant, or if it contains toxicity.  

ä»–ä»¬å¯èƒ½ä¼šè¢«é—®åŠç”Ÿæˆçš„æ‘˜è¦æ˜¯å¦ä¸æºæ–‡ä»¶äº‹å®ä¸€è‡´ï¼Œæˆ–è€…å»ºè®®çš„å›ç­”æ˜¯å¦ç›¸å…³ï¼Œæˆ–è€…æ˜¯å¦åŒ…å«æœ‰æ¯’æ€§ã€‚  

Compared to the Likert scale, binary decisions are more precise, have higher consistency among raters, and lead to higher throughput.  

ä¸æå…‹ç‰¹é‡è¡¨ç›¸æ¯”ï¼ŒäºŒå…ƒå†³ç­–æ›´ä¸ºç²¾ç¡®ï¼Œè¯„åˆ†è€…ä¹‹é—´çš„ä¸€è‡´æ€§æ›´é«˜ï¼Œå¹¶ä¸”èƒ½å¤Ÿæé«˜å·¥ä½œæ•ˆç‡ã€‚  

This was how [Doordash set up their labeling queues](https://doordash.engineering/2020/08/28/overcome-the-cold-start-problem-in-menu-item-tagging/) for tagging menu items through a tree of yes-no questions.  

è¿™æ˜¯ Doordash å¦‚ä½•é€šè¿‡ä¸€ç³»åˆ—æ˜¯éé—®é¢˜æ ‘æ¥è®¾ç½®ä»–ä»¬çš„æ ‡ç­¾é˜Ÿåˆ—ï¼Œç”¨äºæ ‡è®°èœå•é¡¹çš„æ–¹å¼ã€‚

In pairwise comparisons, the annotator is presented with a pair of model responses and asked which is better.  

åœ¨æˆå¯¹æ¯”è¾ƒä¸­ï¼Œæ³¨é‡Šè€…ä¼šçœ‹åˆ°ä¸€å¯¹æ¨¡å‹å“åº”ï¼Œå¹¶è¢«è¦æ±‚é€‰æ‹©å“ªä¸ªæ›´å¥½ã€‚  

Because itâ€™s easier for humans to say â€œA is better than Bâ€ than to assign an individual score to either A or B individually, this leads to faster and more reliable annotations (over Likert scales).  

ç”±äºäººç±»æ›´å®¹æ˜“è¯´â€œA æ¯” B æ›´å¥½â€è€Œä¸æ˜¯ä¸º A æˆ– B åˆ†é…å•ç‹¬çš„åˆ†æ•°ï¼Œè¿™å¯¼è‡´æ›´å¿«é€Ÿå’Œæ›´å¯é çš„æ³¨é‡Šï¼ˆç›¸å¯¹äº Likert é‡è¡¨ï¼‰ã€‚  

At a [Llama2 meetup](https://www.youtube.com/watch?v=CzR3OrOkM9w), Thomas Scialom, an author on the Llama2 paper, confirmed that pairwise-comparisons were faster and cheaper than collecting supervised finetuning data such as written responses.  

åœ¨ Llama2 èšä¼šä¸Šï¼ŒLlama2 è®ºæ–‡çš„ä½œè€… Thomas Scialom ç¡®è®¤ï¼Œæˆå¯¹æ¯”è¾ƒæ¯”æ”¶é›†ç›‘ç£å¾®è°ƒæ•°æ®ï¼ˆå¦‚ä¹¦é¢å›å¤ï¼‰æ›´å¿«æ›´ä¾¿å®œã€‚  

The formerâ€™s cost is $3.5 per unit while the latterâ€™s cost is $25 per unit.  

å‰è€…çš„æˆæœ¬æ˜¯æ¯å•ä½ 3.5 ç¾å…ƒï¼Œè€Œåè€…çš„æˆæœ¬æ˜¯æ¯å•ä½ 25 ç¾å…ƒã€‚

If youâ€™re writing labeling guidelines, here are some [example guidelines](https://eugeneyan.com/writing/labeling-guidelines/) from Google and Bing Search.  

å¦‚æœæ‚¨æ­£åœ¨æ’°å†™æ ‡ç­¾æŒ‡å—ï¼Œè¿™é‡Œæœ‰ä¸€äº›æ¥è‡ªè°·æ­Œå’Œå¿…åº”æœç´¢çš„ç¤ºä¾‹æŒ‡å—ä¾›å‚è€ƒã€‚

### (Reference-free) evals and guardrails can be used interchangeably  

1.4.6 ç‰ˆæœ¬ä¸­ï¼Œè¯„ä¼°å’Œé˜²æŠ¤æ å¯ä»¥äº’ç›¸æ›¿ä»£[](https://applied-llms.org/#reference-free-evals-and-guardrails-can-be-used-interchangeably)

Guardrails help to catch inappropriate or harmful content while evals help to measure the quality and accuracy of the modelâ€™s output.  

é˜²æŠ¤æ æœ‰åŠ©äºæ•æ‰ä¸å½“æˆ–æœ‰å®³å†…å®¹ï¼Œè€Œè¯„ä¼°æœ‰åŠ©äºè¡¡é‡æ¨¡å‹è¾“å‡ºçš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚  

And if your evals are reference-free, they can be used as guardrails too.  

å¦‚æœæ‚¨çš„è¯„ä¼°æ˜¯æ— å‚è€ƒçš„ï¼Œå®ƒä»¬ä¹Ÿå¯ä»¥ä½œä¸ºæŠ¤æ ã€‚  

Reference-free evals are evaluations that donâ€™t rely on a â€œgoldenâ€ reference, such as a human-written answer, and can assess the quality of output based solely on the input prompt and the modelâ€™s response.  

æ— å‚è€ƒè¯„ä¼°æ˜¯æŒ‡è¯„ä¼°è¿‡ç¨‹ä¸­ä¸ä¾èµ–äºâ€œé»„é‡‘â€å‚è€ƒï¼Œå¦‚äººå·¥ç¼–å†™çš„ç­”æ¡ˆï¼Œè€Œæ˜¯ä»…æ ¹æ®è¾“å…¥æç¤ºå’Œæ¨¡å‹çš„å“åº”æ¥è¯„ä¼°è¾“å‡ºè´¨é‡ã€‚

Some examples of these are [summarization evals](https://eugeneyan.com/writing/evals/#summarization-consistency-relevance-length), where we only have to consider the input document to evaluate the summary on factual consistency and relevance.  

ä¸€äº›ä¾‹å­åŒ…æ‹¬æ€»ç»“è¯„ä¼°ï¼Œæˆ‘ä»¬åªéœ€è€ƒè™‘è¾“å…¥æ–‡æ¡£ï¼Œä»¥è¯„ä¼°æ‘˜è¦çš„äº‹å®ä¸€è‡´æ€§å’Œç›¸å…³æ€§ã€‚  

If the summary scores poorly on these metrics, we can choose not to display it to the user, effectively using the eval as a guardrail.  

å¦‚æœæ‘˜è¦åœ¨è¿™äº›æŒ‡æ ‡ä¸Šå¾—åˆ†ä¸ä½³ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸å‘ç”¨æˆ·æ˜¾ç¤ºå®ƒï¼Œå®é™…ä¸Šå°†è¯„ä¼°ä½œä¸ºä¸€ç§ä¿æŠ¤æªæ–½ã€‚  

Similarly, reference-free [translation evals](https://eugeneyan.com/writing/evals/#translation-statistical--learned-evals-for-quality) can assess the quality of a translation without needing a human-translated reference, again allowing us to use it as a guardrail.  

åŒæ ·ï¼Œæ— å‚è€ƒç¿»è¯‘è¯„ä¼°å¯ä»¥è¯„ä¼°ç¿»è¯‘çš„è´¨é‡ï¼Œè€Œæ— éœ€äººå·¥ç¿»è¯‘çš„å‚è€ƒï¼Œå†æ¬¡ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†å…¶ç”¨ä½œä¿éšœã€‚

### LLMs will return output even when they shouldnâ€™t  

1.4.7 LLMs å°†è¿”å›è¾“å‡ºï¼Œå³ä½¿å®ƒä»¬ä¸åº”è¯¥[](https://applied-llms.org/#llms-will-return-output-even-when-they-shouldnt)

A key challenge when working with LLMs is that theyâ€™ll often generate output even when they shouldnâ€™t.  

åœ¨ä½¿ç”¨LLMsæ—¶é¢ä¸´çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯ï¼Œå®ƒä»¬ç»å¸¸ä¼šåœ¨ä¸åº”è¯¥ç”Ÿæˆè¾“å‡ºçš„æƒ…å†µä¸‹ç”Ÿæˆè¾“å‡ºã€‚  

This can lead to harmless but nonsensical responses, or more egregious defects like toxicity or dangerous content.  

è¿™å¯èƒ½å¯¼è‡´æ— å®³ä½†æ¯«æ— æ„ä¹‰çš„å›åº”ï¼Œæˆ–è€…æ›´ä¸¥é‡çš„ç¼ºé™·ï¼Œä¾‹å¦‚æ¯’æ€§æˆ–å±é™©å†…å®¹ã€‚  

For example, when asked to extract specific attributes or metadata from a document, an LLM may confidently return values even when those values donâ€™t actually exist. Alternatively, the model may respond in a language other than English because we provided non-English documents in the context.  

ä¾‹å¦‚ï¼Œå½“è¦æ±‚ä»æ–‡æ¡£ä¸­æå–ç‰¹å®šå±æ€§æˆ–å…ƒæ•°æ®æ—¶ï¼ŒLLMå¯èƒ½ä¼šè‡ªä¿¡åœ°è¿”å›å€¼ï¼Œå³ä½¿è¿™äº›å€¼å®é™…ä¸Šå¹¶ä¸å­˜åœ¨ã€‚æˆ–è€…ï¼Œç”±äºæˆ‘ä»¬æä¾›äº†éè‹±è¯­æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œè¯¥æ¨¡å‹å¯èƒ½ä¼šç”¨å…¶ä»–è¯­è¨€è¿›è¡Œå›åº”ã€‚

While we can try to prompt the LLM to return a â€œnot applicableâ€ or â€œunknownâ€ response, itâ€™s not foolproof.  

è™½ç„¶æˆ‘ä»¬å¯ä»¥å°è¯•ä¿ƒä½¿LLMè¿”å›â€œä¸é€‚ç”¨â€æˆ–â€œæœªçŸ¥â€çš„å“åº”ï¼Œä½†è¿™å¹¶ä¸æ˜¯ç»å¯¹å¯é çš„ã€‚  

Even when the log probabilities are available, theyâ€™re a poor indicator of output quality.  

å³ä½¿æœ‰å¯¹æ•°æ¦‚ç‡å¯ç”¨ï¼Œå®ƒä»¬ä¹Ÿå¹¶ä¸æ˜¯è¾“å‡ºè´¨é‡çš„è‰¯å¥½æŒ‡æ ‡ã€‚  

While log probs indicate the likelihood of a token appearing in the output, they donâ€™t necessarily reflect the correctness of the generated text.  

å¯¹æ•°æ¦‚ç‡è¡¨ç¤ºä¸€ä¸ªæ ‡è®°å‡ºç°åœ¨è¾“å‡ºä¸­çš„å¯èƒ½æ€§ï¼Œä½†å¹¶ä¸ä¸€å®šåæ˜ ç”Ÿæˆæ–‡æœ¬çš„å‡†ç¡®æ€§ã€‚  

On the contrary, for instruction-tuned models that are trained to answer queries and generate coherent responses, log probabilities may not be well-calibrated.  

ç›¸åï¼Œå¯¹äºç»è¿‡æŒ‡å¯¼è®­ç»ƒçš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹è¢«è®­ç»ƒæ¥å›ç­”æŸ¥è¯¢å¹¶ç”Ÿæˆè¿è´¯çš„å“åº”ï¼Œå¯¹æ•°æ¦‚ç‡å¯èƒ½æ— æ³•å¾ˆå¥½åœ°æ ¡å‡†ã€‚  

Thus, while a high log probability may indicate that the output is fluent and coherent, it doesnâ€™t mean itâ€™s accurate or relevant.  

å› æ­¤ï¼Œå°½ç®¡é«˜å¯¹æ•°æ¦‚ç‡å¯èƒ½è¡¨æ˜è¾“å‡ºæµç•…è¿è´¯ï¼Œä½†å¹¶ä¸ä»£è¡¨å…¶å‡†ç¡®æˆ–ç›¸å…³ã€‚

While careful prompt engineering can help to an extent, we should complement it with robust guardrails that detect and filter/regenerate undesired output.  

è™½ç„¶è°¨æ…çš„æç¤ºå·¥ç¨‹åœ¨ä¸€å®šç¨‹åº¦ä¸Šæœ‰æ‰€å¸®åŠ©ï¼Œä½†æˆ‘ä»¬åº”è¯¥é…ä»¥å¼ºå¤§çš„é˜²æŠ¤æ ï¼Œä»¥ä¾¿æ£€æµ‹ã€è¿‡æ»¤å’Œé‡æ–°ç”Ÿæˆä¸éœ€è¦çš„è¾“å‡ºã€‚  

For example, OpenAI provides a [content moderation API](https://platform.openai.com/docs/guides/moderation) that can identify unsafe responses such as hate speech, self-harm, or sexual output.  

ä¾‹å¦‚ï¼ŒOpenAI æä¾›äº†å†…å®¹å®¡æ ¸ APIï¼Œå¯ä»¥è¯†åˆ«ä¸è‰¯å›å¤ï¼Œå¦‚ä»‡æ¨è¨€è®ºã€è‡ªæ®‹æˆ–æ€§å†…å®¹ã€‚  

Similarly, there are numerous packages for [detecting personally identifiable information](https://github.com/topics/pii-detection).  

åŒæ ·ï¼Œæœ‰è®¸å¤šè½¯ä»¶åŒ…å¯ç”¨äºæ£€æµ‹ä¸ªäººèº«ä»½ä¿¡æ¯ã€‚  

One benefit is that guardrails are largely agnostic of the use case and can thus be applied broadly to all output in a given language.  

ä¸€ä¸ªå¥½å¤„æ˜¯æŠ¤æ åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¸å…·ä½“ç”¨ä¾‹æ— å…³ï¼Œå› æ­¤å¯ä»¥å¹¿æ³›åº”ç”¨äºç»™å®šè¯­è¨€ä¸­çš„æ‰€æœ‰è¾“å‡ºã€‚  

In addition, with precise retrieval, our system can deterministically respond â€œI donâ€™t knowâ€ if there are no relevant documents.  

æ­¤å¤–ï¼Œé€šè¿‡ç²¾ç¡®æ£€ç´¢ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿå¯ä»¥åœ¨æ²¡æœ‰ç›¸å…³æ–‡æ¡£æ—¶ç¡®å®šæ€§åœ°å›å¤â€œæˆ‘ä¸çŸ¥é“â€ã€‚

A corollary here is that LLMs may fail to produce outputs when they are expected to.  

è¿™é‡Œçš„ä¸€ä¸ªæ¨è®ºæ˜¯ï¼Œå½“æœŸæœ›äº§ç”Ÿè¾“å‡ºæ—¶ï¼ŒLLMså¯èƒ½ä¼šå¤±è´¥ã€‚  

This can happen for various reasons, from straightforward issues like long-tail latencies from API providers to more complex ones such as outputs being blocked by content moderation filters.  

è¿™å¯èƒ½æ˜¯ç”±å„ç§åŸå› å¼•èµ·çš„ï¼Œä»ç®€å•çš„é—®é¢˜ï¼Œæ¯”å¦‚æ¥è‡ª API æä¾›è€…çš„é•¿å°¾å»¶è¿Ÿï¼Œåˆ°æ›´å¤æ‚çš„é—®é¢˜ï¼Œæ¯”å¦‚è¾“å‡ºè¢«å†…å®¹å®¡æ ¸è¿‡æ»¤å™¨é˜»æ­¢ã€‚  

As such, itâ€™s important to consistently log inputs and (potentially a lack of) outputs for debugging and monitoring.  

ä¸ºäº†è°ƒè¯•å’Œç›‘æ§çš„ç›®çš„ï¼Œé‡è¦çš„æ˜¯å§‹ç»ˆè®°å½•è¾“å…¥å’Œï¼ˆå¯èƒ½çš„ï¼‰è¾“å‡ºï¼Œä»¥ä¾¿æ’æŸ¥é—®é¢˜ã€‚

### Hallucinations are a stubborn problem  

1.4.8 å¹»è§‰æ˜¯ä¸€ä¸ªé¡½å›ºçš„é—®é¢˜[](https://applied-llms.org/#hallucinations-are-a-stubborn-problem)

Unlike content safety or PII defects which have a lot of attention and thus seldom occur, factual inconsistencies are stubbornly persistent and more challenging to detect.  

ä¸å†…å®¹å®‰å…¨æˆ– PII ç¼ºé™·ä¸åŒï¼Œè¿™äº›é—®é¢˜å—åˆ°äº†å¾ˆå¤šå…³æ³¨ï¼Œå› æ­¤å¾ˆå°‘å‘ç”Ÿï¼Œè€Œäº‹å®ä¸Šçš„ä¸ä¸€è‡´æ€§å´é¡½å›ºå­˜åœ¨ï¼Œå¹¶ä¸”æ›´å…·æŒ‘æˆ˜æ€§ï¼Œæ›´éš¾ä»¥æ£€æµ‹ã€‚  

Theyâ€™re more common and occur at a baseline rate of 5 - 10%, and from what weâ€™ve learned from LLM providers, it can be challenging to get it below 2%, even on simple tasks such as summarization.  

å®ƒä»¬æ›´ä¸ºå¸¸è§ï¼ŒåŸºçº¿ç‡ä¸º 5-10ï¼…ï¼Œæ ¹æ®æˆ‘ä»¬ä»LLMæä¾›è€…é‚£é‡Œäº†è§£åˆ°çš„ï¼Œå³ä½¿æ˜¯åƒæ‘˜è¦è¿™æ ·ç®€å•çš„ä»»åŠ¡ï¼Œå°†å…¶é™è‡³ 2ï¼…ä»¥ä¸‹ä¹Ÿæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚

To address this, we can combine prompt engineering (upstream of generation) and factual inconsistency guardrails (downstream of generation).  

è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ç»“åˆæç¤ºå·¥ç¨‹ï¼ˆåœ¨ç”Ÿæˆä¹‹å‰ï¼‰å’Œäº‹å®ä¸ä¸€è‡´çš„é˜²æŠ¤æ ï¼ˆåœ¨ç”Ÿæˆä¹‹åï¼‰ã€‚  

For prompt engineering, techniques like CoT help reduce hallucination by getting the LLM to explain its reasoning before finally returning the output.  

ä¸ºäº†å³æ—¶å·¥ç¨‹ï¼Œç±»ä¼¼ CoT çš„æŠ€æœ¯æœ‰åŠ©äºé€šè¿‡è¦æ±‚LLMåœ¨æœ€ç»ˆè¾“å‡ºä¹‹å‰è§£é‡Šå…¶æ¨ç†æ¥å‡å°‘å¹»è§‰ã€‚  

Then, we can apply a [factual inconsistency guardrail](https://eugeneyan.com/writing/finetuning/) to assess the factuality of summaries and filter or regenerate hallucinations.  

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨äº‹å®ä¸ä¸€è‡´çš„é˜²æŠ¤æ æ¥è¯„ä¼°æ‘˜è¦çš„å‡†ç¡®æ€§ï¼Œå¹¶è¿‡æ»¤æˆ–é‡æ–°ç”Ÿæˆè™šæ„å†…å®¹ã€‚  

In some cases, hallucinations can be deterministically detected.  

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¯ä»¥ç¡®å®šæ€§åœ°æ£€æµ‹åˆ°å¹»è§‰ã€‚  

When using resources from RAG retrieval, if the output is structured and identifies what the resources are, you should be able to manually verify theyâ€™re sourced from the input context.  

å½“ä½¿ç”¨æ¥è‡ª RAG æ£€ç´¢çš„èµ„æºæ—¶ï¼Œå¦‚æœè¾“å‡ºæ˜¯ç»“æ„åŒ–çš„å¹¶ä¸”èƒ½å¤Ÿè¯†åˆ«èµ„æºï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿæ‰‹åŠ¨éªŒè¯å®ƒä»¬æ˜¯å¦æ¥è‡ªè¾“å…¥ä¸Šä¸‹æ–‡ã€‚

## Operational: Day-to-day and Org concerns  

2 è¿è¥ï¼šæ—¥å¸¸å’Œç»„ç»‡å…³æ³¨

## Data  

2.1 æ•°æ®[](https://applied-llms.org/#data)

Just as the quality of ingredients determines the taste of a dish, the quality of input data constrains the performance of machine learning systems. In addition, output data is the only way to tell whether the product is working or not.  

å°±åƒé£Ÿæçš„è´¨é‡å†³å®šäº†ä¸€é“èœçš„å‘³é“ä¸€æ ·ï¼Œè¾“å…¥æ•°æ®çš„è´¨é‡ä¼šé™åˆ¶æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¾“å‡ºæ•°æ®æ˜¯åˆ¤æ–­äº§å“æ˜¯å¦æ­£å¸¸è¿è¡Œçš„å”¯ä¸€æ–¹å¼ã€‚  

All the authors focus on the data, looking at inputs and outputs for several hours a week to better understand the data distribution: its modes, its edge cases, and the limitations of models of it.  

æ‰€æœ‰ä½œè€…éƒ½ä¸“æ³¨äºæ•°æ®ï¼Œæ¯å‘¨èŠ±è´¹æ•°å°æ—¶æŸ¥çœ‹è¾“å…¥å’Œè¾“å‡ºï¼Œä»¥æ›´å¥½åœ°ç†è§£æ•°æ®åˆ†å¸ƒï¼šåŒ…æ‹¬å…¶æ¨¡å¼ã€è¾¹ç¼˜æƒ…å†µå’Œæ¨¡å‹çš„å±€é™æ€§ã€‚

### Check for development-prod skew  

2.1.1 æ£€æŸ¥å¼€å‘ä¸ç”Ÿäº§ç¯å¢ƒä¹‹é—´çš„å·®å¼‚[](https://applied-llms.org/#check-for-development-prod-skew)

A common source of errors in traditional machine learning pipelines is _train-serve skew_. This happens when the data used in training differs from what the model encounters in production.  

ä¼ ç»Ÿæœºå™¨å­¦ä¹ æµç¨‹ä¸­å¸¸è§çš„é”™è¯¯æºæ˜¯è®­ç»ƒ-æœåŠ¡åå·®ã€‚è¿™ç§æƒ…å†µå‘ç”Ÿåœ¨è®­ç»ƒä¸­ä½¿ç”¨çš„æ•°æ®ä¸æ¨¡å‹åœ¨ç”Ÿäº§ç¯å¢ƒä¸­é‡åˆ°çš„æ•°æ®ä¸ä¸€è‡´æ—¶ã€‚  

Although we can use LLMs without training or finetuning, hence thereâ€™s no training set, a similar issue arises with development-prod data skew.  

è™½ç„¶æˆ‘ä»¬å¯ä»¥åœ¨æ²¡æœ‰åŸ¹è®­æˆ–å¾®è°ƒçš„æƒ…å†µä¸‹ä½¿ç”¨LLMsï¼Œå› æ­¤æ²¡æœ‰è®­ç»ƒé›†ï¼Œä½†å¼€å‘å’Œç”Ÿäº§æ•°æ®å€¾æ–œä¹Ÿä¼šå‡ºç°ç±»ä¼¼é—®é¢˜ã€‚  

Essentially, the data we test our systems on during development should mirror what the systems will face in production.  

ä»æœ¬è´¨ä¸Šè®²ï¼Œåœ¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æµ‹è¯•ç³»ç»Ÿçš„æ•°æ®åº”è¯¥ä¸ç³»ç»Ÿåœ¨ç”Ÿäº§ä¸­å°†é¢å¯¹çš„æƒ…å†µç›¸ä¸€è‡´ã€‚  

If not, we might find our production accuracy suffering.  

å¦åˆ™ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå‘ç°ç”Ÿäº§ç²¾åº¦ä¸‹é™ã€‚

LLM development-prod skew can be categorized into two types: structural and content-based.  

LLM å¼€å‘-ç”Ÿäº§åå·®å¯åˆ†ä¸ºä¸¤ç§ç±»å‹ï¼šç»“æ„æ€§å’ŒåŸºäºå†…å®¹çš„ã€‚  

Structural skew includes issues like formatting discrepancies, such as differences between a JSON dictionary with a list-type value and a JSON list, inconsistent casing, and errors like typos or sentence fragments.  

ç»“æ„åå·®åŒ…æ‹¬æ ¼å¼ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œä¾‹å¦‚ JSON å­—å…¸ä¸­çš„åˆ—è¡¨ç±»å‹å€¼ä¸ JSON åˆ—è¡¨ä¹‹é—´çš„å·®å¼‚ï¼Œå¤§å°å†™ä¸ä¸€è‡´ï¼Œä»¥åŠæ‹¼å†™é”™è¯¯æˆ–å¥å­ç‰‡æ®µç­‰é—®é¢˜ã€‚  

These errors can lead to unpredictable model performance because different LLMs are trained on specific data formats, and prompts can be highly sensitive to minor changes.  

è¿™äº›é”™è¯¯å¯èƒ½å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸ç¨³å®šï¼Œå› ä¸ºä¸åŒçš„LLMsæ˜¯æ ¹æ®ç‰¹å®šçš„æ•°æ®æ ¼å¼è¿›è¡Œè®­ç»ƒçš„ï¼Œè€Œæç¤ºå¯èƒ½å¯¹ç»†å¾®å˜åŒ–éå¸¸æ•æ„Ÿã€‚  

Content-based or â€œsemanticâ€ skew refers to differences in the meaning or context of the data.Â   

åŸºäºå†…å®¹æˆ–â€œè¯­ä¹‰â€åå·®æ˜¯æŒ‡æ•°æ®åœ¨å«ä¹‰æˆ–ä¸Šä¸‹æ–‡æ–¹é¢çš„å·®å¼‚ã€‚

As in traditional ML, itâ€™s useful to periodically measure skew between the LLM input/output pairs.  

ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ ä¸€æ ·ï¼Œå®šæœŸè¡¡é‡LLMä¸ªè¾“å…¥/è¾“å‡ºå¯¹ä¹‹é—´çš„åå·®æ˜¯å¾ˆæœ‰å¸®åŠ©çš„ã€‚  

Simple metrics like the length of inputs and outputs or specific formatting requirements (e.g., JSON or XML) are straightforward ways to track changes.  

ç®€å•çš„åº¦é‡æ ‡å‡†ï¼Œå¦‚è¾“å…¥å’Œè¾“å‡ºçš„é•¿åº¦æˆ–ç‰¹å®šçš„æ ¼å¼è¦æ±‚ï¼ˆä¾‹å¦‚ JSON æˆ– XMLï¼‰ï¼Œæ˜¯è·Ÿè¸ªå˜åŒ–çš„ç›´æ¥æ–¹å¼ã€‚  

For more â€œadvancedâ€ drift detection, consider clustering embeddings of input/output pairs to detect semantic drift, such as shifts in the topics users are discussing, which could indicate they are exploring areas the model hasnâ€™t been exposed to before.  

ä¸ºäº†æ›´æ·±å…¥åœ°æ£€æµ‹æ¼‚ç§»ï¼Œå¯ä»¥è€ƒè™‘å¯¹è¾“å…¥/è¾“å‡ºå¯¹çš„åµŒå…¥è¿›è¡Œèšç±»ï¼Œä»¥æ£€æµ‹è¯­ä¹‰æ¼‚ç§»ï¼Œæ¯”å¦‚ç”¨æˆ·è®¨è®ºä¸»é¢˜çš„å˜åŒ–ï¼Œè¿™å¯èƒ½è¡¨æ˜ä»–ä»¬æ­£åœ¨æ¢ç´¢æ¨¡å‹ä¹‹å‰æœªæ¥è§¦è¿‡çš„é¢†åŸŸã€‚  

Â 

When testing changes, such as prompt engineering, ensure that hold-out datasets are current and reflect the most recent types of user interactions.  

åœ¨æµ‹è¯•æ›´æ”¹æ—¶ï¼Œå¦‚æç¤ºå·¥ç¨‹ï¼Œè¯·ç¡®ä¿ä¿ç•™æ•°æ®é›†æ˜¯æœ€æ–°çš„ï¼Œå¹¶åæ˜ æœ€è¿‘ç”¨æˆ·äº¤äº’çš„ç±»å‹ã€‚  

For example, if typos are common in production inputs, they should also be present in the hold-out data.  

ä¾‹å¦‚ï¼Œå¦‚æœç”Ÿäº§è¾“å…¥ä¸­ç»å¸¸å‡ºç°æ‹¼å†™é”™è¯¯ï¼Œé‚£ä¹ˆè¿™äº›é”™è¯¯ä¹Ÿåº”è¯¥å­˜åœ¨äºä¿ç•™æ•°æ®ä¸­ã€‚  

Beyond just numerical skew measurements, itâ€™s beneficial to perform qualitative assessments on outputs.  

é™¤äº†ä»…ä»…è¿›è¡Œæ•°å€¼åæ–œåº¦æµ‹é‡ä¹‹å¤–ï¼Œå¯¹è¾“å‡ºè¿›è¡Œå®šæ€§è¯„ä¼°ä¹Ÿæ˜¯å¾ˆæœ‰å¸®åŠ©çš„ã€‚  

Regularly reviewing your modelâ€™s outputsâ€”a practice colloquially known as â€œvibe checksâ€â€”ensures that the results align with expectations and remain relevant to user needs.  

å®šæœŸå®¡æŸ¥æ¨¡å‹çš„è¾“å‡ºï¼Œä¿—ç§°â€œæ°›å›´æ£€æŸ¥â€ï¼Œå¯ç¡®ä¿ç»“æœç¬¦åˆé¢„æœŸå¹¶ä¿æŒä¸ç”¨æˆ·éœ€æ±‚çš„ç›¸å…³æ€§ã€‚  

Finally, incorporating nondeterminism into skew checks is also usefulâ€”by running the pipeline multiple times for each input in our testing dataset and analyzing all outputs, we increase the likelihood of catching anomalies that might occur only occasionally.  

æœ€åï¼Œå°†ä¸ç¡®å®šæ€§çº³å…¥åæ–œæ£€æŸ¥ä¸­ä¹Ÿæ˜¯æœ‰ç”¨çš„â€”â€”é€šè¿‡å¯¹æµ‹è¯•æ•°æ®é›†ä¸­çš„æ¯ä¸ªè¾“å…¥è¿è¡Œç®¡é“å¤šæ¬¡å¹¶åˆ†ææ‰€æœ‰è¾“å‡ºï¼Œæˆ‘ä»¬å¢åŠ äº†æ•æ‰å¶å°”å‘ç”Ÿçš„å¼‚å¸¸çš„å¯èƒ½æ€§ã€‚

### Look at samples of LLM inputs and outputs every day  

æ¯å¤©æŸ¥çœ‹LLMä¸ªè¾“å…¥å’Œè¾“å‡ºæ ·æœ¬[](https://applied-llms.org/#look-at-samples-of-llm-inputs-and-outputs-every-day)

LLMs are dynamic and constantly evolving.  

LLMs æ˜¯åŠ¨æ€çš„ï¼Œå¹¶ä¸”ä¸æ–­æ¼”å˜ã€‚  

Despite their impressive zero-shot capabilities and often delightful outputs, their failure modes can be highly unpredictable.  

å°½ç®¡å®ƒä»¬å…·æœ‰ä»¤äººå°è±¡æ·±åˆ»çš„é›¶å°„å‡»èƒ½åŠ›å’Œå¸¸å¸¸ä»¤äººæ„‰æ‚¦çš„è¾“å‡ºï¼Œä½†å®ƒä»¬çš„æ•…éšœæ¨¡å¼å¯èƒ½éå¸¸ä¸å¯é¢„æµ‹ï¼Œè€Œä¸”éš¾ä»¥é¢„æ–™ã€‚  

For custom tasks, regularly reviewing data samples is essential to developing an intuitive understanding of how LLMs perform.  

å¯¹äºè‡ªå®šä¹‰ä»»åŠ¡ï¼Œå®šæœŸå®¡æŸ¥æ•°æ®æ ·æœ¬æ˜¯åŸ¹å…»å¯¹LLMsè¡¨ç°ç›´è§‚ç†è§£çš„å…³é”®ã€‚

Input-output pairs from production are the â€œreal things, real placesâ€ (_genchi genbutsu_) of LLM applications, and they cannot be substituted. [Recent research](https://arxiv.org/abs/2404.12272) highlighted that developersâ€™ perceptions of what constitutes â€œgoodâ€ and â€œbadâ€ outputs shift as they interact with more data (i.e., _criteria drift_).  

ç”Ÿäº§ä¸­çš„è¾“å…¥è¾“å‡ºå¯¹æ˜¯LLMåº”ç”¨ç¨‹åºçš„â€œçœŸå®äº‹ç‰©ã€çœŸå®åœ°ç‚¹â€ï¼ˆç°åœºåŸç‰©ï¼‰, æ— æ³•æ›¿ä»£ã€‚æœ€è¿‘çš„ç ”ç©¶å¼ºè°ƒï¼Œéšç€å¼€å‘äººå‘˜ä¸æ›´å¤šæ•°æ®äº’åŠ¨ï¼Œä»–ä»¬å¯¹â€œå¥½â€å’Œâ€œåâ€è¾“å‡ºçš„è®¤çŸ¥ä¼šå‘ç”Ÿå˜åŒ–ï¼ˆå³ï¼Œæ ‡å‡†æ¼‚ç§»ï¼‰ã€‚  

While developers can come up with some criteria upfront for evaluating LLM outputs, these predefined criteria are often incomplete.  

å¼€å‘äººå‘˜å¯ä»¥äº‹å…ˆåˆ¶å®šä¸€äº›æ ‡å‡†æ¥è¯„ä¼°LLMçš„è¾“å‡ºï¼Œä½†è¿™äº›é¢„å®šä¹‰çš„æ ‡å‡†é€šå¸¸æ˜¯ä¸å®Œæ•´çš„ã€‚  

For instance, during the course of development, we might update the prompt to increase the probability of good responses and decrease the probability of bad ones.  

ä¾‹å¦‚ï¼Œåœ¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šæ›´æ–°æç¤ºï¼Œä»¥å¢åŠ è·å¾—è‰¯å¥½å›åº”çš„å¯èƒ½æ€§ï¼Œå‡å°‘è·å¾—ä¸è‰¯å›åº”çš„å¯èƒ½æ€§ã€‚  

This iterative process of evaluation, reevaluation, and criteria update is necessary, as itâ€™s difficult to predict either LLM behavior or human preference without directly observing the outputs.  

è¿™ç§è¯„ä¼°ã€é‡æ–°è¯„ä¼°å’Œæ ‡å‡†æ›´æ–°çš„è¿­ä»£è¿‡ç¨‹æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºå¾ˆéš¾åœ¨æ²¡æœ‰ç›´æ¥è§‚å¯Ÿè¾“å‡ºçš„æƒ…å†µä¸‹é¢„æµ‹LLMçš„è¡Œä¸ºæˆ–äººç±»åå¥½ã€‚

To manage this effectively, we should log LLM inputs and outputs.  

ä¸ºäº†æœ‰æ•ˆç®¡ç†ï¼Œæˆ‘ä»¬åº”è¯¥è®°å½•LLMçš„è¾“å…¥å’Œè¾“å‡ºã€‚  

By examining a sample of these logs daily, we can quickly identify and adapt to new patterns or failure modes.  

é€šè¿‡æ¯æ—¥æ£€æŸ¥è¿™äº›æ—¥å¿—æ ·æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿè¯†åˆ«å¹¶é€‚åº”æ–°çš„æ¨¡å¼æˆ–æ•…éšœæ¨¡å¼ã€‚  

When we spot a new issue, we can immediately write an assertion or eval around it.  

å½“æˆ‘ä»¬å‘ç°ä¸€ä¸ªæ–°é—®é¢˜æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç«‹å³ç¼–å†™ä¸€ä¸ªæ–­è¨€æˆ–è¯„ä¼°ã€‚  

Similarly, any updates to failure mode definitions should be reflected in the evaluation criteria.  

åŒæ ·ï¼Œä»»ä½•å¯¹æ•…éšœæ¨¡å¼å®šä¹‰çš„æ›´æ–°éƒ½åº”è¯¥åæ˜ åœ¨è¯„ä¼°æ ‡å‡†ä¸­ã€‚  

These â€œvibe checksâ€ are signals of bad outputs; code and assertions operationalize them.  

è¿™äº›â€œæ°›å›´æ£€æŸ¥â€æ˜¯ä¸è‰¯è¾“å‡ºçš„ä¿¡å·ï¼›ä»£ç å’Œæ–­è¨€å°†å…¶å®ç°ã€‚  

Finally, this attitude must be socialized, for example by adding review or annotation of inputs and outputs to your on-call rotation.  

æœ€åï¼Œè¿™ç§æ€åº¦å¿…é¡»å¾—åˆ°ç¤¾ä¼šåŒ–ï¼Œä¾‹å¦‚é€šè¿‡å°†è¾“å…¥å’Œè¾“å‡ºçš„å®¡æŸ¥æˆ–æ³¨é‡Šæ·»åŠ åˆ°æ‚¨çš„å€¼ç­è½®æ¢ä¸­ã€‚

## Working with models  

2.2 æ“ä½œæ¨¡å‹[](https://applied-llms.org/#working-with-models)

With LLM APIs, we can rely on intelligence from a handful of providers.  

åˆ©ç”¨LLMä¸ª APIï¼Œæˆ‘ä»¬å¯ä»¥ä¾èµ–æ¥è‡ªå°‘æ•°æä¾›å•†çš„æ™ºæ…§ã€‚  

While this is a boon, these dependencies also involve trade-offs on performance, latency, throughput, and cost. Also, as newer, better models drop (almost every month in the past year), we should be prepared to update our products as we deprecate old models and migrate to newer models.  

è™½ç„¶è¿™æ˜¯ä¸€ä¸ªå¥½å¤„ï¼Œä½†è¿™äº›ä¾èµ–ä¹Ÿæ¶‰åŠæ€§èƒ½ã€å»¶è¿Ÿã€ååé‡å’Œæˆæœ¬æ–¹é¢çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œéšç€æ›´æ–°æ›´å¥½çš„æ¨¡å‹çš„å‘å¸ƒï¼ˆåœ¨è¿‡å»ä¸€å¹´å‡ ä¹æ¯ä¸ªæœˆéƒ½æœ‰ï¼‰ï¼Œæˆ‘ä»¬åº”è¯¥å‡†å¤‡æ›´æ–°æˆ‘ä»¬çš„äº§å“ï¼Œå¼ƒç”¨æ—§æ¨¡å‹å¹¶è¿ç§»åˆ°æ–°æ¨¡å‹ã€‚  

In this section, we share our lessons from working with technologies we donâ€™t have full control over, where the models canâ€™t be self-hosted and managed.  

åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬åˆ†äº«äº†åœ¨ä¸æˆ‘ä»¬æ— æ³•å®Œå…¨æ§åˆ¶çš„æŠ€æœ¯åˆä½œæ—¶çš„ç»éªŒæ•™è®­ï¼Œå…¶ä¸­æ¨¡å‹æ— æ³•è‡ªè¡Œæ‰˜ç®¡å’Œç®¡ç†ã€‚

### Generate structured output to ease downstream integration  

ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºä»¥ä¾¿äºåç»­é›†æˆæ›´åŠ ä¾¿æ·[](https://applied-llms.org/#generate-structured-output-to-ease-downstream-integration)

For most real-world use cases, the output of an LLM will be consumed by a downstream application via some machine-readable format.  

å¯¹äºå¤§å¤šæ•°å®é™…åº”ç”¨åœºæ™¯ï¼ŒLLMçš„è¾“å‡ºå°†è¢«ä¸‹æ¸¸åº”ç”¨ç¨‹åºä»¥æŸç§æœºå™¨å¯è¯»æ ¼å¼æ¶ˆè´¹ã€‚  

For example, [Rechat](https://www.youtube.com/watch?v=B_DMMlDuJB0), a real-estate CRM, required structured responses for the front end to render widgets. Similarly, [Boba](https://martinfowler.com/articles/building-boba.html), a tool for generating product strategy ideas, needed structured output with fields for title, summary, plausibility score, and time horizon.  

ä¾‹å¦‚ï¼ŒRechat æ˜¯ä¸€æ¬¾æˆ¿åœ°äº§ CRMï¼Œéœ€è¦ç»“æ„åŒ–çš„å“åº”æ¥æ¸²æŸ“å‰ç«¯å°éƒ¨ä»¶ã€‚ç±»ä¼¼åœ°ï¼ŒBoba æ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆäº§å“æˆ˜ç•¥æƒ³æ³•çš„å·¥å…·ï¼Œéœ€è¦ç»“æ„åŒ–çš„è¾“å‡ºï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€æ‘˜è¦ã€å¯ä¿¡åº¦è¯„åˆ†å’Œæ—¶é—´èŒƒå›´å­—æ®µã€‚  

Finally, LinkedIn shared about [constraining the LLM to generate YAML](https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product), which is then used to decide which skill to use, as well as provide the parameters to invoke the skill.  

æœ€åï¼ŒLinkedIn åˆ†äº«äº†å¦‚ä½•çº¦æŸLLMç”Ÿæˆ YAMLï¼Œä»¥ä¾¿å†³å®šä½¿ç”¨å“ªç§æŠ€èƒ½ï¼Œå¹¶æä¾›è°ƒç”¨è¯¥æŠ€èƒ½æ‰€éœ€çš„å‚æ•°ã€‚

This application pattern is an extreme version of Postelâ€™s Law: be liberal in what you accept (arbitrary natural language) and conservative in what you send (typed, machine-readable objects).  

è¿™ç§åº”ç”¨ç¨‹åºæ¨¡å¼æ˜¯ Postel æ³•åˆ™çš„ä¸€ä¸ªæç«¯ä½“ç°ï¼šåœ¨æ¥å—å†…å®¹æ—¶è¦å®½å®¹ï¼ˆæ¥å—ä»»æ„è‡ªç„¶è¯­è¨€ï¼‰ï¼Œåœ¨å‘é€å†…å®¹æ—¶è¦è°¨æ…ï¼ˆå‘é€ç±»å‹åŒ–ã€æœºå™¨å¯è¯»çš„å¯¹è±¡ï¼‰ã€‚  

As such, we expect it to be extremely durable.  

å› æ­¤ï¼Œæˆ‘ä»¬æœŸå¾…å®ƒå…·æœ‰æé«˜çš„è€ç”¨æ€§ã€‚

Currently, [Instructor](https://github.com/jxnl/instructor) and [Outlines](https://github.com/outlines-dev/outlines) are the de facto standards for coaxing structured output from LLMs. If youâ€™re using an LLM API (e.g., Anthropic, OpenAI), use Instructor; if youâ€™re working with a self-hosted model (e.g., Huggingface), use Outlines.  

ç›®å‰ï¼ŒInstructor å’Œ Outlines æ˜¯ä» LLMs ä¸­è·å–ç»“æ„åŒ–è¾“å‡ºçš„äº‹å®æ ‡å‡†ã€‚å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨ LLM APIï¼ˆä¾‹å¦‚ Anthropicã€OpenAIï¼‰ï¼Œè¯·ä½¿ç”¨ Instructorï¼›å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨è‡ªæ‰˜ç®¡æ¨¡å‹ï¼ˆä¾‹å¦‚ Huggingfaceï¼‰ï¼Œè¯·ä½¿ç”¨ Outlinesã€‚

### Migrating prompts across models is a pain in the ass  

2.2.2 åœ¨ä¸åŒæ¨¡å‹é—´è¿ç§»æç¤ºæ˜¯ä¸€ä»¶å¾ˆéº»çƒ¦çš„äº‹æƒ…[](https://applied-llms.org/#migrating-prompts-across-models-is-a-pain-in-the-ass)

Sometimes, our carefully crafted prompts work superbly with one model but fall flat with another.  

æœ‰æ—¶å€™ï¼Œæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡çš„æç¤ºå¯¹ä¸€ä¸ªæ¨¡å‹æ•ˆæœå¾ˆå¥½ï¼Œä½†å¯¹å¦ä¸€ä¸ªæ¨¡å‹å´æ•ˆæœä¸ä½³ã€‚  

This can happen when weâ€™re switching between various model providers, as well as when we upgrade across versions of the same model.  

å½“æˆ‘ä»¬åœ¨ä¸åŒçš„æ¨¡å‹æä¾›å•†ä¹‹é—´åˆ‡æ¢ï¼Œæˆ–è€…åœ¨åŒä¸€æ¨¡å‹çš„ä¸åŒç‰ˆæœ¬ä¹‹é—´å‡çº§æ—¶ï¼Œå°±ä¼šå‡ºç°è¿™ç§æƒ…å†µã€‚  

Â 

For example, Voiceflow found that [migrating from gpt-3.5-turbo-0301 to gpt-3.5-turbo-1106 led to a 10% drop](https://www.voiceflow.com/blog/how-much-do-chatgpt-versions-affect-real-world-performance) in their intent classification task. (Thankfully, they had evals!) Similarly, [GoDaddy observed a trend in the positive direction](https://www.godaddy.com/resources/news/llm-from-the-trenches-10-lessons-learned-operationalizing-models-at-godaddy#h-3-prompts-aren-t-portable-across-models), where upgrading to version 1106 narrowed the performance gap between gpt-3.5-turbo and gpt-4.  

ä¾‹å¦‚ï¼ŒVoiceflow å‘ç°å°†ä» gpt-3.5-turbo-0301 è¿ç§»åˆ° gpt-3.5-turbo-1106 å¯¼è‡´ä»–ä»¬çš„æ„å›¾åˆ†ç±»ä»»åŠ¡ä¸‹é™äº† 10%ã€‚ï¼ˆå¹¸è¿çš„æ˜¯ï¼Œä»–ä»¬è¿›è¡Œäº†è¯„ä¼°ï¼ï¼‰åŒæ ·ï¼ŒGoDaddy è§‚å¯Ÿåˆ°äº†ä¸€ä¸ªæ­£å‘çš„è¶‹åŠ¿ï¼Œå‡çº§åˆ° 1106 ç‰ˆæœ¬ç¼©å°äº† gpt-3.5-turbo å’Œ gpt-4 ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚  

(Or, if youâ€™re a glass-half-full person, you might be disappointed that gpt-4â€™s lead was reduced with the new upgrade)  

æˆ–è€…ï¼Œå¦‚æœä½ æ˜¯ä¸€ä¸ªä¹è§‚çš„äººï¼Œä½ å¯èƒ½ä¼šå¯¹ gpt-4 çš„é¢†å…ˆåœ°ä½åœ¨æ–°å‡çº§ä¸­è¢«ç¼©å°æ„Ÿåˆ°å¤±æœ›ã€‚

Thus, if we have to migrate prompts across models, expect it to take more time than simply swapping the API endpoint.  

å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬éœ€è¦åœ¨ä¸åŒæ¨¡å‹ä¹‹é—´è¿ç§»æç¤ºï¼Œé¢„è®¡è¿™å°†æ¯”ç®€å•äº¤æ¢ API ç«¯ç‚¹éœ€è¦æ›´å¤šçš„æ—¶é—´ã€‚  

Donâ€™t assume that plugging in the same prompt will lead to similar or better results.  

ä¸è¦å‡è®¾æ’å…¥ç›¸åŒçš„æç¤ºä¼šå¸¦æ¥ç›¸ä¼¼æˆ–æ›´å¥½çš„ç»“æœã€‚  

Also, having reliable, automated evals helps with measuring task performance before and after migration, and reduces the effort needed for manual verification.  

åŒæ—¶ï¼Œæ‹¥æœ‰å¯é çš„è‡ªåŠ¨è¯„ä¼°æœ‰åŠ©äºåœ¨è¿ç§»å‰åè¡¡é‡ä»»åŠ¡ç»©æ•ˆï¼Œå‡å°‘æ‰‹åŠ¨éªŒè¯æ‰€éœ€çš„å·¥ä½œé‡ã€‚

### Version and pin your models  

ç‰ˆæœ¬åŒ–å’Œå›ºå®šæ‚¨çš„æ¨¡å‹[](https://applied-llms.org/#version-and-pin-your-models)

In any machine learning pipeline, â€œ[changing anything changes everything](https://papers.nips.cc/paper_files/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html)â€.  

åœ¨ä»»ä½•æœºå™¨å­¦ä¹ æµç¨‹ä¸­ï¼Œâ€œæ”¹å˜ä¸€ç‚¹å°±ä¼šæ”¹å˜ä¸€åˆ‡â€æ˜¯å¾ˆé‡è¦çš„ã€‚  

This is particularly relevant as we rely on components like large language models (LLMs) that we donâ€™t train ourselves and that can change without our knowledge.  

è¿™ä¸€ç‚¹å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºæˆ‘ä»¬ä¾èµ–è¯¸å¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹‹ç±»çš„ç»„ä»¶ï¼Œè¿™äº›ç»„ä»¶ä¸æ˜¯æˆ‘ä»¬è‡ªå·±è®­ç»ƒçš„ï¼Œè€Œä¸”å¯èƒ½åœ¨æˆ‘ä»¬ä¸çŸ¥æƒ…çš„æƒ…å†µä¸‹å‘ç”Ÿå˜åŒ–ã€‚

Fortunately, many model providers offer the option to â€œpinâ€ specific model versions (e.g., gpt-4-turbo-1106).  

å¹¸è¿çš„æ˜¯ï¼Œè®¸å¤šæ¨¡å‹æä¾›å•†æä¾›äº†â€œå›ºå®šâ€ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬çš„é€‰é¡¹ï¼Œä¾‹å¦‚ gpt-4-turbo-1106ã€‚  

This enables us to use a specific version of the model weights, ensuring they remain unchanged.  

è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨æ¨¡å‹æƒé‡çš„ç‰¹å®šç‰ˆæœ¬ï¼Œç¡®ä¿å®ƒä»¬ä¿æŒä¸å˜ã€‚  

Pinning model versions in production can help avoid unexpected changes in model behavior, which could lead to customer complaints about issues that may crop up when a model is swapped, such as overly verbose outputs or other unforeseen failure modes.  

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å›ºå®šæ¨¡å‹ç‰ˆæœ¬å¯ä»¥å¸®åŠ©é¿å…æ¨¡å‹è¡Œä¸ºçš„æ„å¤–æ›´æ”¹ï¼Œè¿™å¯èƒ½å¯¼è‡´å®¢æˆ·æŠ•è¯‰å‡ºç°é—®é¢˜ï¼Œä¾‹å¦‚æ¨¡å‹äº¤æ¢æ—¶å¯èƒ½å‡ºç°çš„è¿‡äºå†—é•¿çš„è¾“å‡ºæˆ–å…¶ä»–æœªé¢„æ–™çš„æ•…éšœæ¨¡å¼ã€‚

Additionally, consider maintaining a shadow pipeline that mirrors your production setup but uses the latest model versions.  

æ­¤å¤–ï¼Œè¯·è€ƒè™‘ç»´æŠ¤ä¸€ä¸ªå½±å­ç®¡é“ï¼Œå®ƒé•œåƒæ‚¨çš„ç”Ÿäº§ç¯å¢ƒè®¾ç½®ï¼Œä½†ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹ç‰ˆæœ¬ã€‚  

This enables safe experimentation and testing with new releases.  

è¿™æ ·å¯ä»¥å®‰å…¨åœ°è¿›è¡Œæ–°ç‰ˆæœ¬çš„å®éªŒå’Œæµ‹è¯•ã€‚  

Once youâ€™ve validated the stability and quality of the outputs from these newer models, you can confidently update the model versions in your production environment.  

åœ¨æ‚¨ç¡®è®¤äº†è¿™äº›æ›´æ–°æ¨¡å‹äº§ç”Ÿçš„ç¨³å®šæ€§å’Œè´¨é‡åï¼Œæ‚¨å¯ä»¥æœ‰ä¿¡å¿ƒåœ°åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ›´æ–°æ¨¡å‹ç‰ˆæœ¬ã€‚

### Choose the smallest model that gets the job done  

é€‰æ‹©æœ€é€‚åˆçš„å°å‹æ¨¡å‹å®Œæˆä»»åŠ¡[](https://applied-llms.org/#choose-the-smallest-model-that-gets-the-job-done)

When working on a new application, itâ€™s tempting to use the biggest, most powerful model available.  

åœ¨å¼€å‘æ–°åº”ç”¨ç¨‹åºæ—¶ï¼Œå¾ˆå®¹æ˜“è¢«æœ€å¤§ã€æœ€å¼ºå¤§çš„æ¨¡å‹æ‰€å¸å¼•ã€‚  

But once weâ€™ve established that the task is technically feasible, itâ€™s worth experimenting if a smaller model can achieve comparable results.  

ä½†æ˜¯ä¸€æ—¦æˆ‘ä»¬ç¡®å®šä»»åŠ¡åœ¨æŠ€æœ¯ä¸Šæ˜¯å¯è¡Œçš„ï¼Œå°±å€¼å¾—å°è¯•ä¸€ä¸‹ï¼Œçœ‹çœ‹æ˜¯å¦è¾ƒå°çš„æ¨¡å‹å¯ä»¥è¾¾åˆ°ç›¸ä¼¼çš„æ•ˆæœã€‚

The benefits of a smaller model are lower latency and cost. While it may be weaker, techniques like chain-of-thought, n-shot prompts, and in-context learning can help smaller models punch above their weight.  

è¾ƒå°æ¨¡å‹çš„ä¼˜åŠ¿åœ¨äºæ›´ä½çš„å»¶è¿Ÿå’Œæˆæœ¬ã€‚è™½ç„¶å®ƒå¯èƒ½å¼±ä¸€äº›ï¼Œä½†é€šè¿‡æ€ç»´é“¾ã€n-shot æç¤ºå’Œä¸Šä¸‹æ–‡å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œè¾ƒå°æ¨¡å‹å¯ä»¥å‘æŒ¥å‡ºè‰²çš„ä½œç”¨ã€‚  

Beyond LLM APIs, finetuning our specific tasks can also help increase performance.  

é™¤äº†LLMä¸ª API ä¹‹å¤–ï¼Œä¼˜åŒ–æˆ‘ä»¬çš„ç‰¹å®šä»»åŠ¡ä¹Ÿå¯ä»¥å¸®åŠ©æé«˜æ€§èƒ½ã€‚

Taken together, a carefully crafted workflow using a smaller model can often match, or even surpass, the output quality of a single large model, while being faster and cheaper.  

é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å·¥ä½œæµç¨‹ï¼Œä½¿ç”¨è¾ƒå°çš„æ¨¡å‹å¾€å¾€å¯ä»¥è¾¾åˆ°ç”šè‡³è¶…è¿‡å•ä¸ªå¤§å‹æ¨¡å‹çš„è¾“å‡ºè´¨é‡ï¼ŒåŒæ—¶æ›´å¿«é€Ÿå’Œæ›´ç»æµã€‚  

For example, this [tweet](https://twitter.com/mattshumer_/status/1770823530394833242) shares anecdata of how Haiku + 10-shot prompt outperforms zero-shot Opus and GPT-4.  

ä¸¾ä¾‹æ¥è¯´ï¼Œè¿™æ¡æ¨æ–‡åˆ†äº«äº† Haiku + 10-shot æç¤ºçš„å®ä¾‹æ•°æ®ï¼Œæ˜¾ç¤ºå…¶èƒœè¿‡é›¶-shot Opus å’Œ GPT-4ã€‚  

In the long term, we expect to see more examples of [flow-engineering](https://twitter.com/karpathy/status/1748043513156272416) with smaller models as the optimal balance of output quality, latency, and cost.  

ä»é•¿è¿œæ¥çœ‹ï¼Œæˆ‘ä»¬é¢„è®¡ä¼šçœ‹åˆ°æ›´å¤šçš„æµç¨‹å·¥ç¨‹ç¤ºä¾‹ï¼Œé‡‡ç”¨æ›´å°çš„æ¨¡å‹ï¼Œä»¥å®ç°è¾“å‡ºè´¨é‡ã€å»¶è¿Ÿå’Œæˆæœ¬çš„æœ€ä½³å¹³è¡¡ã€‚

As another example, take the humble classification task.  

ä¸¾ä¸ªä¾‹å­ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹ç®€å•çš„åˆ†ç±»ä»»åŠ¡ã€‚  

Lightweight models like DistilBERT (67M parameters) are a surprisingly strong baseline.  

DistilBERTï¼ˆ6700 ä¸‡å‚æ•°ï¼‰ç­‰è½»é‡çº§æ¨¡å‹æ˜¯ä¸€ä¸ªå‡ºä¹æ„æ–™çš„å¼ºå¤§åŸºå‡†ã€‚  

The 400M parameter DistilBART is another great optionâ€”when finetuned on open-source data, it could [identify hallucinations with an ROC-AUC of 0.84](https://eugeneyan.com/writing/finetuning/), surpassing most LLMs at less than 5% of the latency and cost.  

400M å‚æ•°çš„ DistilBART æ˜¯å¦ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©â€”â€”åœ¨å¼€æºæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒåï¼Œå®ƒå¯ä»¥ä»¥ 0.84 çš„ ROC-AUC è¯†åˆ«å¹»è§‰ï¼Œè¶…è¿‡å¤§å¤šæ•°LLMsï¼Œè€Œä¸”å»¶è¿Ÿå’Œæˆæœ¬ä¸åˆ° 5%ã€‚

The point is, donâ€™t overlook smaller models.  

å…³é”®æ˜¯ï¼Œä¸è¦å¿½è§†è¾ƒå°çš„æ¨¡å‹ã€‚  

While itâ€™s easy to throw a massive model at every problem, with some creativity and experimentation, we can often find a more efficient solution.  

è™½ç„¶å°†åºå¤§çš„æ¨¡å‹åº”ç”¨äºæ¯ä¸ªé—®é¢˜å¾ˆå®¹æ˜“ï¼Œä½†é€šè¿‡ä¸€äº›åˆ›æ„å’Œå®éªŒï¼Œæˆ‘ä»¬é€šå¸¸å¯ä»¥æ‰¾åˆ°æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚  

Â 

## Product  

2.3 äº§å“[](https://applied-llms.org/#product)

While new technology offers new possibilities, the principles of building great products are timeless.  

å°½ç®¡æ–°æŠ€æœ¯å¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ï¼Œä½†æ„å»ºä¼˜ç§€äº§å“çš„åŸåˆ™å´æ˜¯æ°¸æ’ä¸å˜çš„ã€‚  

Thus, even if weâ€™re solving new problems for the first time, we donâ€™t have to reinvent the wheel on product design.  

å› æ­¤ï¼Œå³ä½¿æˆ‘ä»¬ç¬¬ä¸€æ¬¡è§£å†³æ–°é—®é¢˜ï¼Œä¹Ÿä¸å¿…é‡æ–°å‘æ˜äº§å“è®¾è®¡è¿™ä¸ªæ¦‚å¿µã€‚  

Thereâ€™s a lot to gain from grounding our LLM application development in solid product fundamentals, allowing us to deliver real value to the people we serve.  

é€šè¿‡åœ¨æ‰å®çš„äº§å“åŸºç¡€ä¸Šå¼€å±•LLMåº”ç”¨å¼€å‘ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—å¾ˆå¤šå¥½å¤„ï¼Œä»è€Œä¸ºæˆ‘ä»¬æœåŠ¡çš„äººä»¬æä¾›çœŸæ­£çš„ä»·å€¼ã€‚

### Involve design early and often  

æ—©æœŸå’Œé¢‘ç¹åœ°å‚ä¸è®¾è®¡[](https://applied-llms.org/#involve-design-early-and-often)

Having a designer will push you to understand and think deeply about how your product can be built and presented to users.  

æœ‰ä¸€ä½è®¾è®¡å¸ˆå°†ä¿ƒä½¿æ‚¨æ·±å…¥æ€è€ƒå’Œç†è§£å¦‚ä½•æ„å»ºå’Œå‘ˆç°äº§å“ç»™ç”¨æˆ·ã€‚  

We sometimes stereotype designers as folks who take things and make them pretty.  

æˆ‘ä»¬æœ‰æ—¶ä¼šæŠŠè®¾è®¡å¸ˆåˆ»æ¿åœ°å®šä¹‰ä¸ºé‚£äº›æ‹¿æ¥ä¸œè¥¿ç„¶åè®©å®ƒä»¬å˜å¾—æ¼‚äº®çš„äººã€‚  

But beyond just the user interface, they also rethink how the user experience can be improved, even if it means breaking existing rules and paradigms.  

é™¤äº†ç”¨æˆ·ç•Œé¢ï¼Œä»–ä»¬è¿˜é‡æ–°æ€è€ƒå¦‚ä½•æ”¹è¿›ç”¨æˆ·ä½“éªŒï¼Œå³ä½¿è¿™æ„å‘³ç€æ‰“ç ´ç°æœ‰è§„åˆ™å’ŒèŒƒå¼ã€‚

Designers are especially gifted at reframing the userâ€™s needs into various forms. Some of these forms are more tractable to solve than others, and thus, they may offer more or fewer opportunities for AI solutions.  

è®¾è®¡å¸ˆæ“…é•¿å°†ç”¨æˆ·éœ€æ±‚é‡æ–°æ„å»ºæˆä¸åŒå½¢å¼ã€‚æœ‰äº›å½¢å¼æ›´å®¹æ˜“è§£å†³ï¼Œå› æ­¤å¯èƒ½æä¾›æ›´å¤šæˆ–æ›´å°‘çš„ AI è§£å†³æ–¹æ¡ˆæœºä¼šã€‚  

Like many other products, building AI products should be centered around the job to be done, not the technology that powers them.  

ä¸è®¸å¤šå…¶ä»–äº§å“ä¸€æ ·ï¼Œæ„å»º AI äº§å“åº”è¯¥ä»¥éœ€æ±‚ä¸ºä¸­å¿ƒï¼Œè€Œä¸æ˜¯æŠ€æœ¯ã€‚

Focus on asking yourself: â€œWhat job is the user asking this product to do for them?  

ä¸“æ³¨äºé—®è‡ªå·±ï¼šâ€œç”¨æˆ·å¸Œæœ›è¿™ä¸ªäº§å“ä¸ºä»–ä»¬è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ  

Is that job something a chatbot would be good at? How about autocomplete?  

è¿™ä»½å·¥ä½œé€‚åˆèŠå¤©æœºå™¨äººå—ï¼Ÿè‡ªåŠ¨å®Œæˆåˆå¦‚ä½•ï¼Ÿ  

Maybe something different!â€ Consider the existing [design patterns](https://www.tidepool.so/blog/emerging-ux-patterns-for-generative-ai-apps-copilots) and how they relate to the job-to-be-done.  

â€œä¹Ÿè®¸æœ‰äº›ä¸åŒï¼â€è€ƒè™‘ç°æœ‰çš„è®¾è®¡æ¨¡å¼ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•ä¸å·¥ä½œä»»åŠ¡ç›¸å…³ã€‚  

These are the invaluable assets that designers add to your teamâ€™s capabilities.  

è®¾è®¡å¸ˆä¸ºæ‚¨çš„å›¢é˜Ÿå¢æ·»äº†å®è´µçš„èƒ½åŠ›èµ„äº§ã€‚

### Design your UX for Human-In-The-Loop  

2.3.2 ä¸ºäººåœ¨ç¯è®¾è®¡æ‚¨çš„ UX[](https://applied-llms.org/#design-your-ux-for-human-in-the-loop)

One way to get quality annotations is to integrate Human-in-the-Loop (HITL) into the user experience (UX).  

ä¸ºäº†è·å¾—é«˜è´¨é‡çš„æ³¨é‡Šï¼Œä¸€ç§æ–¹æ³•æ˜¯å°†äººåœ¨ç¯ï¼ˆHITLï¼‰é›†æˆåˆ°ç”¨æˆ·ä½“éªŒï¼ˆUXï¼‰ä¸­ã€‚  

By allowing users to provide feedback and corrections easily, we can improve the immediate output and collect valuable data to improve our models.  

é€šè¿‡è®©ç”¨æˆ·è½»æ¾æä¾›åé¦ˆå’Œæ›´æ­£ï¼Œæˆ‘ä»¬å¯ä»¥æ”¹è¿›å³æ—¶è¾“å‡ºï¼Œå¹¶æ”¶é›†å®è´µçš„æ•°æ®ä»¥ä¼˜åŒ–æˆ‘ä»¬çš„æ¨¡å‹ã€‚

Imagine an e-commerce platform where users upload and categorize their products.  

æƒ³è±¡ä¸€ä¸ªç”µå­å•†åŠ¡å¹³å°ï¼Œç”¨æˆ·å¯ä»¥ä¸Šä¼ å¹¶å¯¹å…¶äº§å“è¿›è¡Œåˆ†ç±»ã€‚  

There are several ways we could design the UX:  

æˆ‘ä»¬å¯ä»¥ç”¨å¤šç§æ–¹å¼æ¥è®¾è®¡ç”¨æˆ·ä½“éªŒ

-   The user manually selects the right product category; an LLM periodically checks new products and corrects miscategorization on the backend.  
    
    ç”¨æˆ·æ‰‹åŠ¨é€‰æ‹©æ­£ç¡®çš„äº§å“ç±»åˆ«ï¼›LLMå®šæœŸæ£€æŸ¥æ–°äº§å“ï¼Œå¹¶åœ¨åç«¯çº æ­£é”™è¯¯çš„åˆ†ç±»ã€‚
-   The user doesnâ€™t select any category at all; an LLM periodically categorizes products on the backend (with potential errors).  
    
    ç”¨æˆ·æ ¹æœ¬æ²¡æœ‰é€‰æ‹©ä»»ä½•ç±»åˆ«ï¼›åç«¯å®šæœŸä½¿ç”¨LLMå¯¹äº§å“è¿›è¡Œåˆ†ç±»ï¼ˆå¯èƒ½å­˜åœ¨é”™è¯¯ï¼‰ã€‚
-   An LLM suggests a product category in real-time, which the user can validate and update as needed.  
    
    ä¸€ä¸ªLLMåœ¨å®æ—¶æä¾›å»ºè®®ä¸€ä¸ªäº§å“ç±»åˆ«ï¼Œç”¨æˆ·å¯ä»¥éªŒè¯å¹¶æ ¹æ®éœ€è¦æ›´æ–°ã€‚

While all three approaches involve an LLM, they provide very different UXes.  

å°½ç®¡è¿™ä¸‰ç§æ–¹æ³•éƒ½æ¶‰åŠåˆ°ä¸€ä¸ªLLMï¼Œä½†å®ƒä»¬æä¾›éå¸¸ä¸åŒçš„ç”¨æˆ·ä½“éªŒã€‚  

The first approach puts the initial burden on the user and has the LLM acting as a post-processing check.  

ç¬¬ä¸€ç§æ–¹æ³•å°†åˆå§‹è´Ÿæ‹…æ”¾åœ¨ç”¨æˆ·èº«ä¸Šï¼Œå¹¶å°†LLMä½œä¸ºåå¤„ç†æ£€æŸ¥ã€‚  

The second requires zero effort from the user but provides no transparency or control.  

ç”¨æˆ·æ— éœ€ä»˜å‡ºä»»ä½•åŠªåŠ›ï¼Œä½†å´æ²¡æœ‰é€æ˜åº¦æˆ–æ§åˆ¶æƒã€‚  

The third strikes the right balance.  

ç¬¬ä¸‰è€…æ‰¾åˆ°äº†åˆé€‚çš„å¹³è¡¡ã€‚  

By having the LLM suggest categories upfront, we reduce cognitive load on the user and they donâ€™t have to learn our taxonomy to categorize their product!  

é€šè¿‡è®©LLMåœ¨æœ€å‰é¢å»ºè®®ç±»åˆ«ï¼Œæˆ‘ä»¬å‡è½»äº†ç”¨æˆ·çš„è®¤çŸ¥è´Ÿæ‹…ï¼Œä»–ä»¬æ— éœ€å­¦ä¹ æˆ‘ä»¬çš„åˆ†ç±»æ³•å°±èƒ½å¯¹äº§å“è¿›è¡Œåˆ†ç±»ï¼  

At the same time, by allowing the user to review and edit the suggestion, they have the final say in how their product is classified, putting control firmly in their hands.  

åŒæ—¶ï¼Œé€šè¿‡å…è®¸ç”¨æˆ·æŸ¥çœ‹å’Œç¼–è¾‘å»ºè®®ï¼Œä»–ä»¬å¯¹äº§å“çš„åˆ†ç±»æ–¹å¼æœ‰æœ€ç»ˆå†³å®šæƒï¼Œä»è€Œå°†æ§åˆ¶æƒç‰¢ç‰¢æŒæ¡åœ¨ä»–ä»¬æ‰‹ä¸­ã€‚  

As a bonus, the third approach creates a [natural feedback loop for model improvement](https://eugeneyan.com/writing/llm-patterns/#collect-user-feedback-to-build-our-data-flywheel).  

ä½œä¸ºå¥–åŠ±ï¼Œç¬¬ä¸‰ç§æ–¹æ³•ä¸ºæ¨¡å‹æ”¹è¿›æä¾›äº†ä¸€ä¸ªè‡ªç„¶çš„åé¦ˆå¾ªç¯ã€‚  

Suggestions that are good are accepted (positive labels) and those that are bad are updated (negative followed by positive labels).  

æ¥å—å¥½çš„å»ºè®®ï¼ˆç§¯ææ ‡ç­¾ï¼‰ï¼Œæ›´æ–°ä¸å¥½çš„å»ºè®®ï¼ˆè´Ÿé¢æ ‡ç­¾åè·Ÿç§¯ææ ‡ç­¾ï¼‰ã€‚

This pattern of suggestion, user validation, and data collection is commonly seen in several applications:  

è¿™ç§å»ºè®®ã€ç”¨æˆ·éªŒè¯å’Œæ•°æ®æ”¶é›†çš„æ¨¡å¼åœ¨è®¸å¤šåº”ç”¨ç¨‹åºä¸­éƒ½å¾ˆå¸¸è§

-   Coding assistants: Where users can accept a suggestion (strong positive), accept and tweak a suggestion (positive), or ignore a suggestion (negative)  
    
    ç¼–ç åŠ©æ‰‹ï¼šç”¨æˆ·å¯ä»¥æ¥å—å»ºè®®ï¼ˆå¼ºçƒˆæ­£é¢åé¦ˆï¼‰ï¼Œæ¥å—å¹¶å¾®è°ƒå»ºè®®ï¼ˆæ­£é¢åé¦ˆï¼‰ï¼Œæˆ–å¿½ç•¥å»ºè®®ï¼ˆè´Ÿé¢åé¦ˆï¼‰
-   Midjourney: Where users can choose to upscale and download the image (strong positive), vary an image (positive), or generate a new set of images (negative)  
    
    ä¸­é€”ï¼šç”¨æˆ·å¯ä»¥é€‰æ‹©æ”¾å¤§å¹¶ä¸‹è½½å›¾åƒï¼ˆå¼ºçƒˆæ­£é¢ï¼‰ï¼Œå˜æ¢å›¾åƒï¼ˆæ­£é¢ï¼‰ï¼Œæˆ–ç”Ÿæˆæ–°çš„ä¸€ç»„å›¾åƒï¼ˆè´Ÿé¢ï¼‰
-   Chatbots: Where users can provide thumbs up (positive) or thumbs down (negative) on responses, or choose to regenerate a response if it was really bad (strong negative).  
    
    èŠå¤©æœºå™¨äººï¼šç”¨æˆ·å¯ä»¥å¯¹å“åº”è¿›è¡Œç‚¹èµï¼ˆç§¯æï¼‰æˆ–ç‚¹è¸©ï¼ˆæ¶ˆæï¼‰ï¼Œæˆ–é€‰æ‹©é‡æ–°ç”Ÿæˆå“åº”ï¼Œå¦‚æœå“åº”éå¸¸ç³Ÿç³•ï¼ˆå¼ºçƒˆæ¶ˆæï¼‰ã€‚

Feedback can be explicit or implicit.  

åé¦ˆå¯ä»¥æ˜¯æ˜ç¤ºçš„æˆ–æš—ç¤ºçš„ã€‚  

Explicit feedback is information users provide in response to a request by our product; implicit feedback is information we learn from user interactions without needing users to deliberately provide feedback.  

æ˜ç¡®åé¦ˆæ˜¯ç”¨æˆ·å¯¹æˆ‘ä»¬äº§å“çš„è¯·æ±‚æä¾›ä¿¡æ¯æ—¶æ‰€æä¾›çš„åé¦ˆï¼›éšå¼åé¦ˆæ˜¯æˆ‘ä»¬ä»ç”¨æˆ·äº’åŠ¨ä¸­å­¦åˆ°çš„ä¿¡æ¯ï¼Œæ— éœ€ç”¨æˆ·åˆ»æ„æä¾›åé¦ˆã€‚  

Coding assistants and Midjourney are examples of implicit feedback while thumbs up and thumb downs are explicit feedback.  

ç¼–ç åŠ©æ‰‹å’Œ Midjourney æ˜¯éšå¼åé¦ˆçš„ä¾‹å­ï¼Œè€Œèµå’Œè¸©åˆ™æ˜¯æ˜¾å¼åé¦ˆçš„å½¢å¼ã€‚  

If we design our UX well, like coding assistants and Midjourney, we can collect plenty of implicit feedback to improve our product and models.  

é€šè¿‡ç²¾å¿ƒè®¾è®¡æˆ‘ä»¬çš„ UXï¼Œä¾‹å¦‚ç¼–ç åŠ©æ‰‹å’Œ Midjourneyï¼Œæˆ‘ä»¬å¯ä»¥æ”¶é›†å¤§é‡éšå«åé¦ˆï¼Œä»¥æ”¹è¿›æˆ‘ä»¬çš„äº§å“å’Œæ¨¡å‹ã€‚

### Prioritize your hierarchy of needs ruthlessly  

2.3.3 æœ‰ç›®çš„åœ°ä¼˜å…ˆæ»¡è¶³ä½ çš„éœ€æ±‚[](https://applied-llms.org/#prioritize-your-hierarchy-of-needs-ruthlessly)

As we think about putting our demo into production, weâ€™ll have to think about the requirements for:  

å½“æˆ‘ä»¬è€ƒè™‘å°†æˆ‘ä»¬çš„æ¼”ç¤ºæŠ•å…¥ç”Ÿäº§æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘åˆ°ä»¥ä¸‹è¦æ±‚ï¼š

-   Reliability: 99.9% uptime, adherence to structured output  
    
    å¯é æ€§ï¼š99.9% çš„æ­£å¸¸è¿è¡Œæ—¶é—´ï¼Œéµå¾ªç»“æ„åŒ–è¾“å‡ºï¼Œç¡®ä¿æ•°æ®è¾“å‡ºçš„å‡†ç¡®æ€§
-   Harmlessness: Not generate offensive, NSFW, or otherwise harmful content  
    
    æ— å®³æ€§ï¼šä¸ç”Ÿæˆå†’çŠ¯ã€ä¸è‰¯å†…å®¹æˆ–å…¶ä»–æœ‰å®³å†…å®¹
-   Factual consistency: Being faithful to the context provided, not making things up  
    
    å¿ å®äºæä¾›çš„èƒŒæ™¯ä¿¡æ¯ï¼Œä¸è™šæ„å†…å®¹
-   Usefulness: Relevant to the usersâ€™ needs and request  
    
    æœ‰ç”¨æ€§ï¼šä¸ç”¨æˆ·çš„éœ€æ±‚å’Œè¯·æ±‚ç›¸å…³ï¼Œç¬¦åˆç”¨æˆ·çš„å®é™…éœ€è¦
-   Scalability: Latency SLAs, supported throughput  
    
    å¯ä¼¸ç¼©æ€§ï¼šå»¶è¿ŸæœåŠ¡æ°´å¹³åè®®ï¼Œæ”¯æŒçš„ååé‡
-   Cost: Because we donâ€™t have unlimited budget  
    
    ç”±äºæˆ‘ä»¬çš„é¢„ç®—æœ‰é™
-   And more: Security, privacy, fairness, GDPR, DMA, etc, etc.  
    
    å®‰å…¨ã€éšç§ã€å…¬å¹³æ€§ã€GDPRã€DMA ç­‰ç­‰æ–¹é¢ã€‚

If we try to tackle all these requirements at once, weâ€™re never going to ship anything.  

å¦‚æœæˆ‘ä»¬è¯•å›¾ä¸€æ¬¡è§£å†³æ‰€æœ‰è¿™äº›è¦æ±‚ï¼Œæˆ‘ä»¬æ°¸è¿œæ— æ³•äº¤ä»˜ä»»ä½•ä¸œè¥¿ã€‚  

Thus, we need to prioritize. Ruthlessly.  

å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦æœ‰æ‰€å–èˆã€‚æ¯«ä¸ç•™æƒ…ã€‚  

This means being clear what is non-negotiable (e.g., reliability, harmlessness) without which our product canâ€™t function or wonâ€™t be viable.  

è¿™æ„å‘³ç€æ˜ç¡®äº†ä»€ä¹ˆæ˜¯ä¸å¯å¦¥åçš„ï¼ˆä¾‹å¦‚ï¼Œå¯é æ€§ã€æ— å®³æ€§ï¼‰ï¼Œæ²¡æœ‰è¿™äº›ç‰¹è´¨ï¼Œæˆ‘ä»¬çš„äº§å“å°±æ— æ³•æ­£å¸¸è¿ä½œæˆ–ä¸ä¼šå¯è¡Œã€‚  

Itâ€™s all about identifying the minimum lovable product.  

ä¸€åˆ‡éƒ½åœ¨äºç¡®å®šæœ€å°å¯çˆ±äº§å“ã€‚  

We have to accept that the first version wonâ€™t be perfect, and just launch and iterate.  

æˆ‘ä»¬å¿…é¡»æ¥å—ç¬¬ä¸€ä¸ªç‰ˆæœ¬ä¸ä¼šå®Œç¾ï¼Œåªéœ€å¯åŠ¨å¹¶ä¸æ–­è¿­ä»£ã€‚

### Calibrate your risk tolerance based on the use case  

æ ¹æ®å…·ä½“æƒ…å†µè°ƒæ•´æ‚¨çš„é£é™©æ‰¿å—èƒ½åŠ›[](https://applied-llms.org/#calibrate-your-risk-tolerance-based-on-the-use-case)

When deciding on the language model and level of scrutiny of an application, consider the use case and audience.  

åœ¨ç¡®å®šåº”ç”¨ç¨‹åºçš„è¯­è¨€æ¨¡å‹å’Œå®¡æŸ¥çº§åˆ«æ—¶ï¼Œè¯·è€ƒè™‘ä½¿ç”¨æƒ…å†µå’Œå—ä¼—ã€‚  

For a customer-facing chatbot offering medical or financial advice, weâ€™ll need a very high bar for safety and accuracy.  

é’ˆå¯¹æä¾›åŒ»ç–—æˆ–é‡‘èå»ºè®®çš„é¢å‘å®¢æˆ·çš„èŠå¤©æœºå™¨äººï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿å®‰å…¨æ€§å’Œå‡†ç¡®æ€§è¾¾åˆ°æé«˜æ°´å¹³ã€‚  

Mistakes or bad output could cause real harm and erode trust. But for less critical applications, such as a recommender system, or internal-facing applications like content classification or summarization, excessively strict requirements only slow progress without adding much value.  

é”™è¯¯æˆ–ç³Ÿç³•çš„è¾“å‡ºå¯èƒ½ä¼šå¯¼è‡´å®é™…ä¼¤å®³å¹¶ç ´åä¿¡ä»»ã€‚ä½†å¯¹äºè¾ƒä¸å…³é”®çš„åº”ç”¨ï¼Œå¦‚æ¨èç³»ç»Ÿï¼Œæˆ–å†…éƒ¨åº”ç”¨ï¼Œå¦‚å†…å®¹åˆ†ç±»æˆ–æ‘˜è¦ï¼Œè¿‡äºä¸¥æ ¼çš„è¦æ±‚åªä¼šå‡ç¼“è¿›å±•ï¼Œè€Œå¹¶æ²¡æœ‰å¢åŠ å¤ªå¤šä»·å€¼ã€‚

This aligns with a recent [a16z report](https://a16z.com/generative-ai-enterprise-2024/) showing that many companies are moving faster with internal LLM applications compared to external ones (image below).  

è¿™ä¸æœ€è¿‘çš„ a16z æŠ¥å‘Šä¸€è‡´ï¼Œæ˜¾ç¤ºè®¸å¤šå…¬å¸åœ¨å†…éƒ¨LLMåº”ç”¨ç¨‹åºæ–¹é¢æ¯”å¤–éƒ¨åº”ç”¨ç¨‹åºæ›´å¿«ï¼ˆè§ä¸‹å›¾ï¼‰ã€‚  

By experimenting with AI for internal productivity, organizations can start capturing value while learning how to manage risk in a more controlled environment.  

é€šè¿‡å°è¯•åœ¨å†…éƒ¨ç”Ÿäº§åŠ›ä¸­åº”ç”¨äººå·¥æ™ºèƒ½ï¼Œç»„ç»‡å¯ä»¥å¼€å§‹åœ¨å­¦ä¹ å¦‚ä½•åœ¨æ›´å—æ§åˆ¶çš„ç¯å¢ƒä¸­ç®¡ç†é£é™©çš„åŒæ—¶æ•è·ä»·å€¼ã€‚  

Then, as they gain confidence, they can expand to customer-facing use cases.  

éšç€ä»–ä»¬ä¿¡å¿ƒçš„å¢é•¿ï¼Œä»–ä»¬å¯ä»¥æ‰©å±•åˆ°é¢å‘å®¢æˆ·çš„ä½¿ç”¨æƒ…å†µã€‚

![](How-willing-are-enterprises-to-use-LLMs-for-different-use-cases_-1-scaled.jpg) Proportion of enterprise LLM use across internal and external-facing use cases ([source: a16z report](https://a16z.com/generative-ai-enterprise-2024/))  

ä¼ä¸šLLMåœ¨å†…éƒ¨å’Œå¤–éƒ¨ç”¨ä¾‹ä¸­çš„ä½¿ç”¨æ¯”ä¾‹ï¼ˆæ¥æºï¼ša16z æŠ¥å‘Šï¼‰

## Team & Roles  

2.4 å›¢é˜Ÿä¸è§’è‰²[](https://applied-llms.org/#team-roles)

No job function is easy to define, but writing a job description for the work in this new space is more challenging than others.  

æ²¡æœ‰ä¸€ç§å·¥ä½œèŒèƒ½æ˜¯å®¹æ˜“å®šä¹‰çš„ï¼Œä½†ä¸ºè¿™ä¸ªæ–°é¢†åŸŸçš„å·¥ä½œæ’°å†™æè¿°æ¯”å…¶ä»–å·¥ä½œæ›´å…·æŒ‘æˆ˜æ€§ã€‚  

Weâ€™ll forgo Venn diagrams of intersecting job titles, or suggestions for job descriptions.  

æˆ‘ä»¬ä¸ä¼šä½¿ç”¨äº¤å‰èŒä½æ ‡é¢˜çš„ç»´æ©å›¾ï¼Œä¹Ÿä¸ä¼šæå‡ºå·¥ä½œæè¿°çš„å»ºè®®ã€‚  

We will, however, submit to the existence of a new roleâ€”the AI engineerâ€”and discuss its place.  

ç„¶è€Œï¼Œæˆ‘ä»¬å°†æ¥å—ä¸€ä¸ªæ–°è§’è‰²çš„å­˜åœ¨â€”â€”AI å·¥ç¨‹å¸ˆï¼Œå¹¶è®¨è®ºå…¶å®šä½ã€‚  

Importantly, weâ€™ll discuss the rest of the team and how responsibilities should be assigned.  

é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å°†è®¨è®ºå›¢é˜Ÿä¸­å…¶ä»–æˆå‘˜çš„æƒ…å†µï¼Œä»¥åŠå¦‚ä½•åˆ†é…è´£ä»»ã€‚

### Focus on the process, not tools  

ä¸“æ³¨äºè¿‡ç¨‹ï¼Œè€Œéå·¥å…·[](https://applied-llms.org/#focus-on-the-process-not-tools)

When faced with new paradigms, such as LLMs, software engineers tend to favor tools.  

é¢å¯¹æ–°çš„èŒƒå¼æ—¶ï¼Œä¾‹å¦‚LLMsï¼Œè½¯ä»¶å·¥ç¨‹å¸ˆå€¾å‘äºåçˆ±å·¥å…·ã€‚  

As a result, we overlook the problem and process the tool was supposed to solve.  

å› æ­¤ï¼Œæˆ‘ä»¬å¿½ç•¥äº†è¿™ä¸ªå·¥å…·æœ¬åº”è§£å†³çš„é—®é¢˜å’Œæµç¨‹ã€‚  

In doing so, many engineers assume accidental complexity, which has negative consequences for the teamâ€™s long-term productivity.  

åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œè®¸å¤šå·¥ç¨‹å¸ˆä¼šè¯¯ä»¥ä¸ºå­˜åœ¨æ„å¤–å¤æ‚æ€§ï¼Œè¿™ä¼šå¯¹å›¢é˜Ÿçš„é•¿æœŸç”Ÿäº§åŠ›äº§ç”Ÿè´Ÿé¢å½±å“ã€‚

For example, [this write-up](https://hamel.dev/blog/posts/prompt/) discusses how certain tools can automatically create prompts for large language models.  

ä¾‹å¦‚ï¼Œæœ¬æ–‡è®¨è®ºäº†å¦‚ä½•ä½¿ç”¨æŸäº›å·¥å…·è‡ªåŠ¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæç¤ºã€‚  

It argues (rightfully IMHO) that engineers who use these tools without first understanding the problem-solving methodology or process end up taking on unnecessary technical debt.  

å®ƒï¼ˆåœ¨æˆ‘çœ‹æ¥æ˜¯æ­£ç¡®çš„ï¼‰æŒ‡å‡ºï¼Œé‚£äº›åœ¨ä¸å…ˆäº†è§£é—®é¢˜è§£å†³æ–¹æ³•æˆ–è¿‡ç¨‹çš„æƒ…å†µä¸‹ä½¿ç”¨è¿™äº›å·¥å…·çš„å·¥ç¨‹å¸ˆæœ€ç»ˆä¼šæ‰¿æ‹…ä¸å¿…è¦çš„æŠ€æœ¯å€ºåŠ¡ã€‚

In addition to accidental complexity, tools are often underspecified.  

é™¤äº†å¶ç„¶çš„å¤æ‚æ€§å¤–ï¼Œå·¥å…·é€šå¸¸ç¼ºä¹è¯¦ç»†è§„èŒƒã€‚  

For example, there is a growing industry of LLM evaluation tools that offer â€œLLM Evaluation In A Boxâ€ with generic evaluators for toxicity, conciseness, tone, etc. We have seen many teams adopt these tools without thinking critically about the specific failure modes of their domains.  

ä¾‹å¦‚ï¼Œæœ‰ä¸€ä¸ªä¸æ–­å¢é•¿çš„è¡Œä¸šæä¾›â€œLLMè¯„ä¼°å·¥å…·ä¸€æ½å­â€ï¼Œå…¶ä¸­åŒ…æ‹¬æ¯’æ€§ã€ç®€æ´æ€§ã€è¯­æ°”ç­‰é€šç”¨è¯„ä¼°å™¨ã€‚æˆ‘ä»¬çœ‹åˆ°è®¸å¤šå›¢é˜Ÿåœ¨æ²¡æœ‰æ‰¹åˆ¤æ€§åœ°è€ƒè™‘å…¶é¢†åŸŸç‰¹å®šæ•…éšœæ¨¡å¼çš„æƒ…å†µä¸‹é‡‡ç”¨äº†è¿™äº›å·¥å…·ã€‚  

Contrast this to EvalGen.  

ä¸ EvalGen è¿›è¡Œå¯¹æ¯”ã€‚  

It focuses on teaching users the process of creating domain-specific evals by deeply involving the user each step of the way, from specifying criteria, to labeling data, to checking evals.  

å®ƒä¸“æ³¨äºæ•™ç”¨æˆ·åˆ›å»ºç‰¹å®šé¢†åŸŸçš„è¯„ä¼°è¿‡ç¨‹ï¼Œé€šè¿‡æ·±åº¦å‚ä¸ç”¨æˆ·æ¯ä¸€æ­¥ï¼Œä»æŒ‡å®šæ ‡å‡†ï¼Œåˆ°æ ‡è®°æ•°æ®ï¼Œå†åˆ°æ£€æŸ¥è¯„ä¼°ï¼Œä½¿ç”¨æˆ·æ›´å¥½åœ°ç†è§£ã€‚  

The software leads the user through a workflow that looks like this:  

è½¯ä»¶ä¼šå¼•å¯¼ç”¨æˆ·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®Œæˆå·¥ä½œæµç¨‹ï¼š

![](evalgen.png) Shankar, S., et al.Â (2024). Who Validates the Validators?  

Shankar, S.ç­‰äººï¼ˆ2024 å¹´ï¼‰ã€‚è°æ¥éªŒè¯éªŒè¯è€…ï¼Ÿ  

Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences. Retrieved from [https://arxiv.org/abs/2404.12272](https://arxiv.org/abs/2404.12272)  

åˆ©ç”¨LLMè¾…åŠ©è¯„ä¼°LLMè¾“å‡ºä¸äººç±»åå¥½è¿›è¡Œå¯¹é½ã€‚æ¥æºï¼šhttps://arxiv.org/abs/2404.12272

EvalGen guides the user through a best practice of crafting LLM evaluations, namely:  

EvalGen æŒ‡å¯¼ç”¨æˆ·é€šè¿‡æœ€ä½³å®è·µæ¥åˆ¶ä½œLLMè¯„ä¼°ï¼Œå³ï¼š

-   Defining domain-specific tests (bootstrapped automatically from the prompt).  
    
    è‡ªåŠ¨ä»æç¤ºä¸­å¼•å¯¼å¯åŠ¨ï¼Œå®šä¹‰ç‰¹å®šé¢†åŸŸçš„æµ‹è¯•ã€‚  
    
    These are defined as either assertions with code or with LLM-as-a-Judge.  
    
    è¿™äº›è¢«å®šä¹‰ä¸ºå…·æœ‰ä»£ç æˆ–LLM-ä½œä¸ºæ³•å®˜çš„æ–­è¨€ã€‚
-   The importance of aligning the tests with human judgment, so that the user can check that the tests capture the specified criteria.  
    
    é‡è¦çš„æ˜¯å°†æµ‹è¯•ä¸äººç±»åˆ¤æ–­åŠ›å¯¹é½ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥æ£€æŸ¥æµ‹è¯•æ˜¯å¦ç¬¦åˆæŒ‡å®šçš„æ ‡å‡†ã€‚
-   Iterating on your tests as the system (prompts, etc) changes.Â   
    
    éšç€ç³»ç»Ÿï¼ˆåŒ…æ‹¬æç¤ºç­‰ï¼‰çš„å˜åŒ–ï¼Œä¸æ–­å®Œå–„æ‚¨çš„æµ‹è¯•ã€‚

EvalGen provides developers with a mental model of the evaluation-building process without anchoring them to a specific tool.  

EvalGen ä¸ºå¼€å‘äººå‘˜æä¾›äº†è¯„ä¼°æ„å»ºè¿‡ç¨‹çš„å¿ƒæ™ºæ¨¡å‹ï¼Œè€Œä¸é™åˆ¶ä»–ä»¬ä½¿ç”¨ç‰¹å®šå·¥å…·ã€‚  

We have found that after providing AI Engineers with this context, they often decide to select leaner tools or build their own.  

æˆ‘ä»¬å‘ç°ï¼Œæä¾›äº†è¿™æ ·çš„èƒŒæ™¯ä¿¡æ¯åï¼ŒAI å·¥ç¨‹å¸ˆä»¬é€šå¸¸ä¼šå†³å®šé€‰æ‹©æ›´ç®€æ´çš„å·¥å…·æˆ–è‡ªè¡Œå¼€å‘ã€‚  

Â Â 

There are too many components of LLMs beyond prompt writing and evaluations to list exhaustively here.  

åœ¨è¿™é‡Œæ— æ³•è¯¦å°½åˆ—ä¸¾LLMsçš„å„ä¸ªç»„æˆéƒ¨åˆ†ï¼Œé™¤äº†æç¤ºç¼–å†™å’Œè¯„ä¼°ä¹‹å¤–è¿˜æœ‰å¾ˆå¤šå…¶ä»–å†…å®¹ã€‚  

Â  However, it is important that AI Engineers seek to understand the processes before adopting tools.  

ç„¶è€Œï¼Œé‡è¦çš„æ˜¯ AI å·¥ç¨‹å¸ˆåœ¨é‡‡ç”¨å·¥å…·ä¹‹å‰å…ˆäº†è§£è¿™äº›è¿‡ç¨‹ã€‚

### Always be experimenting  

2.4.2 å§‹ç»ˆä¿æŒå®éªŒç²¾ç¥ï¼Œä¸æ–­å°è¯•æ–°äº‹ç‰©[](https://applied-llms.org/#always-be-experimenting)

ML products are deeply intertwined with experimentation.  

æœºå™¨å­¦ä¹ äº§å“ä¸å®éªŒå¯†ä¸å¯åˆ†ã€‚  

Not only the A/B, Randomized Control Trials kind, but the frequent attempts at modifying the smallest possible components of your system, and doing offline evaluation.  

ä¸ä»…ä»…æ˜¯ A/B æµ‹è¯•ã€éšæœºå¯¹ç…§è¯•éªŒè¿™æ ·çš„ç±»å‹ï¼Œè¿˜åŒ…æ‹¬é¢‘ç¹å°è¯•ä¿®æ”¹ç³»ç»Ÿä¸­å¯èƒ½çš„æœ€å°ç»„ä»¶ï¼Œå¹¶è¿›è¡Œç¦»çº¿è¯„ä¼°ã€‚  

The reason why everyone is so hot for evals is not actually about trustworthiness and confidenceâ€”itâ€™s about enabling experiments!  

å¤§å®¶å¦‚æ­¤çƒ­è¡·äºè¯„ä¼°çš„åŸå› å®é™…ä¸Šå¹¶ä¸åœ¨äºå¯ä¿¡åº¦å’Œä¿¡å¿ƒï¼Œè€Œæ˜¯ä¸ºäº†ä¿ƒè¿›å®éªŒçš„è¿›è¡Œï¼  

The better your evals, the faster you can iterate on experiments, and thus the faster you can converge on the best version of your system.  

è¯„ä¼°è¶Šå¥½ï¼Œæ‚¨å°±èƒ½æ›´å¿«åœ°åœ¨å®éªŒä¸­è¿­ä»£ï¼Œä»è€Œæ›´å¿«åœ°æ”¶æ•›äºç³»ç»Ÿçš„æœ€ä½³ç‰ˆæœ¬ã€‚  

Â 

Itâ€™s common to try different approaches to solving the same problem because experimentation is so cheap now.  

ç°åœ¨å¾ˆå¸¸è§å°è¯•ä¸åŒæ–¹æ³•æ¥è§£å†³åŒä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºå®éªŒå˜å¾—å¦‚æ­¤å»‰ä»·ã€‚  

The high cost of collecting data and training a model is minimizedâ€”prompt engineering costs little more than human time.  

æœ€å¤§ç¨‹åº¦å‡å°‘æ•°æ®é‡‡é›†å’Œæ¨¡å‹è®­ç»ƒçš„é«˜æˆæœ¬ï¼Œå¿«é€Ÿå·¥ç¨‹å‡ ä¹åªéœ€äººåŠ›æˆæœ¬ã€‚  

Position your team so that everyone is taught [the basics of prompt engineering](https://eugeneyan.com/writing/prompting/). This encourages everyone to experiment and leads to diverse ideas from across the organization.  

å®šä½å›¢é˜Ÿï¼Œç¡®ä¿æ¯ä¸ªäººéƒ½æŒæ¡æç¤ºå·¥ç¨‹çš„åŸºç¡€çŸ¥è¯†ã€‚è¿™æ ·å¯ä»¥é¼“åŠ±å¤§å®¶è¿›è¡Œå®éªŒï¼Œä»è€Œäº§ç”Ÿç»„ç»‡å†…æ¶Œç°å‡ºçš„å¤šæ ·åŒ–æƒ³æ³•ã€‚

Additionally, donâ€™t only experiment to exploreâ€”also use them to exploit!  

æ­¤å¤–ï¼Œä¸ä»…è¦è¿›è¡Œå®éªŒæ¥æ¢ç´¢ï¼Œè¿˜è¦åˆ©ç”¨å®ƒä»¬æ¥å¼€å‘ï¼  

Have a working version of a new task?  

æ˜¯å¦æœ‰ä¸€ä¸ªæ–°ä»»åŠ¡çš„å¯ç”¨ç‰ˆæœ¬ï¼Ÿ  

Consider having someone else on the team approach it differently.  

è€ƒè™‘è®©å›¢é˜Ÿä¸­çš„å…¶ä»–äººä»¥ä¸åŒçš„æ–¹å¼æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚  

Try doing it another way thatâ€™ll be faster.  

å°è¯•ä»¥æ›´å¿«çš„æ–¹å¼åšä¸€ä¸‹åˆ«çš„æ–¹æ³•ã€‚  

Investigate prompt techniques like Chain-of-Thought or Few-Shot to make it higher quality.  

è°ƒæŸ¥æç¤ºæŠ€æœ¯ï¼Œå¦‚æ€ç»´é“¾æˆ–å°‘æ‹ï¼Œä»¥æé«˜è´¨é‡ã€‚  

Donâ€™t let your tooling hold you back on experimentation; if it is, rebuild it, or buy something to make it better.  

ä¸è¦è®©å·¥å…·æˆä¸ºå®éªŒçš„éšœç¢ï¼›å¦‚æœæ˜¯è¿™æ ·ï¼Œé‡æ–°æ„å»ºå®ƒï¼Œæˆ–è€…è´­ä¹°æ›´å¥½çš„å·¥å…·ã€‚  

Â 

Finally, during product/project planning, set aside time for building evals and running multiple experiments.  

æœ€åï¼Œåœ¨äº§å“/é¡¹ç›®è§„åˆ’æ—¶ï¼Œè¯·ç•™å‡ºæ—¶é—´è¿›è¡Œè¯„ä¼°å¹¶è¿è¡Œå¤šä¸ªå®éªŒã€‚  

Think of the product spec for engineering products, but add to it clear criteria for evals.  

è€ƒè™‘å·¥ç¨‹äº§å“çš„äº§å“è§„æ ¼ï¼ŒåŒæ—¶æ˜ç¡®è¯„ä¼°æ ‡å‡†ã€‚  

And during roadmapping, donâ€™t underestimate the time required for experimentationâ€”expect to do multiple iterations of development and evals before getting the green light for production.  

åœ¨è·¯çº¿è§„åˆ’è¿‡ç¨‹ä¸­ï¼Œä¸è¦ä½ä¼°å®éªŒæ‰€éœ€çš„æ—¶é—´â€”â€”åœ¨è·å¾—ç”Ÿäº§æ‰¹å‡†ä¹‹å‰ï¼Œé¢„è®¡éœ€è¦è¿›è¡Œå¤šæ¬¡å¼€å‘å’Œè¯„ä¼°ã€‚

### Empower everyone to use new AI technology  

2.4.3 è®©æ¯ä¸ªäººéƒ½èƒ½å¤Ÿä½¿ç”¨æœ€æ–°çš„äººå·¥æ™ºèƒ½æŠ€æœ¯[](https://applied-llms.org/#empower-everyone-to-use-new-ai-technology)

As generative AI increases in adoption, we want the entire teamâ€”not just the expertsâ€”to understand and feel empowered to use this new technology.  

éšç€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æ™®åŠï¼Œæˆ‘ä»¬å¸Œæœ›æ•´ä¸ªå›¢é˜Ÿâ€”â€”è€Œéä»…é™ä¸“å®¶â€”â€”éƒ½èƒ½ç†è§£å¹¶æŒæ¡ä½¿ç”¨è¿™é¡¹æ–°æŠ€æœ¯ã€‚  

Thereâ€™s no better way to develop intuition for how LLMs work (e.g., latencies, failure modes, UX) than to, well, use them.  

æƒ³è¦åŸ¹å…»å¯¹LLMså·¥ä½œæ–¹å¼ï¼ˆä¾‹å¦‚å»¶è¿Ÿã€æ•…éšœæ¨¡å¼ã€ç”¨æˆ·ä½“éªŒï¼‰çš„ç›´è§‰ï¼Œæœ€å¥½çš„æ–¹æ³•è«è¿‡äºäº²è‡ªä½¿ç”¨å®ƒä»¬ã€‚  

LLMs are relatively accessible: You donâ€™t need to know how to code to improve performance for a pipeline, and everyone can start contributing via prompt engineering and evals.  

LLMs ç›¸å¯¹å®¹æ˜“è®¿é—®ï¼šæ‚¨æ— éœ€äº†è§£å¦‚ä½•ç¼–ç å³å¯æ”¹å–„ç®¡é“æ€§èƒ½ï¼Œæ¯ä¸ªäººéƒ½å¯ä»¥é€šè¿‡å¿«é€Ÿå·¥ç¨‹å’Œè¯„ä¼°å¼€å§‹è´¡çŒ®ã€‚

A big part of this is education. It can start as simple as [the basics of prompt engineering](https://eugeneyan.com/writing/prompting/), where techniques like n-shot prompting and CoT help condition the model towards the desired output.  

æ•™è‚²æ˜¯å…¶ä¸­çš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ã€‚å®ƒå¯ä»¥ä»ç®€å•çš„æç¤ºå·¥ç¨‹åŸºç¡€å¼€å§‹ï¼Œä¾‹å¦‚ n-shot æç¤ºå’Œ CoT ç­‰æŠ€æœ¯æœ‰åŠ©äºå°†æ¨¡å‹è°ƒæ•´åˆ°æ‰€éœ€çš„è¾“å‡ºã€‚  

Folks who have the knowledge can also educate about the more technical aspects, such as how LLMs are autoregressive when generating output.  

äº†è§£çš„äººä¹Ÿå¯ä»¥æ•™æˆæ›´å¤šæŠ€æœ¯æ–¹é¢çš„çŸ¥è¯†ï¼Œæ¯”å¦‚åœ¨ç”Ÿæˆè¾“å‡ºæ—¶ï¼ŒLLMs æ˜¯è‡ªå›å½’çš„ã€‚  

In other words, while input tokens are processed in parallel, output tokens are generated sequentially.  

æ¢å¥è¯è¯´ï¼Œè¾“å…¥æ ‡è®°æ˜¯å¹¶è¡Œå¤„ç†çš„ï¼Œä½†è¾“å‡ºæ ‡è®°æ˜¯æŒ‰é¡ºåºç”Ÿæˆçš„ã€‚  

As a result, latency is more a function of output length than input lengthâ€”this is a key consideration when designing UXes and setting performance expectations.  

å› æ­¤ï¼Œå»¶è¿Ÿæ›´å¤šå–å†³äºè¾“å‡ºé•¿åº¦è€Œä¸æ˜¯è¾“å…¥é•¿åº¦â€”è¿™åœ¨è®¾è®¡ç”¨æˆ·ä½“éªŒå’Œè®¾å®šæ€§èƒ½æœŸæœ›æ—¶æ˜¯ä¸€ä¸ªå…³é”®è€ƒè™‘å› ç´ ã€‚

We can go further and provide opportunities for hands-on experimentation and exploration.  

æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥æä¾›æœºä¼šï¼Œè®©äººä»¬è¿›è¡Œå®è·µå’Œæ¢ç´¢ã€‚  

A hackathon perhaps?  

ä¹Ÿè®¸æ˜¯ä¸€ä¸ªé»‘å®¢é©¬æ‹‰æ¾ï¼Ÿ  

While it may seem expensive to have a team spend a few days hacking on speculative projects, the outcomes may surprise you.  

å°½ç®¡è®©å›¢é˜ŸèŠ±å‡ å¤©æ—¶é—´è¿›è¡ŒæŠ•æœºæ€§é¡¹ç›®çš„ç ”ç©¶å¯èƒ½çœ‹èµ·æ¥å¾ˆæ˜‚è´µï¼Œä½†ç»“æœå¯èƒ½ä¼šè®©æ‚¨æ„Ÿåˆ°æƒŠè®¶ã€‚  

We know of a team that, through a hackathon, accelerated and almost completed their three-year roadmap within a year.  

æˆ‘ä»¬çŸ¥é“ä¸€ä¸ªå›¢é˜Ÿï¼Œé€šè¿‡ä¸€æ¬¡é»‘å®¢é©¬æ‹‰æ¾ï¼ŒæˆåŠŸåŠ é€Ÿå¹¶åœ¨ä¸€å¹´å†…å‡ ä¹å®Œæˆäº†ä»–ä»¬åŸå®šä¸‰å¹´çš„å‘å±•è®¡åˆ’ã€‚  

Another team had a hackathon that led to paradigm-shifting UXes that are now possible thanks to LLMs, which have been prioritized for the year and beyond.  

å¦ä¸€ä¸ªå›¢é˜Ÿä¸¾åŠäº†ä¸€æ¬¡é»‘å®¢é©¬æ‹‰æ¾ï¼Œå¸¦æ¥äº†é¢ è¦†æ€§çš„ UX ä½“éªŒï¼Œè¿™äº›ä½“éªŒç°åœ¨å¾—ä»¥å®ç°ï¼Œè¿™è¦å½’åŠŸäºLLMsï¼Œå·²è¢«åˆ—ä¸ºä»Šå¹´åŠä»¥åçš„é‡ç‚¹ã€‚

### Donâ€™t fall into the trap of â€œAI Engineering is all I needâ€  

2.4.4 ä¸è¦é™·å…¥â€œæˆ‘åªéœ€è¦ AI å·¥ç¨‹â€çš„è¯¯åŒº[](https://applied-llms.org/#dont-fall-into-the-trap-of-ai-engineering-is-all-i-need)

As new job titles are coined, there is an initial tendency to overstate the capabilities associated with these roles.  

éšç€æ–°çš„èŒç§°çš„å‡ºç°ï¼Œäººä»¬å¾€å¾€ä¼šè¿‡åˆ†å¤¸å¤§è¿™äº›è§’è‰²æ‰€å…·å¤‡çš„èƒ½åŠ›ã€‚  

This often results in a painful correction as the actual scope of these jobs becomes clear.  

è¿™ç»å¸¸å¯¼è‡´ç—›è‹¦çš„çº æ­£ï¼Œå› ä¸ºè¿™äº›å·¥ä½œçš„å®é™…èŒƒå›´å˜å¾—æ¸…æ™°ã€‚  

Newcomers to the field, as well as hiring managers, might make exaggerated claims or have inflated expectations.  

åˆå…¥è¡Œä¸šçš„äººå’Œæ‹›è˜ç»ç†å¯èƒ½ä¼šå¤¸å¤§å…¶è¯æˆ–æŠ±æœ‰è¿‡é«˜æœŸæœ›ã€‚  

Notable examples over the last decade include:  

åœ¨è¿‡å»çš„åå¹´é‡Œï¼Œä¸€äº›æ˜¾è‘—çš„ä¾‹å­åŒ…æ‹¬ï¼š

-   Data Scientist: â€œ[someone who is better at statistics than any software engineer and better at software engineering than any statistician](https://x.com/josh_wills/status/198093512149958656).â€Â Â   
    
    æ•°æ®ç§‘å­¦å®¶ï¼šâ€œä¸€ä¸ªæ¯”ä»»ä½•è½¯ä»¶å·¥ç¨‹å¸ˆæ›´æ“…é•¿ç»Ÿè®¡å­¦ï¼Œæ¯”ä»»ä½•ç»Ÿè®¡å­¦å®¶æ›´æ“…é•¿è½¯ä»¶å·¥ç¨‹çš„äººã€‚â€
-   Machine Learning Engineer (MLE): a software engineering-centric view of machine learningÂ   
    
    æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆï¼ˆMLEï¼‰ï¼šä»¥è½¯ä»¶å·¥ç¨‹ä¸ºä¸­å¿ƒçš„æœºå™¨å­¦ä¹ è§†è§’

Initially, many assumed that data scientists alone were sufficient for data-driven projects.  

æœ€åˆï¼Œè®¸å¤šäººè®¤ä¸ºä»…æœ‰æ•°æ®ç§‘å­¦å®¶å°±è¶³ä»¥è¿›è¡Œæ•°æ®é©±åŠ¨é¡¹ç›®ï¼Œè¿™æ˜¯ä¸€ä¸ªè¯¯è§£ã€‚  

However, it became apparent that data scientists must collaborate with software and data engineers to develop and deploy data products effectively.  

ç„¶è€Œï¼Œæ˜¾è€Œæ˜“è§çš„æ˜¯ï¼Œæ•°æ®ç§‘å­¦å®¶å¿…é¡»ä¸è½¯ä»¶å’Œæ•°æ®å·¥ç¨‹å¸ˆåˆä½œï¼Œæ‰èƒ½æœ‰æ•ˆåœ°å¼€å‘å’Œéƒ¨ç½²æ•°æ®äº§å“ã€‚  

Â 

This misunderstanding has shown up again with the new role of AI Engineer, with some teams believing that AI Engineers are all you need.  

è¿™ç§è¯¯è§£å†æ¬¡å‡ºç°åœ¨æ–°è§’è‰² AI å·¥ç¨‹å¸ˆèº«ä¸Šï¼Œä¸€äº›å›¢é˜Ÿè®¤ä¸ºåªéœ€è¦ AI å·¥ç¨‹å¸ˆå°±å¤Ÿäº†ã€‚  

In reality, building machine learning or AI products requires a [broad array of specialized roles](https://papers.nips.cc/paper_files/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html).  

å®é™…ä¸Šï¼Œæ„å»ºæœºå™¨å­¦ä¹ æˆ–äººå·¥æ™ºèƒ½äº§å“éœ€è¦æ¶µç›–å¹¿æ³›é¢†åŸŸçš„ä¸“ä¸šè§’è‰²ã€‚  

Weâ€™ve consulted with more than a dozen companies on AI products and have consistently observed that they fall into the trap of believing that â€œAI Engineering is all you need.â€ As a result, products often struggle to scale beyond a demo as companies overlook crucial aspects involved in building a product.  

æˆ‘ä»¬å·²ç»å’¨è¯¢äº†åå¤šå®¶å…¬å¸å…³äºäººå·¥æ™ºèƒ½äº§å“ï¼Œå¹¶ä¸€ç›´è§‚å¯Ÿåˆ°å®ƒä»¬é™·å…¥äº†â€œè®¤ä¸ºâ€˜AI å·¥ç¨‹â€™å°±æ˜¯ä½ æ‰€éœ€è¦çš„â€è¿™ä¸€é™·é˜±ã€‚å› æ­¤ï¼Œäº§å“å¾€å¾€éš¾ä»¥åœ¨æ¼”ç¤ºä¹‹å¤–æ‰©å±•ï¼Œå› ä¸ºå…¬å¸å¿½è§†äº†æ„å»ºäº§å“æ‰€æ¶‰åŠçš„å…³é”®æ–¹é¢ã€‚

For example, evaluation and measurement are crucial for scaling a product beyond vibe checks.  

ä¾‹å¦‚ï¼Œè¯„ä¼°å’Œæµ‹é‡å¯¹äºå°†äº§å“æ‰©å±•è‡³è¶…è¶Šæ°›å›´æ£€æŸ¥è‡³å…³é‡è¦ã€‚  

The skills for effective evaluation align with some of the strengths traditionally seen in machine learning engineersâ€”a team composed solely of AI Engineers will likely lack these skills.  

æœ‰æ•ˆè¯„ä¼°æ‰€éœ€çš„æŠ€èƒ½ä¸ä¼ ç»Ÿä¸Šåœ¨æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆä¸­çœ‹åˆ°çš„ä¸€äº›ä¼˜åŠ¿ç›¸ä¸€è‡´â€”â€”ä¸€ä¸ªå®Œå…¨ç”±äººå·¥æ™ºèƒ½å·¥ç¨‹å¸ˆç»„æˆçš„å›¢é˜Ÿå¯èƒ½ä¼šç¼ºä¹è¿™äº›æŠ€èƒ½ã€‚  

Co-author Hamel Husain illustrates the importance of these skills in his recent work around detecting [data drift](https://github.com/hamelsmu/ft-drift) and [designing domain-specific evals](https://hamel.dev/blog/posts/evals/).  

å…±åŒä½œè€… Hamel Husain åœ¨æœ€è¿‘çš„å·¥ä½œä¸­é˜è¿°äº†è¿™äº›æŠ€èƒ½çš„é‡è¦æ€§ï¼Œå›´ç»•æ£€æµ‹æ•°æ®æ¼‚ç§»å’Œè®¾è®¡ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°ã€‚

Here is a rough progression of the types of roles you need, and when youâ€™ll need them, throughout the journey of building an AI product:  

è¿™æ˜¯åœ¨æ„å»ºäººå·¥æ™ºèƒ½äº§å“è¿‡ç¨‹ä¸­æ‚¨éœ€è¦çš„å„ç§è§’è‰²ç±»å‹ä»¥åŠå®ƒä»¬æ‰€éœ€çš„æ—¶é—´çš„å¤§è‡´å‘å±•è·¯å¾„ï¼š

-   First, focus on building a product. This might include an AI engineer, but it doesnâ€™t have to.  
    
    é¦–å…ˆï¼Œä¸“æ³¨äºäº§å“çš„å»ºè®¾ã€‚è¿™å¯èƒ½æ¶‰åŠåˆ° AI å·¥ç¨‹å¸ˆï¼Œä½†ä¸æ˜¯å¿…é¡»çš„ã€‚  
    
    AI Engineers are valuable for prototyping and iterating quickly on the product (UX, plumbing, etc).  
    
    AI å·¥ç¨‹å¸ˆåœ¨å¿«é€ŸåŸå‹è®¾è®¡å’Œäº§å“è¿­ä»£ï¼ˆUXï¼Œç®¡é“ç­‰æ–¹é¢ï¼‰æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚  
    
    Â 
-   Next, create the right foundations by instrumenting your system and collecting data.  
    
    æ¥ä¸‹æ¥ï¼Œé€šè¿‡ä»ªå™¨åŒ–ç³»ç»Ÿå¹¶æ”¶é›†æ•°æ®æ¥ç¡®ç«‹æ­£ç¡®çš„åŸºç¡€ã€‚  
    
    Depending on the type and scale of data, you might need platform and/or data engineers.  
    
    æ ¹æ®æ•°æ®çš„ç±»å‹å’Œè§„æ¨¡ï¼Œæ‚¨å¯èƒ½éœ€è¦å¹³å°å·¥ç¨‹å¸ˆå’Œ/æˆ–æ•°æ®å·¥ç¨‹å¸ˆã€‚  
    
    You must also have systems for querying and analyzing this data to debug issues.  
    
    æ‚¨è¿˜éœ€è¦æ‹¥æœ‰ç”¨äºæŸ¥è¯¢å’Œåˆ†ææ•°æ®ä»¥è°ƒè¯•é—®é¢˜çš„ç³»ç»Ÿã€‚
-   Next, you will eventually want to optimize your AI system.  
    
    æ¥ä¸‹æ¥ï¼Œæ‚¨æœ€ç»ˆä¼šå¸Œæœ›ä¼˜åŒ–æ‚¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚  
    
    This doesnâ€™t necessarily involve training models.  
    
    è¿™å¹¶ä¸ä¸€å®šæ¶‰åŠè®­ç»ƒæ¨¡å‹ã€‚  
    
    The basics include steps like designing metrics, building evaluation systems, running experiments, optimizing RAG retrieval, debugging stochastic systems, and more.  
    
    åŸºç¡€åŒ…æ‹¬è®¾è®¡æŒ‡æ ‡ã€æ„å»ºè¯„ä¼°ç³»ç»Ÿã€è¿è¡Œå®éªŒã€ä¼˜åŒ– RAG æ£€ç´¢ã€è°ƒè¯•éšæœºç³»ç»Ÿç­‰æ­¥éª¤ã€‚  
    
    MLEs are really good at this (though AI engineers can pick them up too).  
    
    MLE åœ¨è¿™æ–¹é¢éå¸¸æ“…é•¿ï¼ˆå°½ç®¡ AI å·¥ç¨‹å¸ˆä¹Ÿå¯ä»¥æŒæ¡å®ƒä»¬ï¼‰ã€‚  
    
    It usually doesnâ€™t make sense to hire an MLE unless you have completed the prerequisite steps.  
    
    é™¤éæ‚¨å·²ç»å®Œæˆå…ˆå†³æ­¥éª¤ï¼Œå¦åˆ™é›‡ä½£ä¸€ä¸ª MLE é€šå¸¸æ˜¯æ²¡æœ‰æ„ä¹‰çš„ã€‚

Aside from this, you need a domain expert at all times.  

é™¤æ­¤ä¹‹å¤–ï¼Œæ‚¨éœ€è¦éšæ—¶æœ‰ä¸€ä¸ªé¢†åŸŸä¸“å®¶çš„æ”¯æŒã€‚  

At small companies, this would ideally be the founding teamâ€”and at bigger companies, product managers can play this role.  

åœ¨å°å…¬å¸ï¼Œè¿™ä¸ªè§’è‰²ç†æƒ³æƒ…å†µä¸‹åº”è¯¥ç”±åˆ›å§‹å›¢é˜Ÿæ‹…ä»»â€”è€Œåœ¨å¤§å…¬å¸ï¼Œäº§å“ç»ç†å¯ä»¥æ‰®æ¼”è¿™ä¸ªè§’è‰²ã€‚  

Being aware of the progression and timing of roles is critical.  

æ„è¯†åˆ°è§’è‰²çš„å‘å±•å’Œæ—¶é—´å®‰æ’è‡³å…³é‡è¦ï¼ŒæŠŠæ¡è§’è‰²çš„è¿›å±•å’Œæ—¶æœºè‡³å…³é‡è¦ã€‚  

Hiring folks at the wrong time (e.g., [hiring an MLE too early](https://jxnl.co/writing/2024/04/08/hiring-mle-at-early-stage-companies/)) or building in the wrong order is a waste of time and money, and causes churn.  

åœ¨é”™è¯¯çš„æ—¶é—´é›‡ç”¨äººå‘˜ï¼ˆä¾‹å¦‚ï¼Œè¿‡æ—©é›‡ç”¨ MLEï¼‰æˆ–æŒ‰é”™è¯¯é¡ºåºæ„å»ºæ˜¯æµªè´¹æ—¶é—´å’Œé‡‘é’±ï¼Œå¹¶å¯¼è‡´æµå¤±ã€‚  

Â  Furthermore, regularly checking in with an MLE (but not hiring them full-time) during phases 1-2 will help the company build the right foundations.  

æ­¤å¤–ï¼Œåœ¨ 1-2 é˜¶æ®µå®šæœŸä¸ MLE ä¿æŒè”ç³»ï¼ˆä½†ä¸å…¨èŒé›‡ä½£ä»–ä»¬ï¼‰å°†æœ‰åŠ©äºå…¬å¸å»ºç«‹æ­£ç¡®çš„åŸºç¡€ã€‚  

Â 

## Strategy: Building with LLMs without Getting Out-Maneuvered  

åˆ¶å®šç­–ç•¥ï¼šåœ¨ä¸è¢«æŒ‘è¡…çš„æƒ…å†µä¸‹å»ºé€ LLMs

Successful products require thoughtful planning and prioritization, not endless prototyping or following the latest model releases or trends.  

æˆåŠŸçš„äº§å“éœ€è¦ç»è¿‡æ·±æ€ç†Ÿè™‘çš„è§„åˆ’å’Œä¼˜å…ˆè€ƒè™‘ï¼Œè€Œä¸æ˜¯æ— ä¼‘æ­¢åœ°è¿›è¡ŒåŸå‹åˆ¶ä½œæˆ–è¿½éšæœ€æ–°çš„æ¨¡å‹å‘å¸ƒæˆ–æ½®æµã€‚  

In this final section, we look around the corners and think about the strategic considerations for building great AI products.  

åœ¨è¿™æœ€åä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ç¯é¡¾å››å‘¨ï¼Œæ€è€ƒæ„å»ºä¼˜ç§€çš„ AI äº§å“æ—¶çš„æˆ˜ç•¥è€ƒé‡ã€‚  

We also examine key trade-offs teams will face, like when to build and when to buy, and suggest a â€œplaybookâ€ for early LLM application development strategy.  

æˆ‘ä»¬è¿˜ä¼šæ¢è®¨å›¢é˜Ÿå°†é¢ä¸´çš„å…³é”®æŠ‰æ‹©ï¼Œæ¯”å¦‚ä½•æ—¶è‡ªå»ºå’Œä½•æ—¶è´­ä¹°ï¼Œå¹¶ä¸ºæ—©æœŸLLMåº”ç”¨å¼€å‘ç­–ç•¥æä¾›â€œplaybookâ€å»ºè®®ã€‚

## No GPUs before PMF  

PMF ä¹‹å‰æ²¡æœ‰ GPU[](https://applied-llms.org/#no-gpus-before-pmf)

To be great, your product needs to be more than just a thin wrapper around somebody elseâ€™s API.  

è¦æˆä¸ºä¼Ÿå¤§ï¼Œæ‚¨çš„äº§å“éœ€è¦è¶…è¶Šä»…ä»…æ˜¯å¯¹ä»–äºº API çš„ç®€å•å°è£…ã€‚  

But mistakes in the opposite direction can be even more costly.  

ä½†æ˜¯åœ¨ç›¸åçš„æ–¹å‘çŠ¯é”™å¯èƒ½ä¼šæ›´åŠ æ˜‚è´µã€‚  

The past year has also seen a mint of venture capital, including an eye-watering six billion dollar Series A, spent on training and customizing models without a clear product vision or target market.  

è¿‡å»ä¸€å¹´è¿˜è§è¯äº†å¤§é‡é£é™©æŠ•èµ„ï¼ŒåŒ…æ‹¬ä¸€ç¬”ä»¤äººç ç›®ç»“èˆŒçš„ 60 äº¿ç¾å…ƒçš„ A è½®èèµ„ï¼Œç”¨äºåŸ¹è®­å’Œå®šåˆ¶æ¨¡å‹ï¼Œä½†ç¼ºä¹æ˜ç¡®çš„äº§å“æ„¿æ™¯æˆ–ç›®æ ‡å¸‚åœºã€‚  

In this section, weâ€™ll explain why jumping immediately to training your own models is a mistake and consider the role of self-hosting.  

åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†è§£é‡Šä¸ºä»€ä¹ˆç«‹å³å¼€å§‹è®­ç»ƒè‡ªå·±çš„æ¨¡å‹æ˜¯ä¸€ä¸ªé”™è¯¯ï¼Œå¹¶æ€è€ƒè‡ªä¸»æ‰˜ç®¡çš„ä½œç”¨ã€‚

### Training from scratch (almost) never makes sense  

3.1.1 ä»å¤´å¼€å§‹è®­ç»ƒå‡ ä¹æ²¡æœ‰æ„ä¹‰[](https://applied-llms.org/#training-from-scratch-almost-never-makes-sense)

For most organizations, pretraining an LLM from scratch is an impractical distraction from building products.  

å¯¹äºå¤§å¤šæ•°ç»„ç»‡æ¥è¯´ï¼Œä»å¤´å¼€å§‹é¢„è®­ç»ƒä¸€ä¸ªLLMæ˜¯ä¸€ç§ä¸åˆ‡å®é™…çš„åˆ†æ•£æ³¨æ„åŠ›ï¼Œè€Œä¸æ˜¯å»ºè®¾äº§å“ã€‚

As exciting as it is and as much as it seems like everyone else is doing it, developing and maintaining machine learning infrastructure takes a lot of resources.  

å°½ç®¡ä»¤äººå…´å¥‹ï¼Œä¼¼ä¹æ¯ä¸ªäººéƒ½åœ¨åšï¼Œä½†å¼€å‘å’Œç»´æŠ¤æœºå™¨å­¦ä¹ åŸºç¡€è®¾æ–½éœ€è¦å¤§é‡èµ„æºã€‚  

This includes gathering data, training and evaluating models, and deploying them.  

åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ï¼Œä»¥åŠéƒ¨ç½²çš„è¿‡ç¨‹ã€‚  

If youâ€™re still validating product-market fit, these efforts will divert resources from developing your core product.  

å¦‚æœæ‚¨ä»åœ¨éªŒè¯äº§å“å¸‚åœºé€‚åº”æ€§ï¼Œè¿™äº›åŠªåŠ›å°†ä¼šåˆ†æ•£èµ„æºï¼Œä½¿æ‚¨æ— æ³•ä¸“æ³¨äºå¼€å‘æ ¸å¿ƒäº§å“ã€‚  

Even if you had the compute, data, and technical chops, the pretrained LLM may become obsolete in months.  

å³ä½¿æ‚¨å…·å¤‡è®¡ç®—èƒ½åŠ›ã€æ•°æ®å’ŒæŠ€æœ¯èƒ½åŠ›ï¼Œé¢„è®­ç»ƒæ¨¡å‹LLMå¯èƒ½åœ¨å‡ ä¸ªæœˆå†…å°±ä¼šè¿‡æ—¶ã€‚

Consider [BloombergGPT](https://arxiv.org/abs/2303.17564), an LLM specifically trained for financial tasks.  

è€ƒè™‘ BloombergGPTï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹é‡‘èä»»åŠ¡è®­ç»ƒçš„LLMã€‚  

The model was pretrained on 363B tokens via a heroic effort by [nine full-time employees](https://twimlai.com/podcast/twimlai/bloomberggpt-an-llm-for-finance/), four from AI Engineering and five from ML Product and Research. Despite this, it was [outclassed by gpt-3.5-turbo and gpt-4](https://arxiv.org/abs/2305.05862) on those very tasks within a year.  

è¯¥æ¨¡å‹æ˜¯ç”±ä¹åå…¨èŒå‘˜å·¥å…±åŒåŠªåŠ›ï¼Œå…¶ä¸­å››åæ¥è‡ª AI å·¥ç¨‹éƒ¨ï¼Œäº”åæ¥è‡ª ML äº§å“å’Œç ”ç©¶éƒ¨ï¼Œå¯¹ 363B ä¸ªæ ‡è®°è¿›è¡Œäº†é¢„è®­ç»ƒã€‚å°½ç®¡å¦‚æ­¤ï¼Œå®ƒåœ¨ä¸€å¹´å†…å°±è¢« gpt-3.5-turbo å’Œ gpt-4 åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¶…è¶Šäº†ã€‚

This story and others like it suggest that for most practical applications, pretraining an LLM from scratch, even on domain-specific data, is not the best use of resources.  

è¿™ä¸ªæ•…äº‹å’Œå…¶ä»–ç±»ä¼¼çš„æƒ…å†µè¡¨æ˜ï¼Œå¯¹äºå¤§å¤šæ•°å®é™…åº”ç”¨æ¥è¯´ï¼Œå³ä½¿åœ¨ç‰¹å®šé¢†åŸŸçš„æ•°æ®ä¸Šä»å¤´å¼€å§‹è¿›è¡Œé¢„è®­ç»ƒï¼Œä¹Ÿä¸æ˜¯èµ„æºçš„æœ€ä½³åˆ©ç”¨æ–¹å¼ã€‚  

Instead, teams are better off finetuning the strongest open-source models available for their specific needs.  

ç›¸åï¼Œå›¢é˜Ÿæœ€å¥½è°ƒæ•´ä½¿ç”¨æœ€å¼ºå¤§çš„å¼€æºæ¨¡å‹ï¼Œä»¥æ»¡è¶³ä»–ä»¬çš„ç‰¹å®šéœ€æ±‚ã€‚

There are of course exceptions. One shining example is [Replitâ€™s code model](https://blog.replit.com/replit-code-v1_5), trained specifically for code generation and understanding.  

å½“ç„¶ä¹Ÿæœ‰ä¾‹å¤–æƒ…å†µã€‚ä¸€ä¸ªé—ªäº®çš„ä¾‹å­å°±æ˜¯ Replit çš„ä»£ç æ¨¡å‹ï¼Œä¸“é—¨è®­ç»ƒç”¨äºä»£ç ç”Ÿæˆå’Œç†è§£ã€‚  

With pretraining, Replit was able to outperform other models of larger sizes such as CodeLlama7b.  

é€šè¿‡é¢„è®­ç»ƒï¼ŒReplit èƒ½å¤Ÿè¶…è¶Šå…¶ä»–æ›´å¤§å°ºå¯¸çš„æ¨¡å‹ï¼Œå¦‚ CodeLlama7bã€‚  

But as other, increasingly capable models have been released, maintaining utility has required continued investment.  

ç„¶è€Œéšç€å‘å¸ƒäº†å…¶ä»–åŠŸèƒ½è¶Šæ¥è¶Šå¼ºå¤§çš„æ¨¡å‹ï¼Œä¿æŒå®ç”¨æ€§å°±éœ€è¦æŒç»­çš„æŠ•å…¥ã€‚

### Donâ€™t finetune until youâ€™ve proven itâ€™s necessary  

3.1.2 åœ¨è¯æ˜å¿…è¦æ€§ä¹‹å‰ä¸è¦è¿›è¡Œå¾®è°ƒ[](https://applied-llms.org/#dont-finetune-until-youve-proven-its-necessary)

For most organizations, finetuning is driven more by FOMO than by clear strategic thinking.  

å¯¹äºå¤§å¤šæ•°ç»„ç»‡æ¥è¯´ï¼Œå¾®è°ƒæ›´å¤šåœ°æ˜¯å‡ºäºå¯¹é”™å¤±ææƒ§çš„é©±åŠ¨ï¼Œè€Œéæ¸…æ™°çš„æˆ˜ç•¥æ€è€ƒã€‚

Organizations invest in finetuning too early, trying to beat the â€œjust another wrapperâ€ allegations.  

ç»„ç»‡å¾€å¾€è¿‡æ—©æŠ•èµ„äºç»†èŠ‚ä¼˜åŒ–ï¼Œè¯•å›¾æ‘†è„±â€œåªæ˜¯åˆä¸€ä¸ªå°å¥—â€çš„è´¨ç–‘ã€‚  

In reality, finetuning is heavy machinery, to be deployed only after youâ€™ve collected plenty of examples that convince you other approaches wonâ€™t suffice.  

å®é™…ä¸Šï¼Œå¾®è°ƒå°±åƒæ˜¯ä¸€å°é‡å‹æœºæ¢°ï¼Œåªæœ‰åœ¨ä½ æ”¶é›†äº†è¶³å¤Ÿå¤šçš„ä¾‹å­å¹¶ä¸”ç¡®ä¿¡å…¶ä»–æ–¹æ³•ä¸å¤Ÿæ—¶æ‰èƒ½éƒ¨ç½²ã€‚

A year ago, many teams were telling us they were excited to finetune.  

ä¸€å¹´å‰ï¼Œè®¸å¤šå›¢é˜Ÿå‘Šè¯‰æˆ‘ä»¬ä»–ä»¬å¾ˆå…´å¥‹åœ°è¿›è¡Œä¼˜åŒ–ã€‚  

Few have found product-market fit and most regret their decision.  

å¾ˆå°‘æœ‰äººæ‰¾åˆ°äº§å“ä¸å¸‚åœºçš„å¥‘åˆç‚¹ï¼Œå¤§å¤šæ•°äººéƒ½åæ‚”ä»–ä»¬çš„å†³å®šã€‚  

If youâ€™re going to finetune, youâ€™d better be _really_ confident that youâ€™re set up to do it again and again as base models improveâ€”see the [â€œThe model isnâ€™t the productâ€](https://applied-llms.org/#the-model-isnt-the-product-the-system-around-it-is) and [â€œBuild LLMOpsâ€](https://applied-llms.org/#build-llmops-but-build-it-for-the-right-reason-faster-iteration) below.  

å¦‚æœæ‚¨æ‰“ç®—è¿›è¡Œå¾®è°ƒï¼Œæœ€å¥½ç¡®ä¿æ‚¨å·²ç»åšå¥½äº†å†æ¬¡è¿›è¡Œæ“ä½œçš„å‡†å¤‡ï¼Œå› ä¸ºåŸºç¡€æ¨¡å‹åœ¨æ”¹è¿›çš„åŒæ—¶ä¹Ÿä¼šä¸æ–­æé«˜â€”â€”è¯·å‚è§ä¸‹é¢çš„â€œæ¨¡å‹ä¸æ˜¯äº§å“â€å’Œâ€œæ„å»º LLMOpsâ€ã€‚

When might finetuning actually be the right call?  

ä»€ä¹ˆæƒ…å†µä¸‹å¾®è°ƒæ‰æ˜¯æ­£ç¡®çš„é€‰æ‹©ï¼Ÿ  

If the use case requires data not available in the mostly-open web-scale datasets used to train existing modelsâ€”and if youâ€™ve already built an MVP that demonstrates the existing models are insufficient.  

å¦‚æœä½¿ç”¨æ¡ˆä¾‹éœ€è¦çš„æ•°æ®ä¸åœ¨å¤§å¤šæ•°ç”¨äºè®­ç»ƒç°æœ‰æ¨¡å‹çš„å¼€æ”¾ç½‘ç»œè§„æ¨¡æ•°æ®é›†ä¸­ï¼Œå¹¶ä¸”æ‚¨å·²ç»æ„å»ºäº†ä¸€ä¸ªæ¼”ç¤ºç°æœ‰æ¨¡å‹ä¸è¶³çš„ MVPã€‚  

But be careful: if great training data isnâ€™t readily available to the model builders, where are _you_ getting it?  

ä½†æ˜¯è¦å°å¿ƒï¼šå¦‚æœæ¨¡å‹æ„å»ºè€…æ²¡æœ‰å‡†å¤‡å¥½çš„ä¼˜è´¨è®­ç»ƒæ•°æ®ï¼Œä½ ä¼šä»å“ªé‡Œè·å–å‘¢ï¼Ÿ

LLM-powered applications arenâ€™t a science fair project.  

LLM-powered åº”ç”¨ç¨‹åºä¸æ˜¯ä¸€ä¸ªç§‘å­¦å±•ç¤ºé¡¹ç›®ï¼Œè€Œæ˜¯ä¸€ä¸ªæˆç†Ÿçš„å•†ä¸šäº§å“ã€‚  

Investment in them should be commensurate with their contribution to your businessâ€™ strategic objectives and competitive differentiation.  

æŠ•èµ„åº”ä¸ä»–ä»¬å¯¹æ‚¨ä¼ä¸šæˆ˜ç•¥ç›®æ ‡å’Œç«äº‰ä¼˜åŠ¿çš„è´¡çŒ®ç›¸åŒ¹é…ã€‚

### Start with inference APIs, but donâ€™t be afraid of self-hosting  

3.1.3 ä»æ¨ç† API å¼€å§‹ï¼Œä½†ä¸è¦å®³æ€•è‡ªè¡Œæ‰˜ç®¡[](https://applied-llms.org/#start-with-inference-apis-but-dont-be-afraid-of-self-hosting)

With LLM APIs, itâ€™s easier than ever for startups to adopt and integrate language modeling capabilities without training their own models from scratch.  

åˆ©ç”¨LLMä¸ª APIï¼Œåˆ›ä¸šå…¬å¸ç°åœ¨å¯ä»¥æ›´è½»æ¾åœ°é‡‡ç”¨å’Œæ•´åˆè¯­è¨€å»ºæ¨¡èƒ½åŠ›ï¼Œè€Œæ— éœ€ä»é›¶å¼€å§‹è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ã€‚  

Providers like Anthropic, and OpenAI offer general APIs that can sprinkle intelligence into your product with just a few lines of code.  

Anthropic å’Œ OpenAI ç­‰ä¾›åº”å•†æä¾›é€šç”¨ APIï¼Œåªéœ€å‡ è¡Œä»£ç ï¼Œå³å¯ä¸ºæ‚¨çš„äº§å“æ³¨å…¥æ™ºèƒ½ã€‚  

By using these services, you can reduce the effort spent and instead focus on creating value for your customersâ€”this allows you to validate ideas and iterate towards product-market fit faster.  

é€šè¿‡ä½¿ç”¨è¿™äº›æœåŠ¡ï¼Œæ‚¨å¯ä»¥å‡å°‘ç²¾åŠ›æŠ•å…¥ï¼Œæ›´ä¸“æ³¨äºä¸ºå®¢æˆ·åˆ›é€ ä»·å€¼â€”è¿™æ ·å¯ä»¥å¸®åŠ©æ‚¨æ›´å¿«åœ°éªŒè¯æƒ³æ³•ï¼Œå¹¶æœç€äº§å“å¸‚åœºå¥‘åˆè¿­ä»£ã€‚

But, as with databases, managed services arenâ€™t the right fit for every use case, especially as scale and requirements increase.  

ç„¶è€Œï¼Œä¸æ•°æ®åº“ä¸€æ ·ï¼Œæ‰˜ç®¡æœåŠ¡å¹¶éé€‚ç”¨äºæ¯ç§æƒ…å†µï¼Œå°¤å…¶æ˜¯åœ¨è§„æ¨¡å’Œéœ€æ±‚å¢åŠ æ—¶ã€‚  

Indeed, self-hosting may be the only way to use models without sending confidential / private data out of your network, as required in regulated industries like healthcare and finance, or by contractual obligations or confidentiality requirements.  

å®é™…ä¸Šï¼Œè‡ªè¡Œæ‰˜ç®¡å¯èƒ½æ˜¯åœ¨å—ç›‘ç®¡è¡Œä¸šï¼ˆå¦‚åŒ»ç–—ä¿å¥å’Œé‡‘èï¼‰æˆ–åˆåŒä¹‰åŠ¡æˆ–ä¿å¯†è¦æ±‚ä¸‹ï¼Œä½¿ç”¨æ¨¡å‹è€Œä¸å°†æœºå¯†/ç§äººæ•°æ®å‘é€å‡ºç½‘ç»œçš„å”¯ä¸€é€”å¾„ã€‚

Furthermore, self-hosting circumvents limitations imposed by inference providers, like rate limits, model deprecations, and usage restrictions.  

æ­¤å¤–ï¼Œè‡ªè¡Œæ‰˜ç®¡å¯ä»¥è§„é¿æ¨ç†æä¾›å•†æ–½åŠ çš„é™åˆ¶ï¼Œå¦‚é€Ÿç‡é™åˆ¶ã€æ¨¡å‹æ·˜æ±°å’Œä½¿ç”¨é™åˆ¶ã€‚  

In addition, self-hosting gives you complete control over the model, making it easier to construct a differentiated, high-quality system around it.  

å¦å¤–ï¼Œè‡ªä¸»æ‰˜ç®¡è®©æ‚¨å®Œå…¨æŒæ§æ¨¡å‹ï¼Œæ›´å®¹æ˜“æ„å»ºä¸€ä¸ªå·®å¼‚åŒ–ã€é«˜è´¨é‡çš„ç³»ç»Ÿã€‚  

Finally, self-hosting, especially of finetunes, can reduce cost at large scale. For example, [Buzzfeed shared how they finetuned open-source LLMs to reduce costs by 80%](https://tech.buzzfeed.com/lessons-learned-building-products-powered-by-generative-ai-7f6c23bff376#9da5).  

æœ€ç»ˆï¼Œè‡ªè¡Œæ‰˜ç®¡ï¼Œå°¤å…¶æ˜¯å¯¹ finetunes çš„è‡ªè¡Œæ‰˜ç®¡ï¼Œå¯ä»¥åœ¨å¤§è§„æ¨¡ä¸Šé™ä½æˆæœ¬ã€‚ä¾‹å¦‚ï¼ŒBuzzfeed åˆ†äº«äº†ä»–ä»¬å¦‚ä½•é€šè¿‡ finetuned å¼€æºLLMsæ¥é™ä½ 80%çš„æˆæœ¬ã€‚

## Iterate to something great  

3.2 ä¸æ–­è¿­ä»£ï¼Œèµ°å‘ä¼Ÿå¤§[](https://applied-llms.org/#iterate-to-something-great)

To sustain a competitive edge in the long run, you need to think beyond models and consider what will set your product apart.  

è¦åœ¨é•¿æœŸä¿æŒç«äº‰ä¼˜åŠ¿ï¼Œæ‚¨éœ€è¦è¶…è¶Šæ¨¡å‹ï¼Œæ€è€ƒå¦‚ä½•è®©æ‚¨çš„äº§å“è„±é¢–è€Œå‡ºã€‚  

While speed of execution matters, it shouldnâ€™t be your only advantage.  

è™½ç„¶æ‰§è¡Œé€Ÿåº¦å¾ˆé‡è¦ï¼Œä½†å®ƒä¸åº”è¯¥æ˜¯ä½ å”¯ä¸€çš„ä¼˜åŠ¿ã€‚

### The model isnâ€™t the product, the system around it is  

3.2.1 æ¨¡å‹å¹¶éäº§å“ï¼Œè€Œæ˜¯å…¶å‘¨å›´çš„ç³»ç»Ÿ[](https://applied-llms.org/#the-model-isnt-the-product-the-system-around-it-is)

For teams that arenâ€™t building models, the rapid pace of innovation is a boon as they migrate from one SOTA model to the next, chasing gains in context size, reasoning capability, and price-to-value to build better and better products.  

å¯¹äºé‚£äº›ä¸æ„å»ºæ¨¡å‹çš„å›¢é˜Ÿæ¥è¯´ï¼Œåˆ›æ–°çš„å¿«é€Ÿæ­¥ä¼æ˜¯ä¸€ç§ç¦éŸ³ï¼Œå› ä¸ºä»–ä»¬ä¸æ–­è¿ç§»åˆ°æ›´æ–°çš„ SOTA æ¨¡å‹ï¼Œè¿½æ±‚ä¸Šä¸‹æ–‡è§„æ¨¡ã€æ¨ç†èƒ½åŠ›å’Œæ€§ä»·æ¯”çš„æå‡ï¼Œä»¥æ‰“é€ è¶Šæ¥è¶Šä¼˜è´¨çš„äº§å“ã€‚  

This progress is as exciting as it is predictable.  

è¿™ä¸ªè¿›å±•æ—¢ä»¤äººå…´å¥‹åˆå¯ä»¥é¢„æ–™åˆ°ï¼ŒçœŸæ˜¯ä»¤äººæŒ¯å¥‹ã€‚  

Taken together, this means models are likely to be the least durable component in the system.  

æ€»çš„æ¥è¯´ï¼Œè¿™æ„å‘³ç€æ¨¡å‹å¾ˆå¯èƒ½æ˜¯ç³»ç»Ÿä¸­æœ€ä¸è€ç”¨çš„éƒ¨åˆ†ã€‚

Instead, focus your efforts on whatâ€™s going to provide lasting value, such as:  

ä¸“æ³¨äºèƒ½å¤Ÿå¸¦æ¥æŒä¹…ä»·å€¼çš„äº‹åŠ¡ï¼Œæ¯”å¦‚ï¼š

-   Evals: To reliably measure performance on your task across models  
    
    è¯„ä¼°ï¼šå¯é åœ°è¡¡é‡è·¨æ¨¡å‹çš„ä»»åŠ¡è¡¨ç°
-   Guardrails: To prevent undesired outputs no matter the model  
    
    é˜²æŠ¤æ ï¼šæ— è®ºæ¨¡å‹å¦‚ä½•ï¼Œéƒ½å¯ä»¥é˜²æ­¢ä¸è‰¯è¾“å‡ºï¼Œç¡®ä¿è¾“å‡ºç¬¦åˆé¢„æœŸ
-   Caching: To reduce latency and cost by avoiding the model altogether  
    
    é€šè¿‡ç¼“å­˜æ¥å‡å°‘å»¶è¿Ÿå’Œæˆæœ¬ï¼Œé¿å…å®Œå…¨ä½¿ç”¨æ¨¡å‹
-   Data flywheel: To power the iterative improvement of everything above  
    
    æ•°æ®é£è½®ï¼šæ¨åŠ¨ä¸€åˆ‡ä¸æ–­æ”¹è¿›çš„åŠ¨åŠ›

These components create a thicker moat of product quality than raw model capabilities.  

è¿™äº›ç»„ä»¶æ¯”åŸå§‹æ¨¡å‹çš„èƒ½åŠ›åˆ›é€ äº†æ›´é«˜å“è´¨çš„äº§å“ï¼Œå½¢æˆäº†æ›´ä¸ºåšå›ºçš„å£•æ²Ÿã€‚

But that doesnâ€™t mean building at the application layer is risk-free.  

ä½†è¿™å¹¶ä¸æ„å‘³ç€åœ¨åº”ç”¨å±‚è¿›è¡Œæ„å»ºæ˜¯æ²¡æœ‰é£é™©çš„ã€‚  

Donâ€™t point your shears at the same yaks that OpenAI or other model providers will need to shave if they want to provide viable enterprise software.  

ä¸è¦æŠŠä½ çš„å‰ªåˆ€æŒ‡å‘é‚£äº› OpenAI æˆ–å…¶ä»–æ¨¡å‹æä¾›å•†éœ€è¦å‰ƒæ¯›çš„ç‰¦ç‰›ï¼Œå¦‚æœä»–ä»¬æƒ³æä¾›å¯è¡Œçš„ä¼ä¸šè½¯ä»¶ã€‚

For example, some teams invested in building custom tooling to validate structured output from proprietary models; minimal investment here is important, but a deep one is not a good use of time.  

ä¾‹å¦‚ï¼Œä¸€äº›å›¢é˜ŸæŠ•èµ„äºæ„å»ºå®šåˆ¶å·¥å…·ï¼Œç”¨äºéªŒè¯ä¸“æœ‰æ¨¡å‹çš„ç»“æ„åŒ–è¾“å‡ºï¼›åœ¨è¿™é‡Œè¿›è¡Œæœ€å°çš„æŠ•èµ„å¾ˆé‡è¦ï¼Œä½†æ·±å…¥æŠ•èµ„å¹¶ä¸æ˜¯ä¸€ä¸ªæ˜æ™ºçš„æ—¶é—´åˆ©ç”¨ã€‚  

OpenAI needs to ensure that when you ask for a function call, you get a valid function callâ€”because all of their customers want this.  

OpenAI éœ€è¦ç¡®ä¿å½“æ‚¨è¯·æ±‚å‡½æ•°è°ƒç”¨æ—¶ï¼Œæ‚¨ä¼šå¾—åˆ°ä¸€ä¸ªæœ‰æ•ˆçš„å‡½æ•°è°ƒç”¨ï¼Œå› ä¸ºæ‰€æœ‰çš„å®¢æˆ·éƒ½å¸Œæœ›è¿™æ ·ã€‚  

Employ some â€œstrategic procrastinationâ€ here, build what you absolutely need, and await the obvious expansions to capabilities from providers.  

åœ¨è¿™é‡Œé‡‡ç”¨ä¸€äº›â€œæˆ˜ç•¥æ€§æ‹–å»¶â€ï¼Œå…ˆæ„å»ºæ‚¨ç»å¯¹éœ€è¦çš„éƒ¨åˆ†ï¼Œç„¶åç­‰å¾…ä¾›åº”å•†æ˜æ˜¾æ‰©å±•èƒ½åŠ›çš„åˆ°æ¥ã€‚

### Build trust by starting small  

3.2.2 é€šè¿‡å°æ­¥å»ºç«‹ä¿¡ä»»[](https://applied-llms.org/#build-trust-by-starting-small)

Building a product that tries to be everything to everyone is a recipe for mediocrity.  

è¯•å›¾è¿åˆæ‰€æœ‰äººçš„äº§å“æ³¨å®šå¹³åº¸æ— å¥‡ã€‚  

To create compelling products, companies need to specialize in building sticky experiences that keep users coming back.  

è¦æ‰“é€ å¼•äººå…¥èƒœçš„äº§å“ï¼Œå…¬å¸éœ€è¦ä¸“æ³¨äºæ‰“é€ è®©ç”¨æˆ·å›å¤´çš„å¸å¼•åŠ›ä½“éªŒã€‚

Consider a generic RAG system that aims to answer any question a user might ask.  

è€ƒè™‘ä¸€ä¸ªé€šç”¨çš„ RAG ç³»ç»Ÿï¼Œæ—¨åœ¨å›ç­”ç”¨æˆ·å¯èƒ½æå‡ºçš„ä»»ä½•é—®é¢˜ã€‚  

The lack of specialization means that the system canâ€™t prioritize recent information, parse domain-specific formats, or understand the nuances of specific tasks.  

ç”±äºç¼ºä¹ä¸“ä¸šåŒ–ï¼Œç³»ç»Ÿæ— æ³•ä¼˜å…ˆå¤„ç†æœ€æ–°ä¿¡æ¯ï¼Œè§£æç‰¹å®šé¢†åŸŸçš„æ ¼å¼ï¼Œæˆ–ç†è§£ç‰¹å®šä»»åŠ¡çš„ç»†å¾®å·®åˆ«ã€‚  

As a result, users are left with a shallow, unreliable experience that doesnâ€™t meet their needs, leading to churn.  

å› æ­¤ï¼Œç”¨æˆ·å¾—åˆ°äº†è‚¤æµ…ä¸”ä¸å¯é çš„ä½“éªŒï¼Œæ— æ³•æ»¡è¶³ä»–ä»¬çš„éœ€æ±‚ï¼Œä»è€Œå¯¼è‡´æµå¤±ã€‚

To address this, focus on specific domains and use cases.  

è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦ä¸“æ³¨äºç‰¹å®šçš„é¢†åŸŸå’Œä½¿ç”¨æƒ…å†µã€‚  

Narrow the scope by going deep rather than wide.  

æ·±å…¥ä¸“æ³¨ï¼Œè€Œéå¹¿æ³›æ¶‰çŒã€‚  

This will create domain-specific tools that resonate with users.  

è¿™å°†åˆ›å»ºä¸ç”¨æˆ·å…±é¸£çš„é¢†åŸŸç‰¹å®šå·¥å…·ã€‚  

Specialization also allows you to be upfront about your systemâ€™s capabilities and limitations.  

ä¸“ä¸šåŒ–è¿˜èƒ½å¸®åŠ©æ‚¨æ¸…æ¥šåœ°äº†è§£ç³»ç»Ÿçš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚  

Being transparent about what your system can and cannot do demonstrates self-awareness, helps users understand where it can add the most value, and thus builds trust and confidence in the output.  

é€æ˜åœ°è¡¨æ˜æ‚¨çš„ç³»ç»Ÿèƒ½å¤Ÿåšä»€ä¹ˆå’Œä¸èƒ½åšä»€ä¹ˆï¼Œå±•ç¤ºè‡ªæˆ‘æ„è¯†ï¼Œå¸®åŠ©ç”¨æˆ·äº†è§£å®ƒåœ¨å“ªäº›æ–¹é¢å¯ä»¥æä¾›æœ€å¤§ä»·å€¼ï¼Œä»è€Œå»ºç«‹å¯¹è¾“å‡ºçš„ä¿¡ä»»å’Œä¿¡å¿ƒã€‚

### Build LLMOps, but build it for the right reason: faster iteration  

3.2.3 æ„å»º LLMOpsï¼Œä½†å‡ºäºæ­£ç¡®çš„åŸå› æ„å»ºå®ƒï¼šæ›´å¿«çš„è¿­ä»£[](https://applied-llms.org/#build-llmops-but-build-it-for-the-right-reason-faster-iteration)

DevOps is not fundamentally about reproducible workflows or shifting left or empowering two pizza teamsâ€”and itâ€™s definitely not about writing YAML files.  

DevOps å¹¶ä¸æ ¹æœ¬æ˜¯å…³äºå¯å¤åˆ¶çš„å·¥ä½œæµç¨‹ã€å‘å·¦è½¬ç§»æˆ–èµ‹äºˆä¸¤æŠ«è¨å›¢é˜ŸæƒåŠ›ï¼Œå®ƒç»å¯¹ä¸æ˜¯å…³äºç¼–å†™ YAML æ–‡ä»¶çš„ã€‚

DevOps is about shortening the feedback cycles between work and its outcomes so that improvements accumulate instead of errors.  

DevOps æ—¨åœ¨ç¼©çŸ­å·¥ä½œä¸ç»“æœä¹‹é—´çš„åé¦ˆå‘¨æœŸï¼Œä½¿æ”¹è¿›å¾—ä»¥ç§¯ç´¯ï¼Œè€Œéé”™è¯¯ã€‚  

Its roots go back, via the Lean Startup movement, to Lean Manufacturing and the Toyota Production System, with its emphasis on Single Minute Exchange of Die and Kaizen.  

å…¶æ ¹æºå¯ä»¥è¿½æº¯åˆ°ç²¾ç›Šåˆ›ä¸šè¿åŠ¨ï¼Œä»¥åŠç²¾ç›Šç”Ÿäº§å’Œä¸°ç”°ç”Ÿäº§ä½“ç³»ï¼Œå¼ºè°ƒå•åˆ†é’Ÿæ¨¡å…·æ›´æ¢å’Œæ”¹å–„ã€‚

MLOps has adapted the form of DevOps to ML.  

MLOps å·²ç»å°† DevOps çš„ç†å¿µåº”ç”¨åˆ°æœºå™¨å­¦ä¹ ä¸­ã€‚  

We have reproducible experiments and we have all-in-one suites that empower model builders to ship.  

æˆ‘ä»¬æ‹¥æœ‰å¯é‡ç°çš„å®éªŒå’Œä¸€ä½“åŒ–å¥—ä»¶ï¼Œå¯ä»¥è®©æ¨¡å‹æ„å»ºè€…è½»æ¾å‘å¸ƒæ¨¡å‹ã€‚  

And Lordy, do we have YAML files.  

ä¸»å•Šï¼Œæˆ‘ä»¬çœŸæ˜¯æœ‰å¾ˆå¤š YAML æ–‡ä»¶ã€‚

But as an industry, MLOps didnâ€™t adopt the function of DevOps.  

ä½†ä½œä¸ºä¸€ä¸ªè¡Œä¸šï¼ŒMLOps å¹¶æ²¡æœ‰é‡‡çº³ DevOps çš„èŒèƒ½ã€‚  

It didnâ€™t shorten the feedback gap between models and their inferences and interactions in production.  

å®ƒæ²¡æœ‰ç¼©çŸ­æ¨¡å‹å’Œå®ƒä»¬åœ¨ç”Ÿäº§ä¸­çš„æ¨ç†å’Œäº¤äº’ä¹‹é—´çš„åé¦ˆå»¶è¿Ÿã€‚

Hearteningly, the field of LLMOps has shifted away from thinking about hobgoblins of little minds like prompt management and towards the hard problems that block iteration: production monitoring and continual improvement, linked by evaluation.  

ä»¤äººæŒ¯å¥‹çš„æ˜¯ï¼ŒLLMOps é¢†åŸŸå·²ç»æ‘†è„±äº†å¯¹å°å¿ƒçµé­”é¬¼ï¼ˆå¦‚æç¤ºç®¡ç†ï¼‰çš„æ€è€ƒï¼Œè½¬å‘äº†é˜»ç¢è¿­ä»£çš„éš¾é¢˜ï¼šç”Ÿäº§ç›‘æ§å’ŒæŒç»­æ”¹è¿›ï¼ŒäºŒè€…é€šè¿‡è¯„ä¼°ç›¸äº’è”ç³»ã€‚

Already, we have interactive arenas for neutral, crowd-sourced evaluation of chat and coding models â€“ an outer loop of collective, iterative improvement.  

ç›®å‰ï¼Œæˆ‘ä»¬å·²ç»æ‹¥æœ‰ç”¨äºä¸­ç«‹ã€ä¼—åŒ…è¯„ä¼°èŠå¤©å’Œç¼–ç æ¨¡å‹çš„äº’åŠ¨ç«æŠ€åœº - è¿™æ˜¯ä¸€ä¸ªé›†ä½“ã€è¿­ä»£æ”¹è¿›çš„å¤–éƒ¨å¾ªç¯ã€‚  

Tools like LangSmith, Log10, LangFuse, W&B Weave, HoneyHive, and more promise to not only collect and collate data about system outcomes in production, but also to leverage them to improve those systems by integrating deeply with development.  

LangSmithã€Log10ã€LangFuseã€W&B Weaveã€HoneyHive ç­‰å·¥å…·æ‰¿è¯ºä¸ä»…æ”¶é›†å’Œæ•´ç†æœ‰å…³ç”Ÿäº§ç³»ç»Ÿç»“æœçš„æ•°æ®ï¼Œè¿˜é€šè¿‡ä¸å¼€å‘æ·±åº¦æ•´åˆæ¥åˆ©ç”¨è¿™äº›æ•°æ®æ¥æ”¹è¿›è¿™äº›ç³»ç»Ÿã€‚  

Embrace these tools or build your own.  

æ¥å—è¿™äº›å·¥å…·æˆ–è€…è‡ªè¡Œå¼€å‘ã€‚

### Donâ€™t Build LLM Features You Can Buy  

ä¸è¦å¼€å‘LLMä¸ªå¯ä»¥è´­ä¹°çš„åŠŸèƒ½[](https://applied-llms.org/#dont-build-llm-features-you-can-buy)

Most successful businesses are not LLM businesses.  

å¤§å¤šæ•°æˆåŠŸçš„ä¼ä¸šå¹¶ä¸æ˜¯LLMä¼ä¸šã€‚  

Simultaneously, most businesses have opportunities to be improved by LLMs.  

åŒæ—¶ï¼Œå¤§å¤šæ•°ä¼ä¸šéƒ½æœ‰æœºä¼šé€šè¿‡LLMsæ¥å®ç°æ”¹è¿›ã€‚

This pair of observations often mislead leaders into hastily retrofitting systems with LLMs at increased cost and decreased quality and releasing them as ersatz, vanity â€œAIâ€ features, complete with the [now-dreaded sparkle icon](https://x.com/nearcyan/status/1783351706031718412).  

è¿™ä¸€å¯¹è§‚å¯Ÿå¾€å¾€ä¼šè¯¯å¯¼é¢†å¯¼è€…åŒ†å¿™åœ°å¯¹ç³»ç»Ÿè¿›è¡Œæ”¹é€ ï¼Œå¢åŠ æˆæœ¬ã€é™ä½è´¨é‡ï¼Œç„¶åå°†å…¶å‘å¸ƒä¸ºèµå“ã€è™šè£çš„â€œAIâ€åŠŸèƒ½ï¼Œé…å¤‡ç€å¦‚ä»Šå¤‡å—ææƒ§çš„é—ªäº®å›¾æ ‡ã€‚  

Thereâ€™s a better way: focus on LLM applications that truly align with your product goals and enhance your core operations.  

ä¸“æ³¨äºä¸æ‚¨çš„äº§å“ç›®æ ‡çœŸæ­£ä¸€è‡´å¹¶å¢å¼ºæ‚¨çš„æ ¸å¿ƒè¿è¥çš„LLMåº”ç”¨ç¨‹åºï¼Œè¿™æ‰æ˜¯æ›´å¥½çš„æ–¹å¼ã€‚

Consider a few misguided ventures that waste your teamâ€™s time:  

è€ƒè™‘ä¸€äº›è¯¯å¯¼æ€§çš„å†’é™©ï¼Œæµªè´¹å›¢é˜Ÿæ—¶é—´çš„æƒ…å†µï¼š

-   Building custom text-to-SQL capabilities for your business.  
    
    ä¸ºæ‚¨çš„ä¸šåŠ¡å®šåˆ¶æ–‡æœ¬åˆ° SQL çš„åŠŸèƒ½ã€‚
-   Building a chatbot to talk to your documentation.  
    
    åˆ›å»ºä¸€ä¸ªèƒ½ä¸æ‚¨çš„æ–‡æ¡£è¿›è¡Œå¯¹è¯çš„èŠå¤©æœºå™¨äººã€‚
-   Integrating your companyâ€™s knowledge base with your customer support chatbot.  
    
    å°†æ‚¨å…¬å¸çš„çŸ¥è¯†åº“ä¸å®¢æˆ·æ”¯æŒèŠå¤©æœºå™¨äººè¿›è¡Œæ•´åˆã€‚

While the above are the hellos-world of LLM applications, none of them make sense for a product company to build themselves.  

å°½ç®¡ä»¥ä¸Šæ˜¯LLMåº”ç”¨ç¨‹åºçš„åŸºç¡€ï¼Œä½†å¯¹äºäº§å“å…¬å¸è‡ªè¡Œå¼€å‘è¿™äº›åº”ç”¨ç¨‹åºéƒ½æ²¡æœ‰æ„ä¹‰ã€‚  

These are general problems for many businesses with a large gap between promising demo and dependable componentâ€”the customary domain of software companies.  

è¿™äº›æ˜¯è®¸å¤šä¼ä¸šæ™®éå­˜åœ¨çš„é—®é¢˜ï¼Œè¡¨ç°å‡ºæ‰¿è¯ºçš„æ¼”ç¤ºå’Œå¯é ç»„ä»¶ä¹‹é—´å­˜åœ¨å¾ˆå¤§å·®è·â€”â€”è¿™æ˜¯è½¯ä»¶å…¬å¸çš„æƒ¯å¸¸é¢†åŸŸã€‚  

Investing valuable R&D resources on general problems being tackled en masse by the current Y Combinator batch is a waste.  

æŠŠå®è´µçš„ç ”å‘èµ„æºæŠ•å…¥åˆ°å½“å‰ Y Combinator æ‰¹æ¬¡æ­£åœ¨å¤§è§„æ¨¡è§£å†³çš„é€šç”¨é—®é¢˜ä¸Šæ˜¯ä¸€ç§æµªè´¹ã€‚

If this sounds like trite business advice, itâ€™s because in the frothy excitement of the current hype wave, itâ€™s easy to mistake anything â€œLLMâ€ as cutting-edge, accretive differentiation, missing which applications are already old hat.  

å¦‚æœè¿™å¬èµ·æ¥åƒè€ç”Ÿå¸¸è°ˆçš„å•†ä¸šå»ºè®®ï¼Œé‚£æ˜¯å› ä¸ºåœ¨å½“å‰ç‚’ä½œç‹‚æ½®çš„å…´å¥‹ä¸­ï¼Œå¾ˆå®¹æ˜“å°†ä»»ä½•â€œLLMâ€è¯¯è®¤ä¸ºæ˜¯å°–ç«¯ã€å¢å€¼çš„å·®å¼‚åŒ–ï¼Œå¿½ç•¥äº†å“ªäº›åº”ç”¨å·²ç»è¿‡æ—¶ã€‚

### AI in the loop; Humans at the center  

3.2.5 äººå·¥æ™ºèƒ½åœ¨å¾ªç¯ä¸­ï¼›ä»¥äººç±»ä¸ºä¸­å¿ƒ[](https://applied-llms.org/#ai-in-the-loop-humans-at-the-center)

Right now, LLM-powered applications are brittle.  

ç›®å‰ï¼ŒåŸºäºLLMçš„åº”ç”¨ç¨‹åºå¾ˆå®¹æ˜“å‡ºç°é—®é¢˜ã€‚  

They required an incredible amount of safe-guarding and defensive engineering, yet remain hard to predict.  

ä»–ä»¬éœ€è¦å¤§é‡çš„å®‰å…¨é˜²æŠ¤å’Œé˜²å¾¡å·¥ç¨‹ï¼Œä½†ä»ç„¶éš¾ä»¥é¢„æµ‹ã€‚  

Additionally, when tightly scoped these applications can be wildly useful.  

æ­¤å¤–ï¼Œå½“è¿™äº›åº”ç”¨ç¨‹åºè¢«ä¸¥æ ¼é™å®šèŒƒå›´æ—¶ï¼Œå®ƒä»¬å¯ä»¥æä¸ºæœ‰ç”¨ã€‚  

This means that LLMs make excellent tools to accelerate user workflows.  

è¿™æ„å‘³ç€LLMsæ˜¯ä¼˜ç§€çš„å·¥å…·ï¼Œå¯ä»¥åŠ é€Ÿç”¨æˆ·çš„å·¥ä½œæµç¨‹ã€‚

While it may be tempting to imagine LLM-based applications fully replacing a workflow, or standing in for a job function, today the most effective paradigm is a human-computer centaur ([Centaur chess](https://en.wikipedia.org/wiki/Advanced_chess)).  

è™½ç„¶è¯±äººï¼Œä½†æƒ³è±¡LLMä¸ºåŸºç¡€çš„åº”ç”¨ç¨‹åºå®Œå…¨å–ä»£å·¥ä½œæµç¨‹æˆ–æ›¿ä»£å·¥ä½œèŒèƒ½ï¼Œå¦‚ä»Šæœ€æœ‰æ•ˆçš„èŒƒå¼æ˜¯äººæœºåŠäººé©¬ï¼ˆåŠäººé©¬å›½é™…è±¡æ£‹ï¼‰ã€‚  

When capable humans are paired with LLM capabilities tuned for their rapid utilization, productivity and happiness doing tasks can be massively increased.  

å½“æœ‰èƒ½åŠ›çš„äººç±»ä¸ç»è¿‡è°ƒæ•´ä»¥å¿«é€Ÿåˆ©ç”¨çš„LLMèƒ½åŠ›é…å¯¹æ—¶ï¼Œä»»åŠ¡çš„å®Œæˆå¯ä»¥å¤§å¹…æé«˜ç”Ÿäº§åŠ›å’Œå¹¸ç¦æ„Ÿã€‚  

One of the flagship applications of LLMs, GitHub CoPilot, demonstrated the power of these workflows:  

ä½œä¸ºLLMsçš„æ——èˆ°åº”ç”¨ä¹‹ä¸€ï¼ŒGitHub CoPilot å±•ç¤ºäº†è¿™äº›å·¥ä½œæµçš„å¼ºå¤§ä¹‹å¤„ï¼š

> â€œOverall, developers told us they felt more confident because coding is easier, more error-free, more readable, more reusable, more concise, more maintainable, and more resilient with GitHub Copilot and GitHub Copilot Chat than when theyâ€™re coding without it.â€ - [Mario Rodriguez, GitHub](https://resources.github.com/learn/pathways/copilot/essentials/measuring-the-impact-of-github-copilot/)  
> 
> â€œæ€»çš„æ¥è¯´ï¼Œå¼€å‘äººå‘˜å‘Šè¯‰æˆ‘ä»¬ï¼Œä»–ä»¬å› ä¸ºä½¿ç”¨ GitHub Copilot å’Œ GitHub Copilot Chat è€Œæ„Ÿåˆ°æ›´åŠ è‡ªä¿¡ï¼Œå› ä¸ºç¼–ç æ›´å®¹æ˜“ã€æ›´æ— é”™è¯¯ã€æ›´æ˜“è¯»ã€æ›´å¯é‡ç”¨ã€æ›´ç®€æ´ã€æ›´æ˜“ç»´æŠ¤å’Œæ›´å…·å¼¹æ€§ï¼Œè€Œä¸æ˜¯åœ¨æ²¡æœ‰å®ƒçš„æƒ…å†µä¸‹ç¼–ç ã€‚â€ - Mario Rodriguez, GitHub

For those who have worked in ML for a long time, you may jump to the idea of â€œhuman-in-the-loopâ€, but not so fast: HITL Machine Learning is a paradigm built on Human experts ensuring that ML models behave as predicted.  

é•¿æœŸä»äº‹æœºå™¨å­¦ä¹ çš„äººå¯èƒ½ä¼šè¿…é€Ÿè·³åˆ°â€œäººåœ¨å¾ªç¯â€çš„æ¦‚å¿µï¼Œä½†ä¸è¦é‚£ä¹ˆå¿«ï¼šHITL æœºå™¨å­¦ä¹ æ˜¯å»ºç«‹åœ¨äººç±»ä¸“å®¶ç¡®ä¿ ML æ¨¡å‹è¡¨ç°å¦‚é¢„æœŸçš„èŒƒå¼ä¸Šã€‚  

While related, here we are proposing something more subtle.  

è™½ç„¶ç›¸å…³ï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä»¬æå‡ºäº†æ›´å¾®å¦™çš„å»ºè®®ã€‚  

LLM-driven systems should not be the primary drivers of most workflows today, they should merely be a resource.  

ä»Šå¤©ï¼ŒLLMé©±åŠ¨ç³»ç»Ÿä¸åº”è¯¥æ˜¯å¤§å¤šæ•°å·¥ä½œæµç¨‹çš„ä¸»è¦æ¨åŠ¨åŠ›ï¼Œè€Œåº”è¯¥åªæ˜¯ä¸€ç§èµ„æºã€‚

By centering humans, and asking how an LLM can support their workflow, this leads to significantly different product and design decisions.  

é€šè¿‡ä»¥äººä¸ºä¸­å¿ƒï¼Œå¹¶æ¢è®¨LLMå¦‚ä½•æ”¯æŒä»–ä»¬çš„å·¥ä½œæµç¨‹ï¼Œè¿™å°†å¯¼è‡´æ˜¾è‘—ä¸åŒçš„äº§å“å’Œè®¾è®¡å†³ç­–ã€‚  

Ultimately, it will drive you to build different products than competitors who try to rapidly offshore all responsibility to LLMs; better, more useful, and less risky products.  

æœ€ç»ˆï¼Œå®ƒå°†ä¿ƒä½¿æ‚¨å¼€å‘å‡ºä¸è¯•å›¾è¿…é€Ÿå°†æ‰€æœ‰è´£ä»»å¤–åŒ…ç»™LLMsçš„ç«äº‰å¯¹æ‰‹ä¸åŒçš„äº§å“ï¼›æ›´å¥½ã€æ›´å®ç”¨ã€æ›´å®‰å…¨çš„äº§å“ã€‚

## Start with prompting, evals, and data collection  

3.3 ä»æç¤ºã€è¯„ä¼°å’Œæ•°æ®æ”¶é›†å¼€å§‹[](https://applied-llms.org/#start-with-prompting-evals-and-data-collection)

The previous sections have delivered a firehose of techniques and advice. Itâ€™s a lot to take in.  

å…ˆå‰çš„éƒ¨åˆ†æä¾›äº†å¤§é‡çš„æŠ€æœ¯å’Œå»ºè®®ã€‚è¿™äº›å†…å®¹å¾ˆå¤šï¼Œéœ€è¦ä»”ç»†æ¶ˆåŒ–ã€‚  

Letâ€™s consider the minimum useful set of advice: if a team wants to build an LLM product, where should they begin?  

è€ƒè™‘åˆ°æœ€åŸºæœ¬çš„å»ºè®®ï¼šå¦‚æœä¸€ä¸ªå›¢é˜Ÿæƒ³è¦å¼€å‘ä¸€ä¸ªLLMäº§å“ï¼Œä»–ä»¬åº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ

Over the past year, weâ€™ve seen enough to be confident that successful LLM applications follow a consistent trajectory.  

åœ¨è¿‡å»çš„ä¸€å¹´é‡Œï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°è¶³å¤Ÿçš„è¯æ®ï¼Œå¯ä»¥ç¡®ä¿¡æˆåŠŸçš„LLMåº”ç”¨ç¨‹åºéƒ½éµå¾ªç€ä¸€è‡´çš„å‘å±•è½¨è¿¹ã€‚  

We walk through this basic â€œgetting startedâ€ playbook in this section.  

æˆ‘ä»¬å°†åœ¨æœ¬èŠ‚ä¸­ä»‹ç»è¿™ä¸ªåŸºæœ¬çš„â€œå…¥é—¨â€æ‰‹å†Œã€‚  

The core idea is to start simple and only add complexity as needed.  

æ ¸å¿ƒæ€æƒ³æ˜¯ä»ç®€å•å¼€å§‹ï¼Œåªåœ¨å¿…è¦æ—¶å¢åŠ å¤æ‚æ€§ã€‚  

A decent rule of thumb is that each level of sophistication typically requires at least an order of magnitude more effort than the one before it.  

ä¸€ä¸ªç›¸å½“ä¸é”™çš„ç»éªŒæ³•åˆ™æ˜¯ï¼Œé€šå¸¸æ¯æå‡ä¸€ä¸ªå±‚æ¬¡çš„å¤æ‚åº¦ï¼Œæ‰€éœ€çš„åŠªåŠ›è‡³å°‘æ¯”ä¹‹å‰å¤šä¸€ä¸ªæ•°é‡çº§ã€‚  

With this in mindâ€¦  

è€ƒè™‘åˆ°è¿™ä¸€ç‚¹..

### Prompt engineering comes first  

å·¥ç¨‹ä¼˜å…ˆè€ƒè™‘æç¤º[](https://applied-llms.org/#prompt-engineering-comes-first)

Start with prompt engineering. Use all the techniques we discussed in the tactics section before.  

ä»æç¤ºå·¥ç¨‹å¼€å§‹ã€‚åœ¨æˆ˜æœ¯éƒ¨åˆ†è®¨è®ºçš„æ‰€æœ‰æŠ€æœ¯ä¹‹å‰ä½¿ç”¨ã€‚  

Chain-of-thought, n-shot examples, and structured input and output are almost always a good idea.  

æ€ç»´é“¾ã€n-shot ç¤ºä¾‹ä»¥åŠç»“æ„åŒ–çš„è¾“å…¥å’Œè¾“å‡ºå‡ ä¹æ€»æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚  

Prototype with the most highly capable models before trying to squeeze performance out of weaker models.  

åœ¨å°è¯•æå‡è¾ƒå¼±å‹å·æ€§èƒ½ä¹‹å‰ï¼Œå…ˆä½¿ç”¨æœ€é«˜æ€§èƒ½çš„åŸå‹æœºã€‚

Only if prompt engineering cannot achieve the desired level of performance should you consider finetuning.  

åªæœ‰å½“å·¥ç¨‹æ— æ³•è¾¾åˆ°æ‰€éœ€çš„æ€§èƒ½æ°´å¹³æ—¶ï¼Œæ‰åº”è€ƒè™‘å¾®è°ƒã€‚  

This will come up more often if there are non-functional requirements (e.g., data privacy, complete control, cost) that block the use of proprietary models and thus require you to self-host. Just make sure those same privacy requirements donâ€™t block you from using user data for finetuning!  

å¦‚æœå­˜åœ¨é˜»æ­¢ä½¿ç”¨ä¸“æœ‰æ¨¡å‹å¹¶å› æ­¤éœ€è¦è‡ªè¡Œæ‰˜ç®¡çš„éåŠŸèƒ½æ€§è¦æ±‚ï¼ˆä¾‹å¦‚ï¼Œæ•°æ®éšç§ã€å®Œå…¨æ§åˆ¶ã€æˆæœ¬ï¼‰ï¼Œè¿™å°†æ›´é¢‘ç¹åœ°å‡ºç°ã€‚åªéœ€ç¡®ä¿è¿™äº›éšç§è¦æ±‚ä¸ä¼šé˜»æ­¢æ‚¨ä½¿ç”¨ç”¨æˆ·æ•°æ®è¿›è¡Œå¾®è°ƒï¼

### Build evals and kickstart a data flywheel  

3.3.2 æ„å»ºè¯„ä¼°å¹¶å¯åŠ¨æ•°æ®é£è½®[](https://applied-llms.org/#build-evals-and-kickstart-a-data-flywheel)

Even teams that are just getting started need evals.  

å³ä½¿æ˜¯åˆšåˆšèµ·æ­¥çš„å›¢é˜Ÿä¹Ÿéœ€è¦è¿›è¡Œè¯„ä¼°ã€‚  

Otherwise, you wonâ€™t know whether your prompt engineering is sufficient or when your finetuned model is ready to replace the base model.  

å¦åˆ™ï¼Œæ‚¨å°†æ— æ³•ç¡®å®šæ‚¨çš„æç¤ºå·¥ç¨‹æ˜¯å¦è¶³å¤Ÿï¼Œæˆ–è€…æ‚¨çš„å¾®è°ƒæ¨¡å‹ä½•æ—¶å‡†å¤‡å¥½æ›¿æ¢åŸºç¡€æ¨¡å‹ã€‚

Effective evals are [specific to your tasks](https://twitter.com/thesephist/status/1707839140018974776) and mirror the intended use cases. The first level of evals that we [recommend](https://hamel.dev/blog/posts/evals/) is unit testing.  

æœ‰æ•ˆçš„è¯„ä¼°åº”å½“å…·ä½“é’ˆå¯¹æ‚¨çš„ä»»åŠ¡ï¼Œå¹¶åæ˜ é¢„æœŸçš„ä½¿ç”¨æƒ…å†µã€‚æˆ‘ä»¬å»ºè®®çš„ç¬¬ä¸€çº§è¯„ä¼°æ˜¯å•å…ƒæµ‹è¯•ã€‚  

These simple assertions detect known or hypothesized failure modes and help drive early design decisions.  

è¿™äº›ç®€å•çš„æ–­è¨€å¯æ£€æµ‹å·²çŸ¥æˆ–å‡è®¾çš„æ•…éšœæ¨¡å¼ï¼Œå¹¶æœ‰åŠ©äºæ¨åŠ¨æ—©æœŸè®¾è®¡å†³ç­–ã€‚  

Also see other [task-specific evals](https://eugeneyan.com/writing/evals/) for classification, summarization, etc.  

è¯·å‚è€ƒå…¶ä»–ä¸“é—¨ä»»åŠ¡çš„è¯„ä¼°ï¼Œå¦‚åˆ†ç±»ã€æ‘˜è¦ç­‰ã€‚

While unit tests and model-based evaluations are useful, they donâ€™t replace the need for human evaluation.  

è™½ç„¶å•å…ƒæµ‹è¯•å’ŒåŸºäºæ¨¡å‹çš„è¯„ä¼°å¾ˆæœ‰ç”¨ï¼Œä½†å®ƒä»¬å¹¶ä¸èƒ½å®Œå…¨æ›¿ä»£äººç±»è¯„ä¼°çš„å¿…è¦æ€§ã€‚  

Have people use your model/product and provide feedback.  

é¼“åŠ±äººä»¬ä½¿ç”¨æ‚¨çš„æ¨¡å‹/äº§å“å¹¶æä¾›åé¦ˆæ„è§ã€‚  

This serves the dual purpose of measuring real-world performance and defect rates while also collecting high-quality annotated data that can be used to finetune future models.  

è¿™æ ·åšä¸ä»…å¯ä»¥è¡¡é‡ç°å®ä¸–ç•Œçš„æ€§èƒ½å’Œç¼ºé™·ç‡ï¼Œè¿˜å¯ä»¥æ”¶é›†é«˜è´¨é‡çš„å¸¦æ³¨é‡Šæ•°æ®ï¼Œç”¨äºä¼˜åŒ–æœªæ¥çš„æ¨¡å‹ã€‚  

This creates a positive feedback loop, or data flywheel, which compounds over time:  

è¿™åˆ›é€ äº†ä¸€ä¸ªç§¯æçš„åé¦ˆå¾ªç¯ï¼Œä¹Ÿå°±æ˜¯æ•°æ®é£è½®ï¼Œéšç€æ—¶é—´çš„æ¨ç§»è€Œä¸æ–­å¢å¼ºï¼š

-   Human evaluation to assess model performance and/or find defects  
    
    äººå·¥è¯„ä¼°ä»¥è¯„ä¼°æ¨¡å‹æ€§èƒ½å’Œ/æˆ–å‘ç°ç¼ºé™·
-   Use the annotated data to finetune the model or update the prompt  
    
    ä½¿ç”¨å¸¦æœ‰æ³¨é‡Šçš„æ•°æ®æ¥å¾®è°ƒæ¨¡å‹æˆ–æ›´æ–°æç¤º
-   Repeat  
    
    é‡å¤ä¸€é

For example, when auditing LLM-generated summaries for defects we might label each sentence with fine-grained feedback identifying factual inconsistency, irrelevance, or poor style.  

ä¾‹å¦‚ï¼Œåœ¨å®¡æ ¸LLMç”Ÿæˆçš„æ‘˜è¦ä¸­æŸ¥æ‰¾ç¼ºé™·æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šä¸ºæ¯ä¸ªå¥å­æ·»åŠ ç»†è‡´çš„åé¦ˆï¼ŒæŒ‡å‡ºäº‹å®ä¸ä¸€è‡´ã€æ— å…³æˆ–ç³Ÿç³•çš„é£æ ¼ã€‚  

We can then use these factual inconsistency annotations to [train a hallucination classifier](https://eugeneyan.com/writing/finetuning/) or use the relevance annotations to train a [relevance-reward model](https://arxiv.org/abs/2009.01325). As another example, LinkedIn shared about their success with using [model-based evaluators](https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product) to estimate hallucinations, responsible AI violations, coherence, etc. in their write-up  

æˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™äº›äº‹å®ä¸ä¸€è‡´æ€§çš„æ³¨é‡Šæ¥è®­ç»ƒå¹»è§‰åˆ†ç±»å™¨ï¼Œæˆ–è€…ä½¿ç”¨ç›¸å…³æ€§çš„æ³¨é‡Šæ¥è®­ç»ƒç›¸å…³æ€§å¥–åŠ±æ¨¡å‹ã€‚ä¸¾ä¸ªä¾‹å­ï¼ŒLinkedIn åœ¨ä»–ä»¬çš„æ–‡ç« ä¸­åˆ†äº«äº†ä½¿ç”¨åŸºäºæ¨¡å‹çš„è¯„ä¼°å™¨æ¥ä¼°è®¡å¹»è§‰ã€è´Ÿè´£ä»»çš„ AI è¿è§„è¡Œä¸ºã€è¿è´¯æ€§ç­‰æ–¹é¢å–å¾—çš„æˆåŠŸã€‚

By creating assets that compound their value over time, we upgrade building evals from a purely operational expense to a strategic investment, and build our data flywheel in the process.  

é€šè¿‡åˆ›å»ºéšç€æ—¶é—´å¢å€¼çš„èµ„äº§ï¼Œæˆ‘ä»¬å°†å»ºç­‘è¯„ä¼°ä»çº¯ç²¹çš„è¿è¥è´¹ç”¨å‡çº§ä¸ºæˆ˜ç•¥æŠ•èµ„ï¼Œå¹¶åœ¨æ­¤è¿‡ç¨‹ä¸­æ„å»ºæˆ‘ä»¬çš„æ•°æ®é£è½®ã€‚

## The high-level trend of low-cost cognition  

ä½æˆæœ¬è®¤çŸ¥çš„é«˜å±‚è¶‹åŠ¿[](https://applied-llms.org/#the-high-level-trend-of-low-cost-cognition)

In 1971, the researchers at Xerox PARC predicted the future: the world of networked personal computers that we are now living in.  

1971 å¹´ï¼Œæ–½ä¹å¸•å…‹çš„ç ”ç©¶äººå‘˜é¢„è¨€äº†æœªæ¥ï¼šæˆ‘ä»¬ç°åœ¨ç”Ÿæ´»åœ¨çš„ç½‘ç»œä¸ªäººç”µè„‘ä¸–ç•Œã€‚  

They helped birth that future by playing pivotal roles in the invention of the technologies that made it possible, from Ethernet and graphics rendering to the mouse and the window.  

ä»–ä»¬é€šè¿‡åœ¨å‘æ˜ä½¿å…¶æˆä¸ºå¯èƒ½çš„æŠ€æœ¯ä¸­å‘æŒ¥å…³é”®ä½œç”¨ï¼Œå¦‚ä»¥å¤ªç½‘å’Œå›¾å½¢æ¸²æŸ“ï¼Œé¼ æ ‡å’Œçª—å£ï¼Œå¸®åŠ©å­•è‚²äº†é‚£ä¸ªæœªæ¥ã€‚

But they also engaged in a simple exercise: they looked at applications that were very useful (e.g.Â video displays) but were not yet economical (i.e.Â enough RAM to drive a video display was many thousands of dollars).  

ä½†ä»–ä»¬ä¹Ÿè¿›è¡Œäº†ä¸€ä¸ªç®€å•çš„ç»ƒä¹ ï¼šä»–ä»¬ç ”ç©¶äº†ä¸€äº›éå¸¸å®ç”¨çš„åº”ç”¨ï¼ˆæ¯”å¦‚è§†é¢‘æ˜¾ç¤ºï¼‰ï¼Œä½†ç›®å‰è¿˜ä¸åˆ’ç®—ï¼ˆå³ï¼Œé©±åŠ¨è§†é¢‘æ˜¾ç¤ºæ‰€éœ€çš„å†…å­˜æˆæœ¬é«˜è¾¾æ•°åƒç¾å…ƒï¼‰ã€‚  

Then they looked at historic price trends for that technology (a la Mooreâ€™s Law) and predicted when those technologies would become economical.  

ç„¶åä»–ä»¬æŸ¥çœ‹äº†è¯¥æŠ€æœ¯çš„å†å²ä»·æ ¼è¶‹åŠ¿ï¼ˆç±»ä¼¼äºæ‘©å°”å®šå¾‹ï¼‰ï¼Œå¹¶é¢„æµ‹äº†è¿™äº›æŠ€æœ¯ä½•æ—¶ä¼šå˜å¾—ç»æµå®æƒ ã€‚

We can do the same for LLM technologies, even though we donâ€™t have something quite as clean as transistors per dollar to work with.  

æˆ‘ä»¬å¯ä»¥ä¸ºLLMæŠ€æœ¯åšåŒæ ·çš„äº‹æƒ…ï¼Œå°½ç®¡æˆ‘ä»¬æ²¡æœ‰åƒæ¯ç¾å…ƒæ™¶ä½“ç®¡é‚£æ ·å¹²å‡€çš„ä¸œè¥¿å¯ä»¥ä½¿ç”¨ã€‚  

Take a popular, long-standing benchmark, like the Massively-Multitask Language Understanding dataset, and a consistent input approach (five-shot prompting).  

é€‰æ‹©ä¸€ä¸ªæµè¡Œä¸”å†å²æ‚ ä¹…çš„åŸºå‡†æµ‹è¯•ï¼Œå¦‚å¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£æ•°æ®é›†ï¼Œé‡‡ç”¨ä¸€è‡´çš„è¾“å…¥æ–¹æ³•ï¼ˆäº”æ¬¡æç¤ºï¼‰ã€‚  

Then, compare the cost to run language models with various performance levels on this benchmark over time.  

ç„¶åï¼Œæ¯”è¾ƒéšç€æ—¶é—´æ¨ç§»åœ¨æ­¤åŸºå‡†æµ‹è¯•ä¸Šä»¥ä¸åŒæ€§èƒ½æ°´å¹³è¿è¡Œè¯­è¨€æ¨¡å‹çš„æˆæœ¬ã€‚

![](models-prices.png) Figure. For a fixed cost, capabilities are rapidly increasing.  

é’ˆå¯¹å›ºå®šæˆæœ¬ï¼Œèƒ½åŠ›è¿…é€Ÿå¢å¼ºã€‚  

For a fixed capability level, costs are rapidly decreasing.  

é’ˆå¯¹å›ºå®šçš„èƒ½åŠ›æ°´å¹³ï¼Œæˆæœ¬æ­£åœ¨è¿…é€Ÿä¸‹é™ã€‚  

Created by co-author Charles Frye using public data on May 13, 2024.  

ç”±åˆè‘—è€…æŸ¥å°”æ–¯Â·å¼—è±äº 2024 å¹´ 5 æœˆ 13 æ—¥ä½¿ç”¨å…¬å…±æ•°æ®åˆ›å»ºã€‚

In the four years since the launch of OpenAIâ€™s davinci model as an API, the cost of running a model with equivalent performance on that task at the scale of one million tokens (about one hundred copies of this document) has dropped from $20 to less than 10Â¢ â€“ a halving time of just six months.  

è‡ª OpenAI çš„ davinci æ¨¡å‹ä½œä¸º API æ¨å‡ºä»¥æ¥çš„å››å¹´é‡Œï¼Œä»¥ç›¸åŒæ€§èƒ½åœ¨ä¸€ç™¾ä¸‡æ ‡è®°è§„æ¨¡ï¼ˆå¤§çº¦ç›¸å½“äºä¸€ç™¾ä»½æœ¬æ–‡ï¼‰ä¸Šè¿è¡Œæ¨¡å‹çš„æˆæœ¬å·²ç»ä» 20 ç¾å…ƒé™è‡³ä¸åˆ° 10 ç¾åˆ† - ä»…ä»…å…­ä¸ªæœˆçš„å‡åŠæ—¶é—´ã€‚  

Similarly, the cost to run Metaâ€™s LLaMA 3 8B, via an API provider or on your own, is just 20Â¢ per million tokens as of May of 2024, and it has similar performance to OpenAIâ€™s text-davinci-003, the model that enabled ChatGPT.  

åŒæ ·ï¼Œæˆªè‡³ 2024 å¹´ 5 æœˆï¼Œé€šè¿‡ API æä¾›å•†æˆ–è‡ªè¡Œè¿è¡Œ Meta çš„ LLaMA 3 8B çš„æˆæœ¬ä»…ä¸ºæ¯ç™¾ä¸‡ä»¤ç‰Œ 20 ç¾åˆ†ï¼Œå…¶æ€§èƒ½ä¸ OpenAI çš„ text-davinci-003 ç±»ä¼¼ï¼Œåè€…ä½¿ChatGPTæˆä¸ºå¯èƒ½ã€‚  

That model also cost about $20 per million tokens when it was released in late November of 2023.  

é‚£æ¬¾æ¨¡å‹åœ¨ 2023 å¹´ 11 æœˆåº•å‘å¸ƒæ—¶ï¼Œæ¯ä¸€ç™¾ä¸‡ä¸ªæ ‡è®°çš„æˆæœ¬å¤§çº¦æ˜¯ 20 ç¾å…ƒã€‚  

Thatâ€™s two orders of magnitude in just 18 months â€“ the same timeframe in which Mooreâ€™s Law predicts a mere doubling.  

è¿™ä»…ä»…æ˜¯åœ¨çŸ­çŸ­ 18 ä¸ªæœˆå†…è¾¾åˆ°äº†ä¸¤ä¸ªæ•°é‡çº§çš„å¢é•¿ï¼Œä¸æ‘©å°”å®šå¾‹é¢„æµ‹çš„ä»…ä»…ç¿»å€ç›¸æ¯”ã€‚

Now, letâ€™s consider an application of LLMs that is very useful (powering generative video game characters, a la [Park et al](https://arxiv.org/abs/2304.03442)) but is not yet economical (their cost was estimated at $625 per hour [here](https://arxiv.org/abs/2310.02172)).  

ç°åœ¨ï¼Œè®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„LLMsåº”ç”¨ï¼Œç”¨äºä¸ºç”Ÿæˆå¼è§†é¢‘æ¸¸æˆè§’è‰²æä¾›åŠ¨åŠ›ï¼Œç±»ä¼¼äº Park ç­‰äººçš„åšæ³•ï¼Œä½†ç›®å‰å°šä¸ç»æµï¼ˆä»–ä»¬çš„æˆæœ¬åœ¨è¿™é‡Œä¼°è®¡ä¸ºæ¯å°æ—¶ 625 ç¾å…ƒï¼‰ã€‚  

Since that paper was published in August of 2023, the cost has dropped roughly one order of magnitude, to $62.50 per hour.  

è‡ªä» 2023 å¹´ 8 æœˆå‘è¡¨è¯¥è®ºæ–‡ä»¥æ¥ï¼Œæˆæœ¬å·²ç»å¤§è‡´é™ä½äº†ä¸€ä¸ªæ•°é‡çº§ï¼Œæ¯å°æ—¶ 62.50 ç¾å…ƒã€‚  

We might expect it to drop to $6.25 per hour in another nine months.  

æˆ‘ä»¬å¯èƒ½é¢„è®¡å®ƒä¼šåœ¨å¦å¤–ä¹ä¸ªæœˆå†…ä¸‹é™è‡³æ¯å°æ—¶ 6.25 ç¾å…ƒã€‚

Meanwhile, when Pac-Man was released in 1980, $1 of todayâ€™s money would buy you a credit, good to play for a few minutes or tens of minutes â€“ call it six games per hour, or $6 per hour.  

ä¸æ­¤åŒæ—¶ï¼Œ1980 å¹´å‘å¸ƒçš„ã€Šåƒè±†äººã€‹æ¸¸æˆï¼Œä»Šå¤©çš„ 1 ç¾å…ƒå¯ä»¥è®©ä½ è·å¾—ä¸€æ¬¡æ¸¸æˆæœºä¼šï¼Œå¯ä»¥ç©å‡ åˆ†é’Ÿæˆ–å‡ ååˆ†é’Ÿ - æ¯å°æ—¶å¤§çº¦å¯ä»¥ç© 6 å±€æ¸¸æˆï¼Œç›¸å½“äºæ¯å°æ—¶ 6 ç¾å…ƒã€‚  

This napkin math suggests that a compelling LLM-enhanced gaming experience will become economical sometime in 2025.  

è¿™ä¸ªé¤å·¾çº¸ä¸Šçš„è®¡ç®—è¡¨æ˜ï¼Œåˆ° 2025 å¹´æŸä¸ªæ—¶å€™ï¼Œä¸€ä¸ªå¼•äººå…¥èƒœçš„LLMå¢å¼ºæ¸¸æˆä½“éªŒå°†å˜å¾—ç»æµå®æƒ ã€‚

These trends are new, only a few years old.  

è¿™äº›è¶‹åŠ¿æ˜¯æ–°çš„ï¼Œä»…æœ‰å‡ å¹´çš„å†å²ã€‚  

But there is little reason to expect this process to slow down in the next few years.  

ä½†åœ¨æ¥ä¸‹æ¥çš„å‡ å¹´é‡Œï¼Œå¾ˆå°‘æœ‰ç†ç”±æŒ‡æœ›è¿™ä¸ªè¿‡ç¨‹ä¼šå‡ç¼“ã€‚  

Even as we perhaps use up low-hanging fruit in algorithms and datasets, like scaling past the â€œChinchilla ratioâ€ of ~20 tokens per parameter, deeper innovations and investments inside the data center and at the silicon layer promise to pick up the slack.  

å³ä½¿æˆ‘ä»¬å¯èƒ½åœ¨ç®—æ³•å’Œæ•°æ®é›†ä¸­è€—å°½ä½æŒ‚æœå®ï¼Œæ¯”å¦‚è¶…è¿‡æ¯ä¸ªå‚æ•°çš„â€œæ¯›ä¸é¼ æ¯”â€çº¦ 20 ä¸ªä»¤ç‰Œï¼Œæ•°æ®ä¸­å¿ƒå†…éƒ¨å’Œç¡…å±‚çš„æ›´æ·±å±‚åˆ›æ–°å’ŒæŠ•èµ„æ‰¿è¯ºå¡«è¡¥ç©ºç¼ºã€‚

And this is perhaps the most important strategic fact: what is a completely infeasible floor demo or research paper today will become a premium feature in a few years and then a commodity shortly after.  

è¿™ä¹Ÿè®¸æ˜¯æœ€é‡è¦çš„æˆ˜ç•¥äº‹å®ï¼šä»Šå¤©å®Œå…¨ä¸å¯è¡Œçš„åœ°æ¿æ¼”ç¤ºæˆ–ç ”ç©¶è®ºæ–‡å°†åœ¨å‡ å¹´åæˆä¸ºé«˜çº§åŠŸèƒ½ï¼Œç„¶åå¾ˆå¿«å˜æˆå•†å“ã€‚  

We should build our systems, and our organizations, with this in mind.  

æˆ‘ä»¬åº”è¯¥ä»¥æ­¤ä¸ºæŒ‡å¯¼åŸåˆ™æ¥æ„å»ºæˆ‘ä»¬çš„ç³»ç»Ÿå’Œç»„ç»‡ã€‚

## Enough 0 to 1 demos, itâ€™s time for 1 to N products  

4 ä¸ªè¶³å¤Ÿçš„æ¼”ç¤ºï¼Œæ˜¯æ—¶å€™ä» 1 ä¸ªåˆ° N ä¸ªäº§å“äº†

We get it, building LLM demos is a ton of fun.  

æˆ‘ä»¬æ˜ç™½ï¼Œåˆ¶ä½œLLMä¸ªæ¼”ç¤ºçœŸçš„å¾ˆæœ‰è¶£ã€‚  

With just a few lines of code, a vector database, and a carefully crafted prompt, we create âœ¨magic âœ¨.  

ä»…éœ€å‡ è¡Œä»£ç ã€ä¸€ä¸ªå‘é‡æ•°æ®åº“å’Œä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œæˆ‘ä»¬å°±èƒ½åˆ›é€ å‡º âœ¨é­”æ³•âœ¨ã€‚  

And in the past year, this magic has been compared to the internet, the smartphone, and even the printing press.  

åœ¨è¿‡å»çš„ä¸€å¹´é‡Œï¼Œè¿™ç§é­”æ³•è¢«æ¯”ä½œäº’è”ç½‘ã€æ™ºèƒ½æ‰‹æœºï¼Œç”šè‡³å°åˆ·æœ¯ã€‚

Unfortunately, as anyone who has worked on shipping real-world software knows, thereâ€™s a world of difference between a demo that works in a controlled setting and a product that operates reliably at scale.  

ä¸å¹¸çš„æ˜¯ï¼Œä»»ä½•åœ¨å®é™…è½¯ä»¶äº¤ä»˜ä¸­å·¥ä½œè¿‡çš„äººéƒ½çŸ¥é“ï¼Œåœ¨å—æ§ç¯å¢ƒä¸­è¿è¡Œè‰¯å¥½çš„æ¼”ç¤ºä¸åœ¨è§„æ¨¡ä¸Šå¯é è¿è¡Œçš„äº§å“ä¹‹é—´å­˜åœ¨ç€å¤©å£¤ä¹‹åˆ«ã€‚

> Thereâ€™s a large class of problems that are easy to imagine and build demos for, but extremely hard to make products out of.  
> 
> æœ‰ä¸€ç±»é—®é¢˜å¾ˆå®¹æ˜“æƒ³è±¡å¹¶åˆ¶ä½œæ¼”ç¤ºï¼Œä½†è¦å°†å…¶åšæˆäº§å“å´å¼‚å¸¸å›°éš¾ã€‚  
> 
> For example, self-driving: Itâ€™s easy to demo a car self-driving around a block; making it into a product takes a decade.  
> 
> ä¾‹å¦‚ï¼Œè‡ªåŠ¨é©¾é©¶ï¼šæ¼”ç¤ºä¸€è¾†è½¦åœ¨è¡—åŒºè‡ªåŠ¨é©¾é©¶å¾ˆå®¹æ˜“ï¼›ä½†è¦æŠŠå®ƒå˜æˆäº§å“éœ€è¦åå¹´çš„æ—¶é—´ã€‚  
> 
> \- [Andrej Karpathy](https://x.com/eugeneyan/status/1672692174704766976)  
> 
> å®‰å¾·çƒˆÂ·å¡å¸•è¥¿ (Andrej Karpathy)

Take, for example, self-driving cars. The first car was driven by a neural network in [1988](https://proceedings.neurips.cc/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf). Twenty-five years later, Andrej Karpathy [took his first demo ride in a Waymo](https://x.com/karpathy/status/1689819017610227712). A decade after that, the company received its [driverless permit](https://x.com/Waymo/status/1689809230293819392).  

ä»¥è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸ºä¾‹ï¼Œç¬¬ä¸€è¾†æ±½è½¦æ˜¯åœ¨ 1988 å¹´ç”±ç¥ç»ç½‘ç»œé©¾é©¶çš„ã€‚äºŒåäº”å¹´åï¼ŒAndrej Karpathy åœ¨ Waymo ä¸­è¿›è¡Œäº†ä»–çš„ç¬¬ä¸€æ¬¡æ¼”ç¤ºä¹˜è½¦ã€‚åå¹´åï¼Œè¯¥å…¬å¸è·å¾—äº†æ— äººé©¾é©¶è®¸å¯ã€‚  

Thatâ€™s thirty-five years of rigorous engineering, testing, refinement, and regulatory navigation to go from prototype to commercial product.  

ç»è¿‡ä¸‰åäº”å¹´çš„ä¸¥æ ¼å·¥ç¨‹ã€æµ‹è¯•ã€æ”¹è¿›å’Œç›‘ç®¡å¯¼èˆªï¼Œæ‰èƒ½ä»åŸå‹èµ°å‘å•†ä¸šåŒ–äº§å“ã€‚

Across industry and academia, weâ€™ve observed the ups and downs for the past year: Year 1 of N for LLM applications.  

åœ¨è¡Œä¸šå’Œå­¦æœ¯ç•Œï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°äº†è¿‡å»ä¸€å¹´çš„èµ·ä¼ï¼šLLMåº”ç”¨çš„ç¬¬ä¸€å¹´ã€‚  

We hope that the lessons weâ€™ve learnedâ€”from [tactics](https://applied-llms.org/#tactical-nuts-bolts-of-working-with-llms) like evals, prompt engineering, and guardrails, to [operational](https://applied-llms.org/#operational-day-to-day-and-org-concerns) techniques and building teams to [strategic](https://applied-llms.org/#strategy-building-with-llms-without-getting-out-maneuvered) perspectives like which capabilities to build internallyâ€”help you in year 2 and beyond, as we all build on this exciting new technology together.  

æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬ä»è¯„ä¼°ã€æç¤ºå·¥ç¨‹å’Œé˜²æŠ¤æ ç­‰ç­–ç•¥ï¼Œåˆ°æ“ä½œæŠ€æœ¯å’Œå›¢é˜Ÿå»ºè®¾ï¼Œå†åˆ°æˆ˜ç•¥è§†è§’ï¼Œæ¯”å¦‚å†…éƒ¨å»ºè®¾å“ªäº›èƒ½åŠ›æ‰€å­¦åˆ°çš„ç»éªŒï¼Œèƒ½å¤Ÿåœ¨ç¬¬äºŒå¹´åŠä»¥åå¸®åŠ©æ‚¨ï¼Œå› ä¸ºæˆ‘ä»¬éƒ½åœ¨å…±åŒæ„å»ºè¿™ä¸€ä»¤äººå…´å¥‹çš„æ–°æŠ€æœ¯ã€‚

___

## Stay In Touch  

ä¿æŒè”ç³»ï¼Œä¿æŒè”ç³»

If you found this useful and want updates on write-ups, courses, and activities, subscribe below.  

å¦‚æœæ‚¨è§‰å¾—è¿™ç¯‡å†…å®¹æœ‰å¸®åŠ©ï¼Œå¹¶ä¸”æƒ³è·å–æœ‰å…³æ–‡ç« ã€è¯¾ç¨‹å’Œæ´»åŠ¨çš„æ›´æ–°ï¼Œè¯·åœ¨ä¸‹æ–¹è®¢é˜…ã€‚

You can also find our individual contact information on our [about page](https://applied-llms.org/about.html).  

æ‚¨ä¹Ÿå¯ä»¥åœ¨æˆ‘ä»¬çš„å…³äºé¡µé¢ä¸Šæ‰¾åˆ°æˆ‘ä»¬çš„ä¸ªäººè”ç³»æ–¹å¼ã€‚

## Acknowledgements  

6 è‡´è°¢

This series started as a convo in a group chat, where Bryan quipped that he was inspired to write â€œA Year of AI Engineeringâ€.  

è¿™ä¸ªç³»åˆ—æœ€åˆæºè‡ªä¸€ä¸ªç¾¤èŠä¸­çš„å¯¹è¯ï¼ŒBryan å¼€ç©ç¬‘è¯´ä»–å—åˆ°å¯å‘è¦å†™â€œAI å·¥ç¨‹çš„ä¸€å¹´â€ã€‚  

Then, âœ¨magicâœ¨ happened, and we were all pitched in to share what weâ€™ve learned so far.  

ç„¶åï¼Œâœ¨é­”æ³•âœ¨å‘ç”Ÿäº†ï¼Œæˆ‘ä»¬è¢«é‚€è¯·åˆ†äº«è¿„ä»Šä¸ºæ­¢æ‰€å­¦åˆ°çš„çŸ¥è¯†ã€‚

The authors would like to thank Eugene for leading the bulk of the document integration and overall structure in addition to a large proportion of the lessons.  

ä½œè€…ä»¬è¦æ„Ÿè°¢å°¤é‡‘åœ¨æ•´ä¸ªæ–‡ä»¶çš„æ•´åˆå’Œæ€»ä½“ç»“æ„æ–¹é¢çš„é¢†å¯¼ï¼Œä»¥åŠåœ¨å¾ˆå¤§ä¸€éƒ¨åˆ†è¯¾ç¨‹ä¸­æ‰€ä¼ æˆçš„çŸ¥è¯†ã€‚  

Additionally, for primary editing responsibilities and document direction.  

æ­¤å¤–ï¼Œé’ˆå¯¹ä¸»è¦çš„ç¼–è¾‘èŒè´£å’Œæ–‡ä»¶æŒ‡å¯¼ã€‚  

The authors would like to thank Bryan for the spark that led to this writeup, restructuring the write-up into tactical, operational, and strategic sections and their intros, and for pushing us to think bigger on how we could reach and help the community.  

ä½œè€…ä»¬è¦æ„Ÿè°¢ Bryanï¼Œå› ä¸ºä»–ç‚¹ç‡ƒäº†è¿™ç¯‡å†™ä½œçš„ç«èŠ±ï¼Œå°†å†™ä½œé‡ç»„ä¸ºæˆ˜æœ¯ã€è¿è¥å’Œæˆ˜ç•¥éƒ¨åˆ†åŠå…¶ä»‹ç»ï¼Œå¹¶æ¨åŠ¨æˆ‘ä»¬æ€è€ƒå¦‚ä½•æ›´å¹¿æ³›åœ°æ¥è§¦å’Œå¸®åŠ©ç¤¾åŒºã€‚  

The authors would like to thank Charles for his deep dives on cost and LLMOps, as well as weaving the lessons to make them more coherent and tighterâ€”you have him to thank for this being 30 instead of 40 pages!  

ä½œè€…ä»¬æ„Ÿè°¢æŸ¥å°”æ–¯åœ¨æˆæœ¬å’Œ LLMOps æ–¹é¢çš„æ·±å…¥æ¢è®¨ï¼Œä»¥åŠå°†è¿™äº›ç»éªŒèåˆåœ¨ä¸€èµ·ä½¿ä¹‹æ›´åŠ è¿è´¯å’Œç´§å‡‘ â€”â€” ä½ ä»¬åº”è¯¥æ„Ÿè°¢ä»–ï¼Œå› ä¸ºè¿™æœ¬ä¹¦åªæœ‰ 30 é¡µï¼Œè€Œä¸æ˜¯ 40 é¡µï¼  

The authors thank Hamel and Jason for their insights from advising clients and being on the front lines, for their broad generalizable learnings from clients, and for deep knowledge of tools.  

ä½œè€…æ„Ÿè°¢ Hamel å’Œ Jasonï¼Œæ„Ÿè°¢ä»–ä»¬åœ¨ä¸ºå®¢æˆ·æä¾›å»ºè®®å’Œå¤„äºä¸€çº¿å·¥ä½œä¸­çš„è§è§£ï¼Œæ„Ÿè°¢ä»–ä»¬ä»å®¢æˆ·é‚£é‡Œè·å¾—çš„å¹¿æ³›å¯æ¨å¹¿çš„ç»éªŒæ•™è®­ï¼Œä»¥åŠå¯¹å·¥å…·çš„æ·±å…¥äº†è§£ã€‚  

And finally, thank you Shreya for reminding us of the importance of evals and rigorous production practices and for bringing her research and original results.  

æœ€åï¼Œæ„Ÿè°¢ Shreya æé†’æˆ‘ä»¬è¯„ä¼°å’Œä¸¥æ ¼çš„ç”Ÿäº§å®è·µçš„é‡è¦æ€§ï¼Œå¹¶åˆ†äº«å¥¹çš„ç ”ç©¶å’ŒåŸåˆ›æˆæœã€‚

Finally, we would like to thank all the teams who so generously shared your challenges and lessons in your own write-ups which weâ€™ve referenced throughout this series, along with the AI communities for your vibrant participation and engagement with this group.  

æœ€åï¼Œæˆ‘ä»¬è¦æ„Ÿè°¢æ‰€æœ‰å›¢é˜Ÿï¼Œä»–ä»¬æ…·æ…¨åœ°åˆ†äº«äº†ä½ ä»¬åœ¨è‡ªå·±çš„æ–‡ç« ä¸­æåˆ°çš„æŒ‘æˆ˜å’Œç»éªŒæ•™è®­ï¼Œæˆ‘ä»¬åœ¨æ•´ä¸ªç³»åˆ—ä¸­éƒ½å¼•ç”¨äº†è¿™äº›å†…å®¹ï¼ŒåŒæ—¶ä¹Ÿæ„Ÿè°¢äººå·¥æ™ºèƒ½ç¤¾åŒºå¯¹è¿™ä¸ªå›¢é˜Ÿçš„çƒ­æƒ…å‚ä¸å’Œäº’åŠ¨ã€‚

## About the authors  

6.1 ä½œè€…ç®€ä»‹[](https://applied-llms.org/#about-the-authors)

See the [about page](https://applied-llms.org/about.html) for more information on the authors.  

æŸ¥çœ‹å…³äºé¡µé¢ï¼Œäº†è§£æ›´å¤šå…³äºä½œè€…çš„ä¿¡æ¯ã€‚

If you found this useful, please cite this write-up as:  

å¦‚æœæ‚¨è§‰å¾—è¿™ç¯‡æ–‡ç« å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨æ­¤æ–‡

> Yan, Eugene, Bryan Bischof, Charles Frye, Hamel Husain, Jason Liu, and Shreya Shankar. 2024.  
> 
> Yan, Eugene, Bryan Bischof, Charles Frye, Hamel Husain, Jason Liu å’Œ Shreya Shankarã€‚2024 å¹´ã€‚  
> 
> â€˜Applied LLMs - What Weâ€™ve Learned From A Year of Building with LLMsâ€™. Applied LLMs. 8 June 2024.  
> 
> â€œåº”ç”¨LLMs - æˆ‘ä»¬ä»ä¸€å¹´çš„LLMså»ºè®¾ä¸­å­¦åˆ°çš„ç»éªŒâ€ã€‚åº”ç”¨LLMsã€‚2024 å¹´ 6 æœˆ 8 æ—¥ã€‚  
> 
> https://applied-llms.org/.

or

```
@article{AppliedLLMs2024,
  title = {What We've Learned From A Year of Building with LLMs},
  author = {Yan, Eugene and Bischof, Bryan and Frye, Charles and Husain, Hamel and Liu, Jason and Shankar, Shreya},
  journal = {Applied LLMs},
  year = {2024},
  month = {Jun},
  url = {https://applied-llms.org/}
}
```
